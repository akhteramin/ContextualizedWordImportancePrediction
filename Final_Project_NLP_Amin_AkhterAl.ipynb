{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Importance Model for Predicting Importance of Words in Captions\n",
    " \n",
    "     Paper Title: A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts\n",
    "        https://arxiv.org/pdf/1801.09746.pdf\n",
    "\n",
    "    Corpus: The Switchboard corpus consists of audio recordings of approximately 260 hours of speech consisting of about 2,400 two-sided telephone conversations among 543 speakers (302 male, 241 female) from across the United States. In January 2003, the Institute for Signal and Information Processing (ISIP) released written transcripts for the entire corpus, which consists of nearly 400,000 conversational turns. The ISIP transcripts include a complete lexicon list and automatic word alignment timing corresponding to the original audio files\n",
    "    \n",
    "    The importance score ranges from 0 to 1. However, the paper, referred above, have categorized the words based on the importance level as follows:\n",
    "        \n",
    "    Importance Distribution:\n",
    "    Importance 1: [0-0.1) \n",
    "    Importance 2: [0.1-0.3) \n",
    "    Importance 3: [0.3-0.5) \n",
    "    Importance 4: [0.5-0.7) \n",
    "    Importance 5: [0.7-0.9) \n",
    "    Importance 6: [0.9-1] \n",
    "\n",
    "    The higher the imporance value is, the higher the weight of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Unsupervised Learning Approaches:\n",
    "    1. Bag-of-Words\n",
    "    2. Term frequency and Inverse Document Frequency\n",
    "    3. Word Embedding(CBOW) Summerization with interpolation of POS importance\n",
    "    4. Composition of Contextualized Word Embedding (BERT)\n",
    "    \n",
    "    Supervised Learning Approaches:\n",
    "    Note: Contextualized Word Embeddings (BERT) as feature\n",
    "    1. Logistic Regression\n",
    "    2. Random Forest Classifier\n",
    "    \n",
    "    Existing Best Performing Prior Work:\n",
    "    Neural Network-based approach(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/jmespath/visitor.py:32: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if x is 0 or x is 1:\n",
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/jmespath/visitor.py:32: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if x is 0 or x is 1:\n",
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/jmespath/visitor.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif y is 0 or y is 1:\n",
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/jmespath/visitor.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif y is 0 or y is 1:\n",
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/jmespath/visitor.py:260: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if original_result is 0:\n",
      "I1209 12:24:49.885309 140170348189504 file_utils.py:41] PyTorch version 1.5.0 available.\n",
      "I1209 12:24:56.302066 140170348189504 file_utils.py:57] TensorFlow version 2.3.1 available.\n",
      "I1209 12:24:57.836507 140170348189504 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/aa7510/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load pre-trained model's tokenizer (BERT vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Definition\n",
    "folder = {\n",
    "    '20':['2005'],\n",
    "    '21':['2191'],\n",
    "    '22':['2222'],\n",
    "    '23':['2348'],\n",
    "    '24':['2450'],\n",
    "    '25':['2565'],\n",
    "    '26':['2636'],\n",
    "    '27':['2710'],\n",
    "    '28':['2886'],\n",
    "    '30':['3044','3083'],\n",
    "    '32':['3203'],\n",
    "    '33':['3301','3324'],\n",
    "    '36':['3601'],\n",
    "    '38':['3817'],\n",
    "    '40':['4010','4021'],\n",
    "    '43':['4320'],\n",
    "    '44':['4400'],\n",
    "    '45':['4531'],\n",
    "    '47':['4721']\n",
    "}\n",
    "\n",
    "file_type = ['A','B']\n",
    "size_of_bow = 100\n",
    "threshold = 0.4\n",
    "\n",
    "alpha = 5\n",
    "\n",
    "wem_vector_size = 50\n",
    "\n",
    "POS_IMPORTANCE = {'NN':3.95,'NNP':3.95,'NNS':3.95,\n",
    "                  'VB':3.82, 'VBZ':3.82,'VBP':3.82,\n",
    "                  'VBG':3.82,'VBD':3.82,'VBN':3.82,\n",
    "                  'JJ':3.80,'RB':3.43 }\n",
    "\n",
    "CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scalar(Y, typ = 0):\n",
    "    y_all =[]\n",
    "    for y in Y:\n",
    "        \n",
    "        if typ == 0:\n",
    "            if 0 <= float(y) < 0.1:\n",
    "                y_all.append(1);\n",
    "            elif 0.1 <=  float(y) < 0.3:\n",
    "                y_all.append(2);\n",
    "            elif 0.3 <=  float(y) < 0.5:\n",
    "                y_all.append(3);\n",
    "            elif 0.5 <= float(y) < 0.7:\n",
    "                y_all.append(4);\n",
    "            elif 0.7 <= float(y) < 0.9:\n",
    "                y_all.append(5);\n",
    "            else:\n",
    "                y_all.append(6);\n",
    "        else:\n",
    "            y_all.append(y)\n",
    "    return y_all\n",
    "\n",
    "def make_confusion_matrices(Y_pred, Y_gold, typ):\n",
    "    Y_pred = make_scalar(Y_pred, typ)\n",
    "    Y_gold = make_scalar(Y_gold, typ)\n",
    "    cm = confusion_matrix(Y_pred, Y_gold)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bag of Words Model\n",
    "Measuring word importance using bag of words model. If a token appear in BOW, that will be annotated as `1` or highly important. Otherwise, the token will be weighted as `0`. We consider a variable `threshold` to define the boundary between important and less important words.\n",
    "\n",
    "We have applied certain Modification of Bag of words approach. We know that in Bag of Words approach, create a one-hot vector for each word would be approapriate. But given the requirement of the problem, we have assgined cumulative importance score for each word which is assigned following the method discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Text Preprocessing\n",
    "At this step, we have processed raw transcript files to extract tokens and word importance annotated files to extract annotated word to score mapping. Since the importance score was annotated for each word individually, it was recommended not to remove stopwords from the corpora. There is a possibility that in conversational text, a good amount of words can be excluded if we remove stopwords in pre-processing pipeline. By removing punctuation and other special characters, we have normalized the text. In this process, I have extracted lemma of the words to define the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessor\n",
    " # Prcessing the text: to extract the text and corresponding sense from each line of the file\n",
    "def process_text(line):\n",
    "    splitLine = line.split(\" \")\n",
    "    return splitLine[-1].replace(\"\\n\",\"\")\n",
    "\n",
    "def process_wimp(line):\n",
    "    splitLine = line.split(\" \")\n",
    "    last_word = splitLine[-1].replace(\"\\n\",\"\")\n",
    "    splitLine[-1] =  last_word\n",
    "    return splitLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], '')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "def normalize_text(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = lemmatizing(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words from the switchboard word list\n",
    "def get_words_list(file_name):\n",
    "    files = open(file_name)\n",
    "    text = \"\"\n",
    "    for sentence in files.readlines():\n",
    "        sentence = sentence.split(\" \")\n",
    "        actual_sentence = \" \".join(sentence[3:])\n",
    "        text = text + actual_sentence\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(file):\n",
    "    corpus = get_words_list(file)\n",
    "    wordfreq = {}\n",
    "    tokens = nltk.word_tokenize(str(corpus))\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bag_of_words(file):\n",
    "    wordfreq = get_word_freq(file)\n",
    "    most_freq = heapq.nlargest(size_of_bow, wordfreq, key=wordfreq.get)\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(file_name):\n",
    "    files = open(file_name)\n",
    "    sentence_set=[]\n",
    "    for sentence in files.readlines():\n",
    "        sentence = sentence.split(\" \")\n",
    "        actual_sentence = \" \".join(sentence[3:])\n",
    "        sentence_set.append(actual_sentence.replace(\"\\n\",\"\"))\n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_importance(file):\n",
    "    corpus = get_sentence_list(file)\n",
    "    sentence_vectors = []\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    for sentence in corpus:\n",
    "        sentence = normalize_text(sentence)\n",
    "#         print(sentence)\n",
    "        sentence_tokens = nltk.word_tokenize(str(sentence))\n",
    "        sent_vec = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in most_freq:\n",
    "                sent_vec.append(1)\n",
    "            else:\n",
    "                sent_vec.append(0)\n",
    "\n",
    "        sentence_vectors.append(sent_vec)\n",
    "#     print(sentence_vectors)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Performance Measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wimp_scores(file_name ):\n",
    "    files = open(file_name)\n",
    "    scores = []\n",
    "    for sentence in files.readlines():\n",
    "        score_array = process_wimp(sentence) \n",
    "        scores.append(score_array[3:])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_word_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f5270be5704f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./swb_ms98_transcriptions/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/sw\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-ms98-a-trans.text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mwimp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./wimp_corpus/annotations/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/sw\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-ms98-a-trans.text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wimp_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwimp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_word_importance' is not defined"
     ]
    }
   ],
   "source": [
    "BOW_MSE = 0\n",
    "BOW_Accuracy = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            bow = get_word_importance(file)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(bow[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    for j in range(0,len(scores[i])):\n",
    "\n",
    "                        if (float(bow[i][j])>threshold and float(scores[i][j])>threshold) or (float(bow[i][j])<=threshold and float(scores[i][j])<=threshold):\n",
    "                            correct = correct +1\n",
    "                            BOW_MSE = BOW_MSE + pow(float(bow[i][j])-float(threshold),2)\n",
    "\n",
    "\n",
    "            accuracy = correct/total\n",
    "            BOW_Accuracy.append(accuracy)\n",
    "            print(\"Document Processed- \"+str(len(BOW_Accuracy))+\"/44: Accuracy \"+ str(accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TF-IDF Implementation\n",
    "The idea behind the TF-IDF approach is that the words that are more common in one sentence and less common in other sentences should be given high weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_idf(file):\n",
    "    word_idf_values = {}\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    corpus = get_sentence_list(file)\n",
    "    for token in most_freq:\n",
    "        doc_containing_word = 0\n",
    "        for document in corpus:\n",
    "            if token in nltk.word_tokenize(document):\n",
    "                doc_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "#     print(word_idf_values)\n",
    "    return word_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_tf_score(file):\n",
    "    word_tf_values = {}\n",
    "    corpus = get_sentence_list(file)\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    for token in most_freq:\n",
    "        sent_tf_vector = []\n",
    "        for document in corpus:\n",
    "            doc_freq = 0\n",
    "            for word in nltk.word_tokenize(document):\n",
    "                if token == word:\n",
    "                      doc_freq += 1\n",
    "\n",
    "            word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "    return word_tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_tf_idf_score(file):\n",
    "    word_tf_values = measure_tf_score(file)\n",
    "    word_idf_values =  measure_idf(file)\n",
    "    tfidf_values = []\n",
    "    for token in word_tf_values.keys():\n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "    return tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_word_importance(file):\n",
    "    corpus = get_sentence_list(file)\n",
    "    sentence_vectors = []\n",
    "    word_tf_values = measure_tf_score(file)\n",
    "    word_idf_values = measure_idf(file)\n",
    "    count = 0 \n",
    "    for sentence in corpus:\n",
    "        sentence = normalize_text(sentence)\n",
    "        sentence_tokens = nltk.word_tokenize(str(sentence))\n",
    "        sent_vec = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in word_tf_values:\n",
    "                sent_vec.append(word_tf_values[token][count]*word_idf_values[token])\n",
    "            else:\n",
    "                sent_vec.append(0)\n",
    "        \n",
    "        sentence_vectors.append(sent_vec)\n",
    "        count=count+1\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Performance Measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(X, Y):\n",
    "    if (0 <= X < 0.1 and 0 <= Y < 0.1) or (0.1 <= X < 0.3 and 0.1 <= Y < 0.3) or (0.3 <= X < 0.5 and 0.3 <= Y < 0.5) or (0.5 <= X < 0.7 and 0.5 <= Y < 0.7) or (0.7 <= X < 0.9 and 0.7 <= Y < 0.9) or (0.9 <= X <=1 and 0.9 <= Y <= 1):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wimp_scores(file_name = wimp_file):\n",
    "    files = open(file_name)\n",
    "    scores = []\n",
    "    for sentence in files.readlines():\n",
    "        score_array = process_wimp(sentence) \n",
    "        scores.append(score_array[3:])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeTfIdfData(data):\n",
    "#     print(\"data::\",data)\n",
    "    min_v = 0.01\n",
    "    max_v = 0\n",
    "    for sentence_score in data:\n",
    "        for words_score in sentence_score:\n",
    "                min_v = min(min_v, words_score)\n",
    "                max_v = max(max_v, words_score)\n",
    "    \n",
    "    data[:] = [(np.array(x)-min_v)/(max_v-min_v) if len(x)>1 else [0] for x in data]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 1/44: Accuracy 0.27\n",
      "Document Processed- 2/44: Accuracy 0.23\n",
      "Document Processed- 3/44: Accuracy 0.23\n",
      "Document Processed- 4/44: Accuracy 0.18\n",
      "Document Processed- 5/44: Accuracy 0.28\n",
      "Document Processed- 6/44: Accuracy 0.29\n",
      "Document Processed- 7/44: Accuracy 0.25\n",
      "Document Processed- 8/44: Accuracy 0.23\n",
      "Document Processed- 9/44: Accuracy 0.19\n",
      "Document Processed- 10/44: Accuracy 0.21\n",
      "Document Processed- 11/44: Accuracy 0.31\n",
      "Document Processed- 12/44: Accuracy 0.22\n",
      "Document Processed- 13/44: Accuracy 0.22\n",
      "Document Processed- 14/44: Accuracy 0.26\n",
      "Document Processed- 15/44: Accuracy 0.30\n",
      "Document Processed- 16/44: Accuracy 0.24\n",
      "Document Processed- 17/44: Accuracy 0.22\n",
      "Document Processed- 18/44: Accuracy 0.25\n",
      "Document Processed- 19/44: Accuracy 0.28\n",
      "Document Processed- 20/44: Accuracy 0.26\n",
      "Document Processed- 21/44: Accuracy 0.25\n",
      "Document Processed- 22/44: Accuracy 0.14\n",
      "Document Processed- 23/44: Accuracy 0.30\n",
      "Document Processed- 24/44: Accuracy 0.25\n",
      "Document Processed- 25/44: Accuracy 0.31\n",
      "Document Processed- 26/44: Accuracy 0.25\n",
      "Document Processed- 27/44: Accuracy 0.33\n",
      "Document Processed- 28/44: Accuracy 0.32\n",
      "Document Processed- 29/44: Accuracy 0.23\n",
      "Document Processed- 30/44: Accuracy 0.23\n",
      "Document Processed- 31/44: Accuracy 0.36\n",
      "Document Processed- 32/44: Accuracy 0.35\n",
      "Document Processed- 33/44: Accuracy 0.25\n",
      "Document Processed- 34/44: Accuracy 0.22\n",
      "Document Processed- 35/44: Accuracy 0.28\n",
      "Document Processed- 36/44: Accuracy 0.29\n",
      "Document Processed- 37/44: Accuracy 0.19\n",
      "Document Processed- 38/44: Accuracy 0.24\n",
      "Document Processed- 39/44: Accuracy 0.23\n",
      "Document Processed- 40/44: Accuracy 0.22\n",
      "Document Processed- 41/44: Accuracy 0.26\n",
      "Document Processed- 42/44: Accuracy 0.29\n",
      "Document Processed- 43/44: Accuracy 0.24\n",
      "Document Processed- 44/44: Accuracy 0.34\n",
      "SE:13.281897634500368\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_MSE = 0\n",
    "TF_IDF_Accuracy = []\n",
    "Y_gold_tf = [] \n",
    "Y_pred_tf = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            sent_vector = get_tf_idf_word_importance(file)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            sent_vector = NormalizeTfIdfData(sent_vector)\n",
    "            for i in range(0,len(scores)):\n",
    "                if len(sent_vector[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    Y_gold_tf += make_scalar(scores[i], 0)\n",
    "                    Y_pred_tf += make_scalar(sent_vector[i], 0)\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        \n",
    "                        if compare(float(sent_vector[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "                            TF_IDF_MSE = TF_IDF_MSE + pow((float(sent_vector[i][j])-float(scores[i][j])),2)\n",
    "            accuracy = correct/total\n",
    "            TF_IDF_Accuracy.append(accuracy)\n",
    "            print(\"Document Processed- \"+str(len(TF_IDF_Accuracy))+\"/44: Accuracy %.2f\"%accuracy)\n",
    "\n",
    "print(\"SE:\" +str(TF_IDF_MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_gold_tf, density=True, bins=10)  # density=False would make counts\n",
    "\n",
    "\n",
    "plt.hist(Y_pred_tf, density=True, bins=10)  # density=False would make counts\n",
    "plt.ylabel('Data')\n",
    "plt.xlabel('Weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      0.88      0.36      5955\n",
      "           2       0.41      0.12      0.19      9461\n",
      "           3       0.22      0.01      0.02      4926\n",
      "           4       0.05      0.00      0.00      2571\n",
      "           5       0.00      0.00      0.00      2360\n",
      "           6       0.00      0.00      0.00       869\n",
      "\n",
      "    accuracy                           0.25     26142\n",
      "   macro avg       0.15      0.17      0.10     26142\n",
      "weighted avg       0.25      0.25      0.15     26142\n",
      "\n",
      "Accuracy: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5265,  592,   64,   19,    9,    6],\n",
       "       [8196, 1135,   95,   27,    4,    4],\n",
       "       [4254,  603,   51,   11,    6,    1],\n",
       "       [2358,  196,   12,    3,    1,    1],\n",
       "       [2176,  172,    7,    3,    0,    2],\n",
       "       [ 810,   56,    3,    0,    0,    0]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(Y_gold_tf,Y_pred_tf))\n",
    "print(\"Accuracy: %.2f\"%accuracy_score(Y_gold_tf, Y_pred_tf))\n",
    "cm = make_confusion_matrices(Y_gold_tf, Y_pred_tf, 1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Incorporating POS with Word Embedding in generating Word Importance \n",
    "The training algorithm that has been used to define the word vector is Continuous Bag of Words(CBOW).\n",
    "\n",
    "Importance of various POS is subjective and depends on the application as well as the domain under consideration, it can be very useful to evaluate their importance even in a general setup. In this paper(http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.599.3242), researchers present a study to understand this importance. \n",
    "\n",
    "We have used 1st step of Neural Bag-of-word2. \n",
    "\n",
    "Composition(Summarizing word vectors) of word embedding may refer to semantic or syntactic meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Building Word-Embedding using Word2Vec \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(all_words):\n",
    "    word2vec = Word2Vec(all_words, min_count=2, size= wem_vector_size, workers=3, window = 10)\n",
    "    vocabulary = word2vec.wv.vocab\n",
    "    WV=np.zeros(shape=(len(vocabulary),wem_vector_size))\n",
    "    count=0\n",
    "    for voc in vocabulary.keys():\n",
    "        WV[count] = word2vec.wv[voc]\n",
    "        count = count + 1\n",
    "    return (word2vec,vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_imp(vocabulary, all_words, pos_tagged):\n",
    "    POS_IMP={}\n",
    "    count=0\n",
    "    for voc in vocabulary.keys():\n",
    "        if voc in all_words[0]:\n",
    "            for sent_pos in pos_tagged:\n",
    "                for word, pos in sent_pos:\n",
    "                    if word == voc:\n",
    "                        POS_IMP[voc] = POS_IMPORTANCE.get(pos,2.5)\n",
    "                        break\n",
    "        else:\n",
    "            POS_IMP[voc] = 2.5\n",
    "\n",
    "        count=count+1\n",
    "    return POS_IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_matrix(vocabulary, all_words, POS_IMP,word2vec):\n",
    "    BOW=np.zeros(shape=(len(all_words),1))\n",
    "    WV=np.zeros(shape=(len(all_words),wem_vector_size))\n",
    "    POSA = np.zeros(shape=(len(all_words),1))\n",
    "    count=0\n",
    "    for word in all_words:\n",
    "        if word in  vocabulary.keys():\n",
    "            BOW[count] = [1.0]\n",
    "            WV[count] = word2vec.wv[word]\n",
    "            POSA[count] = [POS_IMP[word]]\n",
    "        else:\n",
    "            BOW[count] = [0]\n",
    "        count=count+1\n",
    "\n",
    "    return (BOW, WV, POSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "#     print(\"data::\",data)\n",
    "    min_v = 0.01\n",
    "    max_v = 0\n",
    "    for sentence_score in data:\n",
    "        for words_score in sentence_score:\n",
    "                min_v = min(min_v, words_score)\n",
    "                max_v = max(max_v, words_score)\n",
    "    data = [(x-min_v)/(max_v-min_v) if len(x)>1 else [0] for x in data]\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:54.322254 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:54.323143 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:54.323552 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:54.324129 140399148152640 word2vec.py:1405] collected 285 word types from a corpus of 1092 raw words and 73 sentences\n",
      "I1208 09:54:54.324495 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:54.325105 140399148152640 word2vec.py:1480] effective_min_count=2 retains 135 unique words (47% of original 285, drops 150)\n",
      "I1208 09:54:54.325448 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 942 word corpus (86% of original 1092, drops 150)\n",
      "I1208 09:54:54.326331 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 285 items\n",
      "I1208 09:54:54.326695 140399148152640 word2vec.py:1550] sample=0.001 downsamples 82 most-common words\n",
      "I1208 09:54:54.327041 140399148152640 word2vec.py:1551] downsampling leaves estimated 414 word corpus (44.0% of prior 942)\n",
      "I1208 09:54:54.327608 140399148152640 base_any2vec.py:1006] estimated required memory for 135 words and 50 dimensions: 121500 bytes\n",
      "I1208 09:54:54.327976 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:54.363902 140399148152640 base_any2vec.py:1192] training model with 3 workers on 135 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:54.365775 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.366167 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.366590 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.366956 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 1092 raw words (408 effective words) took 0.0s, 255611 effective words/s\n",
      "I1208 09:54:54.368775 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.369168 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.369835 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.370190 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 1092 raw words (423 effective words) took 0.0s, 231506 effective words/s\n",
      "I1208 09:54:54.371768 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.372135 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.372795 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.373138 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 1092 raw words (407 effective words) took 0.0s, 232680 effective words/s\n",
      "I1208 09:54:54.374768 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.375168 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.375600 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.375935 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 1092 raw words (407 effective words) took 0.0s, 260375 effective words/s\n",
      "I1208 09:54:54.377678 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.378087 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.378526 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.378858 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 1092 raw words (387 effective words) took 0.0s, 248881 effective words/s\n",
      "I1208 09:54:54.379210 140399148152640 base_any2vec.py:1366] training on a 5460 raw words (2032 effective words) took 0.0s, 136946 effective words/s\n",
      "W1208 09:54:54.379539 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:54.606600 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:54.607363 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:54.607755 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:54.608235 140399148152640 word2vec.py:1405] collected 188 word types from a corpus of 540 raw words and 75 sentences\n",
      "I1208 09:54:54.608583 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:54.609098 140399148152640 word2vec.py:1480] effective_min_count=2 retains 83 unique words (44% of original 188, drops 105)\n",
      "I1208 09:54:54.609440 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 435 word corpus (80% of original 540, drops 105)\n",
      "I1208 09:54:54.610180 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 188 items\n",
      "I1208 09:54:54.610543 140399148152640 word2vec.py:1550] sample=0.001 downsamples 83 most-common words\n",
      "I1208 09:54:54.610886 140399148152640 word2vec.py:1551] downsampling leaves estimated 150 word corpus (34.6% of prior 435)\n",
      "I1208 09:54:54.611381 140399148152640 base_any2vec.py:1006] estimated required memory for 83 words and 50 dimensions: 74700 bytes\n",
      "I1208 09:54:54.611732 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:54.633236 140399148152640 base_any2vec.py:1192] training model with 3 workers on 83 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:54.635848 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 1/44: Accuracy 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:54.636434 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.636863 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.637228 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 540 raw words (149 effective words) took 0.0s, 88716 effective words/s\n",
      "I1208 09:54:54.639025 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.639488 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.639848 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.640188 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 540 raw words (138 effective words) took 0.0s, 94106 effective words/s\n",
      "I1208 09:54:54.641876 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.642353 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.642710 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.643055 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 540 raw words (138 effective words) took 0.0s, 93319 effective words/s\n",
      "I1208 09:54:54.644739 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.645208 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.645569 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.645912 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 540 raw words (129 effective words) took 0.0s, 88792 effective words/s\n",
      "I1208 09:54:54.647566 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.648046 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.648392 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.648734 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 540 raw words (152 effective words) took 0.0s, 106428 effective words/s\n",
      "I1208 09:54:54.649090 140399148152640 base_any2vec.py:1366] training on a 2700 raw words (706 effective words) took 0.0s, 45741 effective words/s\n",
      "W1208 09:54:54.649423 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:54.866549 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:54.867321 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:54.867723 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:54.868227 140399148152640 word2vec.py:1405] collected 244 word types from a corpus of 678 raw words and 81 sentences\n",
      "I1208 09:54:54.868571 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:54.869130 140399148152640 word2vec.py:1480] effective_min_count=2 retains 102 unique words (41% of original 244, drops 142)\n",
      "I1208 09:54:54.869475 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 536 word corpus (79% of original 678, drops 142)\n",
      "I1208 09:54:54.870330 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 244 items\n",
      "I1208 09:54:54.870696 140399148152640 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1208 09:54:54.871049 140399148152640 word2vec.py:1551] downsampling leaves estimated 210 word corpus (39.2% of prior 536)\n",
      "I1208 09:54:54.871558 140399148152640 base_any2vec.py:1006] estimated required memory for 102 words and 50 dimensions: 91800 bytes\n",
      "I1208 09:54:54.871912 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:54.898189 140399148152640 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 2/44: Accuracy 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:54.901003 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.901419 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.901873 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.902224 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 678 raw words (207 effective words) took 0.0s, 134907 effective words/s\n",
      "I1208 09:54:54.903932 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.904336 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.904777 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.905130 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 678 raw words (198 effective words) took 0.0s, 130752 effective words/s\n",
      "I1208 09:54:54.906795 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.907190 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.907609 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.907974 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 678 raw words (199 effective words) took 0.0s, 134843 effective words/s\n",
      "I1208 09:54:54.909632 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.910033 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.910455 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.910810 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 678 raw words (224 effective words) took 0.0s, 151879 effective words/s\n",
      "I1208 09:54:54.912518 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:54.912937 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:54.913375 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:54.913720 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 678 raw words (214 effective words) took 0.0s, 143007 effective words/s\n",
      "I1208 09:54:54.914076 140399148152640 base_any2vec.py:1366] training on a 3390 raw words (1042 effective words) took 0.0s, 67928 effective words/s\n",
      "W1208 09:54:54.914408 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 3/44: Accuracy 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:55.264873 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:55.265591 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:55.265990 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:55.266612 140399148152640 word2vec.py:1405] collected 356 word types from a corpus of 1421 raw words and 128 sentences\n",
      "I1208 09:54:55.266988 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:55.267647 140399148152640 word2vec.py:1480] effective_min_count=2 retains 169 unique words (47% of original 356, drops 187)\n",
      "I1208 09:54:55.268014 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 1234 word corpus (86% of original 1421, drops 187)\n",
      "I1208 09:54:55.268981 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 356 items\n",
      "I1208 09:54:55.269342 140399148152640 word2vec.py:1550] sample=0.001 downsamples 86 most-common words\n",
      "I1208 09:54:55.269674 140399148152640 word2vec.py:1551] downsampling leaves estimated 606 word corpus (49.1% of prior 1234)\n",
      "I1208 09:54:55.270309 140399148152640 base_any2vec.py:1006] estimated required memory for 169 words and 50 dimensions: 152100 bytes\n",
      "I1208 09:54:55.270660 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:55.315329 140399148152640 base_any2vec.py:1192] training model with 3 workers on 169 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:55.317136 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.317519 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.318259 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.318616 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 1421 raw words (593 effective words) took 0.0s, 302123 effective words/s\n",
      "I1208 09:54:55.320346 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.320742 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.321501 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.321857 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 1421 raw words (605 effective words) took 0.0s, 305011 effective words/s\n",
      "I1208 09:54:55.323594 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.324000 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.324759 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.325108 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 1421 raw words (605 effective words) took 0.0s, 308481 effective words/s\n",
      "I1208 09:54:55.326823 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.327228 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.327985 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.328328 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 1421 raw words (610 effective words) took 0.0s, 308649 effective words/s\n",
      "I1208 09:54:55.330042 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.330431 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.331216 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.331557 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 1421 raw words (620 effective words) took 0.0s, 311564 effective words/s\n",
      "I1208 09:54:55.331925 140399148152640 base_any2vec.py:1366] training on a 7105 raw words (3033 effective words) took 0.0s, 187598 effective words/s\n",
      "W1208 09:54:55.332266 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:55.464738 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:55.465498 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:55.465903 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:55.466316 140399148152640 word2vec.py:1405] collected 104 word types from a corpus of 237 raw words and 21 sentences\n",
      "I1208 09:54:55.466665 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:55.467116 140399148152640 word2vec.py:1480] effective_min_count=2 retains 43 unique words (41% of original 104, drops 61)\n",
      "I1208 09:54:55.467457 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 176 word corpus (74% of original 237, drops 61)\n",
      "I1208 09:54:55.468016 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 104 items\n",
      "I1208 09:54:55.468397 140399148152640 word2vec.py:1550] sample=0.001 downsamples 43 most-common words\n",
      "I1208 09:54:55.468749 140399148152640 word2vec.py:1551] downsampling leaves estimated 41 word corpus (23.8% of prior 176)\n",
      "I1208 09:54:55.469181 140399148152640 base_any2vec.py:1006] estimated required memory for 43 words and 50 dimensions: 38700 bytes\n",
      "I1208 09:54:55.469521 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:55.480874 140399148152640 base_any2vec.py:1192] training model with 3 workers on 43 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:55.483278 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.483731 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.484098 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.484436 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 237 raw words (39 effective words) took 0.0s, 28188 effective words/s\n",
      "I1208 09:54:55.486005 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.486436 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.486792 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.487133 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 237 raw words (47 effective words) took 0.0s, 34785 effective words/s\n",
      "I1208 09:54:55.488674 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.489245 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.489606 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.489974 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 237 raw words (47 effective words) took 0.0s, 30958 effective words/s\n",
      "I1208 09:54:55.491518 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.492449 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.492820 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.493169 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 237 raw words (44 effective words) took 0.0s, 23430 effective words/s\n",
      "I1208 09:54:55.494732 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.495192 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.495549 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.495895 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 237 raw words (36 effective words) took 0.0s, 25859 effective words/s\n",
      "I1208 09:54:55.496255 140399148152640 base_any2vec.py:1366] training on a 1185 raw words (213 effective words) took 0.0s, 15365 effective words/s\n",
      "W1208 09:54:55.496594 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 4/44: Accuracy 0.75\n",
      "Document Processed- 5/44: Accuracy 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:55.676004 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:55.676728 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:55.677112 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:55.677599 140399148152640 word2vec.py:1405] collected 238 word types from a corpus of 706 raw words and 64 sentences\n",
      "I1208 09:54:55.677999 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:55.678518 140399148152640 word2vec.py:1480] effective_min_count=2 retains 100 unique words (42% of original 238, drops 138)\n",
      "I1208 09:54:55.678864 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 568 word corpus (80% of original 706, drops 138)\n",
      "I1208 09:54:55.679672 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 238 items\n",
      "I1208 09:54:55.680044 140399148152640 word2vec.py:1550] sample=0.001 downsamples 100 most-common words\n",
      "I1208 09:54:55.680386 140399148152640 word2vec.py:1551] downsampling leaves estimated 221 word corpus (38.9% of prior 568)\n",
      "I1208 09:54:55.680892 140399148152640 base_any2vec.py:1006] estimated required memory for 100 words and 50 dimensions: 90000 bytes\n",
      "I1208 09:54:55.681245 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:55.707104 140399148152640 base_any2vec.py:1192] training model with 3 workers on 100 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:55.709784 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.710243 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.710610 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.710970 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 706 raw words (222 effective words) took 0.0s, 147902 effective words/s\n",
      "I1208 09:54:55.712709 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.713209 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.713632 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.713984 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 706 raw words (231 effective words) took 0.0s, 143536 effective words/s\n",
      "I1208 09:54:55.715673 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.716195 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.716545 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.716890 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 706 raw words (223 effective words) took 0.0s, 147450 effective words/s\n",
      "I1208 09:54:55.718532 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.718997 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.719338 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.719669 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 706 raw words (225 effective words) took 0.0s, 157280 effective words/s\n",
      "I1208 09:54:55.721349 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.721776 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.722209 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.722549 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 706 raw words (220 effective words) took 0.0s, 147607 effective words/s\n",
      "I1208 09:54:55.722908 140399148152640 base_any2vec.py:1366] training on a 3530 raw words (1121 effective words) took 0.0s, 73313 effective words/s\n",
      "W1208 09:54:55.723260 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:55.957195 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:55.957938 140399148152640 word2vec.py:1399] collecting all words and their counts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 6/44: Accuracy 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:55.958442 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:55.958948 140399148152640 word2vec.py:1405] collected 175 word types from a corpus of 489 raw words and 94 sentences\n",
      "I1208 09:54:55.959292 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:55.959785 140399148152640 word2vec.py:1480] effective_min_count=2 retains 74 unique words (42% of original 175, drops 101)\n",
      "I1208 09:54:55.960139 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 388 word corpus (79% of original 489, drops 101)\n",
      "I1208 09:54:55.960835 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 175 items\n",
      "I1208 09:54:55.961191 140399148152640 word2vec.py:1550] sample=0.001 downsamples 74 most-common words\n",
      "I1208 09:54:55.961519 140399148152640 word2vec.py:1551] downsampling leaves estimated 123 word corpus (32.0% of prior 388)\n",
      "I1208 09:54:55.962019 140399148152640 base_any2vec.py:1006] estimated required memory for 74 words and 50 dimensions: 66600 bytes\n",
      "I1208 09:54:55.962358 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:55.982826 140399148152640 base_any2vec.py:1192] training model with 3 workers on 74 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:55.984362 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.984834 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.985199 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.985533 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 489 raw words (117 effective words) took 0.0s, 81319 effective words/s\n",
      "I1208 09:54:55.987226 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.987667 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.988046 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.988373 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 489 raw words (128 effective words) took 0.0s, 91432 effective words/s\n",
      "I1208 09:54:55.990061 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.990533 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.990895 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.991236 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 489 raw words (118 effective words) took 0.0s, 80983 effective words/s\n",
      "I1208 09:54:55.992868 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.993339 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.993698 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.994041 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 489 raw words (128 effective words) took 0.0s, 89169 effective words/s\n",
      "I1208 09:54:55.995663 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:55.996150 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:55.996507 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:55.996859 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 489 raw words (131 effective words) took 0.0s, 90568 effective words/s\n",
      "I1208 09:54:55.997210 140399148152640 base_any2vec.py:1366] training on a 2445 raw words (622 effective words) took 0.0s, 44488 effective words/s\n",
      "W1208 09:54:55.997539 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 7/44: Accuracy 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:56.315981 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:56.316710 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:56.317107 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:56.317665 140399148152640 word2vec.py:1405] collected 278 word types from a corpus of 1046 raw words and 127 sentences\n",
      "I1208 09:54:56.318055 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:56.318656 140399148152640 word2vec.py:1480] effective_min_count=2 retains 141 unique words (50% of original 278, drops 137)\n",
      "I1208 09:54:56.319012 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 909 word corpus (86% of original 1046, drops 137)\n",
      "I1208 09:54:56.319919 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 278 items\n",
      "I1208 09:54:56.320295 140399148152640 word2vec.py:1550] sample=0.001 downsamples 99 most-common words\n",
      "I1208 09:54:56.320624 140399148152640 word2vec.py:1551] downsampling leaves estimated 422 word corpus (46.4% of prior 909)\n",
      "I1208 09:54:56.321216 140399148152640 base_any2vec.py:1006] estimated required memory for 141 words and 50 dimensions: 126900 bytes\n",
      "I1208 09:54:56.321564 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:56.359068 140399148152640 base_any2vec.py:1192] training model with 3 workers on 141 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:56.360783 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.361171 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.361636 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.361997 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 1046 raw words (411 effective words) took 0.0s, 255288 effective words/s\n",
      "I1208 09:54:56.363854 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.364259 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.364721 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.365057 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 1046 raw words (418 effective words) took 0.0s, 261772 effective words/s\n",
      "I1208 09:54:56.366914 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.367348 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.367810 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.368187 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 1046 raw words (420 effective words) took 0.0s, 250477 effective words/s\n",
      "I1208 09:54:56.370035 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.370433 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.371095 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.371424 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 1046 raw words (423 effective words) took 0.0s, 236011 effective words/s\n",
      "I1208 09:54:56.373118 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.373501 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.374169 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.374505 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 1046 raw words (446 effective words) took 0.0s, 248039 effective words/s\n",
      "I1208 09:54:56.375108 140399148152640 base_any2vec.py:1366] training on a 5230 raw words (2118 effective words) took 0.0s, 135231 effective words/s\n",
      "W1208 09:54:56.375439 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 8/44: Accuracy 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:56.668441 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:56.669203 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:56.669579 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:56.670130 140399148152640 word2vec.py:1405] collected 288 word types from a corpus of 905 raw words and 98 sentences\n",
      "I1208 09:54:56.670496 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:56.671109 140399148152640 word2vec.py:1480] effective_min_count=2 retains 140 unique words (48% of original 288, drops 148)\n",
      "I1208 09:54:56.671449 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 757 word corpus (83% of original 905, drops 148)\n",
      "I1208 09:54:56.672538 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 288 items\n",
      "I1208 09:54:56.672905 140399148152640 word2vec.py:1550] sample=0.001 downsamples 140 most-common words\n",
      "I1208 09:54:56.673248 140399148152640 word2vec.py:1551] downsampling leaves estimated 360 word corpus (47.6% of prior 757)\n",
      "I1208 09:54:56.673828 140399148152640 base_any2vec.py:1006] estimated required memory for 140 words and 50 dimensions: 126000 bytes\n",
      "I1208 09:54:56.674175 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:56.711171 140399148152640 base_any2vec.py:1192] training model with 3 workers on 140 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:56.712871 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.713265 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.713742 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.714092 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 905 raw words (339 effective words) took 0.0s, 213133 effective words/s\n",
      "I1208 09:54:56.715901 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.716293 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.716972 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.717312 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 905 raw words (360 effective words) took 0.0s, 201474 effective words/s\n",
      "I1208 09:54:56.718880 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.719272 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.719958 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.720290 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 905 raw words (337 effective words) took 0.0s, 192026 effective words/s\n",
      "I1208 09:54:56.721873 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.722236 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.722899 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.723243 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 905 raw words (368 effective words) took 0.0s, 212808 effective words/s\n",
      "I1208 09:54:56.724839 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.725235 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.725708 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.726041 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 905 raw words (356 effective words) took 0.0s, 228488 effective words/s\n",
      "I1208 09:54:56.726383 140399148152640 base_any2vec.py:1366] training on a 4525 raw words (1760 effective words) took 0.0s, 118822 effective words/s\n",
      "W1208 09:54:56.726722 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:56.940437 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:56.941179 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:56.941555 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:56.942077 140399148152640 word2vec.py:1405] collected 229 word types from a corpus of 762 raw words and 65 sentences\n",
      "I1208 09:54:56.942421 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:56.942979 140399148152640 word2vec.py:1480] effective_min_count=2 retains 116 unique words (50% of original 229, drops 113)\n",
      "I1208 09:54:56.943318 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 649 word corpus (85% of original 762, drops 113)\n",
      "I1208 09:54:56.944203 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 229 items\n",
      "I1208 09:54:56.944565 140399148152640 word2vec.py:1550] sample=0.001 downsamples 116 most-common words\n",
      "I1208 09:54:56.944910 140399148152640 word2vec.py:1551] downsampling leaves estimated 273 word corpus (42.2% of prior 649)\n",
      "I1208 09:54:56.945451 140399148152640 base_any2vec.py:1006] estimated required memory for 116 words and 50 dimensions: 104400 bytes\n",
      "I1208 09:54:56.945795 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:56.975573 140399148152640 base_any2vec.py:1192] training model with 3 workers on 116 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 9/44: Accuracy 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:56.978436 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.978829 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.979326 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.979665 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 762 raw words (261 effective words) took 0.0s, 167901 effective words/s\n",
      "I1208 09:54:56.981400 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.981806 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.982295 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.982623 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 762 raw words (276 effective words) took 0.0s, 177924 effective words/s\n",
      "I1208 09:54:56.984314 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.984717 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.985149 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.985484 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 762 raw words (281 effective words) took 0.0s, 188331 effective words/s\n",
      "I1208 09:54:56.987203 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.987585 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.988030 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.988365 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 762 raw words (275 effective words) took 0.0s, 179726 effective words/s\n",
      "I1208 09:54:56.990089 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:56.990469 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:56.990891 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:56.991249 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 762 raw words (261 effective words) took 0.0s, 175436 effective words/s\n",
      "I1208 09:54:56.991593 140399148152640 base_any2vec.py:1366] training on a 3810 raw words (1354 effective words) took 0.0s, 87759 effective words/s\n",
      "W1208 09:54:56.991942 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:57.184981 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:57.185839 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:57.186239 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:57.186773 140399148152640 word2vec.py:1405] collected 199 word types from a corpus of 781 raw words and 58 sentences\n",
      "I1208 09:54:57.187138 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:57.187675 140399148152640 word2vec.py:1480] effective_min_count=2 retains 104 unique words (52% of original 199, drops 95)\n",
      "I1208 09:54:57.188051 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 686 word corpus (87% of original 781, drops 95)\n",
      "I1208 09:54:57.188888 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 199 items\n",
      "I1208 09:54:57.189266 140399148152640 word2vec.py:1550] sample=0.001 downsamples 104 most-common words\n",
      "I1208 09:54:57.189596 140399148152640 word2vec.py:1551] downsampling leaves estimated 267 word corpus (39.1% of prior 686)\n",
      "I1208 09:54:57.190137 140399148152640 base_any2vec.py:1006] estimated required memory for 104 words and 50 dimensions: 93600 bytes\n",
      "I1208 09:54:57.190487 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:57.217255 140399148152640 base_any2vec.py:1192] training model with 3 workers on 104 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:57.219883 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.220278 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.220726 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.221083 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 781 raw words (260 effective words) took 0.0s, 168423 effective words/s\n",
      "I1208 09:54:57.222866 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.223290 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.223745 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.224096 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 781 raw words (246 effective words) took 0.0s, 157399 effective words/s\n",
      "I1208 09:54:57.225766 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.226173 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.226620 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.226981 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 781 raw words (258 effective words) took 0.0s, 168425 effective words/s\n",
      "I1208 09:54:57.228624 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.229034 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.229490 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.229845 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 781 raw words (278 effective words) took 0.0s, 180405 effective words/s\n",
      "I1208 09:54:57.231521 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 10/44: Accuracy 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:57.232054 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.232504 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.232864 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 781 raw words (281 effective words) took 0.0s, 169982 effective words/s\n",
      "I1208 09:54:57.233222 140399148152640 base_any2vec.py:1366] training on a 3905 raw words (1323 effective words) took 0.0s, 91692 effective words/s\n",
      "W1208 09:54:57.233554 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:57.449841 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:57.450572 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:57.450973 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:57.451441 140399148152640 word2vec.py:1405] collected 214 word types from a corpus of 584 raw words and 77 sentences\n",
      "I1208 09:54:57.451802 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:57.452318 140399148152640 word2vec.py:1480] effective_min_count=2 retains 89 unique words (41% of original 214, drops 125)\n",
      "I1208 09:54:57.452657 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 459 word corpus (78% of original 584, drops 125)\n",
      "I1208 09:54:57.453471 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 214 items\n",
      "I1208 09:54:57.453820 140399148152640 word2vec.py:1550] sample=0.001 downsamples 89 most-common words\n",
      "I1208 09:54:57.454163 140399148152640 word2vec.py:1551] downsampling leaves estimated 166 word corpus (36.2% of prior 459)\n",
      "I1208 09:54:57.454654 140399148152640 base_any2vec.py:1006] estimated required memory for 89 words and 50 dimensions: 80100 bytes\n",
      "I1208 09:54:57.455021 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:57.477931 140399148152640 base_any2vec.py:1192] training model with 3 workers on 89 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 11/44: Accuracy 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:57.480665 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.481143 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.481507 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.481861 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 584 raw words (156 effective words) took 0.0s, 105839 effective words/s\n",
      "I1208 09:54:57.483576 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.484095 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.484448 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.484798 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 584 raw words (185 effective words) took 0.0s, 122398 effective words/s\n",
      "I1208 09:54:57.486434 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.486909 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.487266 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.487599 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 584 raw words (151 effective words) took 0.0s, 105055 effective words/s\n",
      "I1208 09:54:57.489273 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.489772 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.490139 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.490479 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 584 raw words (183 effective words) took 0.0s, 122002 effective words/s\n",
      "I1208 09:54:57.492154 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.492634 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.493018 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.493360 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 584 raw words (177 effective words) took 0.0s, 117308 effective words/s\n",
      "I1208 09:54:57.493718 140399148152640 base_any2vec.py:1366] training on a 2920 raw words (852 effective words) took 0.0s, 55835 effective words/s\n",
      "W1208 09:54:57.494066 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:57.683081 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:57.683796 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:57.684184 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:57.684693 140399148152640 word2vec.py:1405] collected 232 word types from a corpus of 688 raw words and 62 sentences\n",
      "I1208 09:54:57.685060 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:57.685599 140399148152640 word2vec.py:1480] effective_min_count=2 retains 107 unique words (46% of original 232, drops 125)\n",
      "I1208 09:54:57.685955 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 563 word corpus (81% of original 688, drops 125)\n",
      "I1208 09:54:57.686793 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 232 items\n",
      "I1208 09:54:57.687186 140399148152640 word2vec.py:1550] sample=0.001 downsamples 107 most-common words\n",
      "I1208 09:54:57.687520 140399148152640 word2vec.py:1551] downsampling leaves estimated 227 word corpus (40.5% of prior 563)\n",
      "I1208 09:54:57.688068 140399148152640 base_any2vec.py:1006] estimated required memory for 107 words and 50 dimensions: 96300 bytes\n",
      "I1208 09:54:57.688421 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:57.716195 140399148152640 base_any2vec.py:1192] training model with 3 workers on 107 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:57.718861 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.719285 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.719734 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.720096 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 688 raw words (232 effective words) took 0.0s, 148678 effective words/s\n",
      "I1208 09:54:57.721785 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.722285 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.722629 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.722978 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 688 raw words (224 effective words) took 0.0s, 148755 effective words/s\n",
      "I1208 09:54:57.724657 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.725100 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 12/44: Accuracy 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:57.725641 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.726029 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 688 raw words (210 effective words) took 0.0s, 126285 effective words/s\n",
      "I1208 09:54:57.727730 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.728130 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.728562 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.728912 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 688 raw words (238 effective words) took 0.0s, 158390 effective words/s\n",
      "I1208 09:54:57.730613 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.731176 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.731549 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.731919 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 688 raw words (203 effective words) took 0.0s, 126759 effective words/s\n",
      "I1208 09:54:57.732266 140399148152640 base_any2vec.py:1366] training on a 3440 raw words (1107 effective words) took 0.0s, 70944 effective words/s\n",
      "W1208 09:54:57.732598 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:57.923390 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:57.924139 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:57.924548 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:57.925001 140399148152640 word2vec.py:1405] collected 149 word types from a corpus of 448 raw words and 70 sentences\n",
      "I1208 09:54:57.925374 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:57.925879 140399148152640 word2vec.py:1480] effective_min_count=2 retains 76 unique words (51% of original 149, drops 73)\n",
      "I1208 09:54:57.926226 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 375 word corpus (83% of original 448, drops 73)\n",
      "I1208 09:54:57.926929 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 149 items\n",
      "I1208 09:54:57.927284 140399148152640 word2vec.py:1550] sample=0.001 downsamples 76 most-common words\n",
      "I1208 09:54:57.927618 140399148152640 word2vec.py:1551] downsampling leaves estimated 124 word corpus (33.3% of prior 375)\n",
      "I1208 09:54:57.928116 140399148152640 base_any2vec.py:1006] estimated required memory for 76 words and 50 dimensions: 68400 bytes\n",
      "I1208 09:54:57.928470 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:57.948116 140399148152640 base_any2vec.py:1192] training model with 3 workers on 76 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:57.950664 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.951138 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.951501 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.951847 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 448 raw words (120 effective words) took 0.0s, 83047 effective words/s\n",
      "I1208 09:54:57.953521 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.954009 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.954385 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.954728 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 448 raw words (137 effective words) took 0.0s, 93079 effective words/s\n",
      "I1208 09:54:57.956345 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.956809 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.957185 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.957510 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 448 raw words (116 effective words) took 0.0s, 82174 effective words/s\n",
      "I1208 09:54:57.959155 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.959639 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.960026 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.960368 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 448 raw words (125 effective words) took 0.0s, 84354 effective words/s\n",
      "I1208 09:54:57.962077 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:57.962545 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:57.962925 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:57.963274 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 448 raw words (129 effective words) took 0.0s, 87235 effective words/s\n",
      "I1208 09:54:57.963617 140399148152640 base_any2vec.py:1366] training on a 2240 raw words (627 effective words) took 0.0s, 41653 effective words/s\n",
      "W1208 09:54:57.963979 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 13/44: Accuracy 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:58.128223 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:58.128985 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:58.129372 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:58.129803 140399148152640 word2vec.py:1405] collected 118 word types from a corpus of 305 raw words and 66 sentences\n",
      "I1208 09:54:58.130179 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:58.130624 140399148152640 word2vec.py:1480] effective_min_count=2 retains 59 unique words (50% of original 118, drops 59)\n",
      "I1208 09:54:58.131009 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 246 word corpus (80% of original 305, drops 59)\n",
      "I1208 09:54:58.131615 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 118 items\n",
      "I1208 09:54:58.131978 140399148152640 word2vec.py:1550] sample=0.001 downsamples 59 most-common words\n",
      "I1208 09:54:58.132309 140399148152640 word2vec.py:1551] downsampling leaves estimated 70 word corpus (28.5% of prior 246)\n",
      "I1208 09:54:58.132764 140399148152640 base_any2vec.py:1006] estimated required memory for 59 words and 50 dimensions: 53100 bytes\n",
      "I1208 09:54:58.133107 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:58.148516 140399148152640 base_any2vec.py:1192] training model with 3 workers on 59 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:58.151027 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.151551 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.151933 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.152282 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 305 raw words (67 effective words) took 0.0s, 44264 effective words/s\n",
      "I1208 09:54:58.153919 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.154410 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.154797 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.155144 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 305 raw words (61 effective words) took 0.0s, 40959 effective words/s\n",
      "I1208 09:54:58.156751 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.157217 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.157579 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.157926 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 305 raw words (68 effective words) took 0.0s, 48222 effective words/s\n",
      "I1208 09:54:58.159526 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.160028 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.160408 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.160753 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 305 raw words (66 effective words) took 0.0s, 45431 effective words/s\n",
      "I1208 09:54:58.162333 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.162800 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.163163 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.163498 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 305 raw words (70 effective words) took 0.0s, 50248 effective words/s\n",
      "I1208 09:54:58.163883 140399148152640 base_any2vec.py:1366] training on a 1525 raw words (332 effective words) took 0.0s, 23992 effective words/s\n",
      "W1208 09:54:58.164230 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 14/44: Accuracy 0.07\n",
      "Document Processed- 15/44: Accuracy 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:58.319468 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:58.320218 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:58.320605 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:58.321064 140399148152640 word2vec.py:1405] collected 167 word types from a corpus of 377 raw words and 63 sentences\n",
      "I1208 09:54:58.321430 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:58.321908 140399148152640 word2vec.py:1480] effective_min_count=2 retains 67 unique words (40% of original 167, drops 100)\n",
      "I1208 09:54:58.322257 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 277 word corpus (73% of original 377, drops 100)\n",
      "I1208 09:54:58.322912 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 167 items\n",
      "I1208 09:54:58.323293 140399148152640 word2vec.py:1550] sample=0.001 downsamples 67 most-common words\n",
      "I1208 09:54:58.323618 140399148152640 word2vec.py:1551] downsampling leaves estimated 86 word corpus (31.1% of prior 277)\n",
      "I1208 09:54:58.324102 140399148152640 base_any2vec.py:1006] estimated required memory for 67 words and 50 dimensions: 60300 bytes\n",
      "I1208 09:54:58.324439 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:58.342961 140399148152640 base_any2vec.py:1192] training model with 3 workers on 67 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:58.344430 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.344887 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.345254 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.345584 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 377 raw words (91 effective words) took 0.0s, 64489 effective words/s\n",
      "I1208 09:54:58.347200 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.347632 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.348001 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.348333 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 377 raw words (99 effective words) took 0.0s, 72472 effective words/s\n",
      "I1208 09:54:58.349980 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.350454 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.350820 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.351167 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 377 raw words (95 effective words) took 0.0s, 65472 effective words/s\n",
      "I1208 09:54:58.352782 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.353254 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.353601 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.353948 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 377 raw words (77 effective words) took 0.0s, 54655 effective words/s\n",
      "I1208 09:54:58.355558 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.356040 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.356388 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.356722 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 377 raw words (76 effective words) took 0.0s, 54075 effective words/s\n",
      "I1208 09:54:58.357077 140399148152640 base_any2vec.py:1366] training on a 1885 raw words (438 effective words) took 0.0s, 31909 effective words/s\n",
      "W1208 09:54:58.357417 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:58.530841 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:58.531590 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:58.531986 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:58.532449 140399148152640 word2vec.py:1405] collected 186 word types from a corpus of 482 raw words and 68 sentences\n",
      "I1208 09:54:58.532811 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:58.533303 140399148152640 word2vec.py:1480] effective_min_count=2 retains 78 unique words (41% of original 186, drops 108)\n",
      "I1208 09:54:58.533639 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 374 word corpus (77% of original 482, drops 108)\n",
      "I1208 09:54:58.534347 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 186 items\n",
      "I1208 09:54:58.534715 140399148152640 word2vec.py:1550] sample=0.001 downsamples 78 most-common words\n",
      "I1208 09:54:58.535058 140399148152640 word2vec.py:1551] downsampling leaves estimated 126 word corpus (33.8% of prior 374)\n",
      "I1208 09:54:58.535538 140399148152640 base_any2vec.py:1006] estimated required memory for 78 words and 50 dimensions: 70200 bytes\n",
      "I1208 09:54:58.535886 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:58.556223 140399148152640 base_any2vec.py:1192] training model with 3 workers on 78 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:58.558833 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.559305 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.559663 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.560019 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 482 raw words (125 effective words) took 0.0s, 85722 effective words/s\n",
      "I1208 09:54:58.561695 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.562180 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.562537 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.562879 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 482 raw words (130 effective words) took 0.0s, 89048 effective words/s\n",
      "I1208 09:54:58.564544 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.565063 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.565415 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.565757 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 482 raw words (130 effective words) took 0.0s, 88742 effective words/s\n",
      "I1208 09:54:58.567355 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.567824 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.568182 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.568529 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 482 raw words (138 effective words) took 0.0s, 97185 effective words/s\n",
      "I1208 09:54:58.570159 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.570618 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.570988 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.571325 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 482 raw words (135 effective words) took 0.0s, 94177 effective words/s\n",
      "I1208 09:54:58.571662 140399148152640 base_any2vec.py:1366] training on a 2410 raw words (658 effective words) took 0.0s, 47789 effective words/s\n",
      "W1208 09:54:58.572021 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 16/44: Accuracy 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:58.779432 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:58.780199 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:58.780583 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:58.781090 140399148152640 word2vec.py:1405] collected 164 word types from a corpus of 520 raw words and 82 sentences\n",
      "I1208 09:54:58.781452 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:58.781951 140399148152640 word2vec.py:1480] effective_min_count=2 retains 75 unique words (45% of original 164, drops 89)\n",
      "I1208 09:54:58.782302 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 431 word corpus (82% of original 520, drops 89)\n",
      "I1208 09:54:58.782984 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 164 items\n",
      "I1208 09:54:58.783329 140399148152640 word2vec.py:1550] sample=0.001 downsamples 75 most-common words\n",
      "I1208 09:54:58.783659 140399148152640 word2vec.py:1551] downsampling leaves estimated 140 word corpus (32.5% of prior 431)\n",
      "I1208 09:54:58.784159 140399148152640 base_any2vec.py:1006] estimated required memory for 75 words and 50 dimensions: 67500 bytes\n",
      "I1208 09:54:58.784499 140399148152640 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 17/44: Accuracy 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:54:58.804241 140399148152640 base_any2vec.py:1192] training model with 3 workers on 75 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:58.805896 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.806366 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.806733 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.807074 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 520 raw words (134 effective words) took 0.0s, 91553 effective words/s\n",
      "I1208 09:54:58.808781 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.809261 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.809614 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.809960 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 520 raw words (133 effective words) took 0.0s, 90319 effective words/s\n",
      "I1208 09:54:58.811635 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.812110 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.812462 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.812815 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 520 raw words (125 effective words) took 0.0s, 86361 effective words/s\n",
      "I1208 09:54:58.814461 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.815032 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.815390 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.815730 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 520 raw words (134 effective words) took 0.0s, 87583 effective words/s\n",
      "I1208 09:54:58.817347 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:58.817855 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:58.818232 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:58.818571 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 520 raw words (129 effective words) took 0.0s, 86380 effective words/s\n",
      "I1208 09:54:58.818935 140399148152640 base_any2vec.py:1366] training on a 2600 raw words (655 effective words) took 0.0s, 46093 effective words/s\n",
      "W1208 09:54:58.819314 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:54:58.991181 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:58.991915 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:58.992306 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:58.992792 140399148152640 word2vec.py:1405] collected 237 word types from a corpus of 673 raw words and 57 sentences\n",
      "I1208 09:54:58.993167 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:58.993708 140399148152640 word2vec.py:1480] effective_min_count=2 retains 102 unique words (43% of original 237, drops 135)\n",
      "I1208 09:54:58.994067 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 538 word corpus (79% of original 673, drops 135)\n",
      "I1208 09:54:58.994879 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 237 items\n",
      "I1208 09:54:58.995242 140399148152640 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1208 09:54:58.995575 140399148152640 word2vec.py:1551] downsampling leaves estimated 211 word corpus (39.3% of prior 538)\n",
      "I1208 09:54:58.996107 140399148152640 base_any2vec.py:1006] estimated required memory for 102 words and 50 dimensions: 91800 bytes\n",
      "I1208 09:54:58.996448 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:59.023001 140399148152640 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:59.025637 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.026146 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.026501 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.026846 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 673 raw words (201 effective words) took 0.0s, 132486 effective words/s\n",
      "I1208 09:54:59.028582 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.029016 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.029459 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.029817 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 673 raw words (228 effective words) took 0.0s, 147595 effective words/s\n",
      "I1208 09:54:59.031499 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.031904 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.032340 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.032670 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 673 raw words (218 effective words) took 0.0s, 148122 effective words/s\n",
      "I1208 09:54:59.034345 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.034748 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.035169 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.035520 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 673 raw words (228 effective words) took 0.0s, 154263 effective words/s\n",
      "I1208 09:54:59.037163 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.037547 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.037994 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.038330 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 673 raw words (213 effective words) took 0.0s, 144957 effective words/s\n",
      "I1208 09:54:59.038667 140399148152640 base_any2vec.py:1366] training on a 3365 raw words (1088 effective words) took 0.0s, 77543 effective words/s\n",
      "W1208 09:54:59.039021 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 18/44: Accuracy 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:59.216244 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:59.217012 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:59.217410 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:59.217896 140399148152640 word2vec.py:1405] collected 207 word types from a corpus of 616 raw words and 56 sentences\n",
      "I1208 09:54:59.218275 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:59.218798 140399148152640 word2vec.py:1480] effective_min_count=2 retains 93 unique words (44% of original 207, drops 114)\n",
      "I1208 09:54:59.219150 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 502 word corpus (81% of original 616, drops 114)\n",
      "I1208 09:54:59.219933 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 207 items\n",
      "I1208 09:54:59.220293 140399148152640 word2vec.py:1550] sample=0.001 downsamples 93 most-common words\n",
      "I1208 09:54:59.220641 140399148152640 word2vec.py:1551] downsampling leaves estimated 184 word corpus (36.8% of prior 502)\n",
      "I1208 09:54:59.221215 140399148152640 base_any2vec.py:1006] estimated required memory for 93 words and 50 dimensions: 83700 bytes\n",
      "I1208 09:54:59.221543 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:59.245526 140399148152640 base_any2vec.py:1192] training model with 3 workers on 93 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:59.248240 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.248727 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.249098 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.249433 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 616 raw words (177 effective words) took 0.0s, 118031 effective words/s\n",
      "I1208 09:54:59.251098 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.251489 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.251927 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.252272 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 616 raw words (197 effective words) took 0.0s, 133324 effective words/s\n",
      "I1208 09:54:59.253917 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.254378 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.254759 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.255104 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 616 raw words (176 effective words) took 0.0s, 119585 effective words/s\n",
      "I1208 09:54:59.256717 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.257128 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.257556 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.257897 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 616 raw words (168 effective words) took 0.0s, 115129 effective words/s\n",
      "I1208 09:54:59.259522 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.259931 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.260360 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.260701 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 616 raw words (200 effective words) took 0.0s, 136361 effective words/s\n",
      "I1208 09:54:59.261055 140399148152640 base_any2vec.py:1366] training on a 3080 raw words (918 effective words) took 0.0s, 61272 effective words/s\n",
      "W1208 09:54:59.261399 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 19/44: Accuracy 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:59.437757 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:59.438487 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:59.438883 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:59.439365 140399148152640 word2vec.py:1405] collected 227 word types from a corpus of 566 raw words and 59 sentences\n",
      "I1208 09:54:59.439718 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:59.440242 140399148152640 word2vec.py:1480] effective_min_count=2 retains 90 unique words (39% of original 227, drops 137)\n",
      "I1208 09:54:59.440580 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 429 word corpus (75% of original 566, drops 137)\n",
      "I1208 09:54:59.441346 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 227 items\n",
      "I1208 09:54:59.441723 140399148152640 word2vec.py:1550] sample=0.001 downsamples 90 most-common words\n",
      "I1208 09:54:59.442056 140399148152640 word2vec.py:1551] downsampling leaves estimated 157 word corpus (36.7% of prior 429)\n",
      "I1208 09:54:59.442567 140399148152640 base_any2vec.py:1006] estimated required memory for 90 words and 50 dimensions: 81000 bytes\n",
      "I1208 09:54:59.442906 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:59.466336 140399148152640 base_any2vec.py:1192] training model with 3 workers on 90 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:59.468987 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.469452 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.469822 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.470168 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 566 raw words (160 effective words) took 0.0s, 108791 effective words/s\n",
      "I1208 09:54:59.471838 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.472311 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.472666 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.473021 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 566 raw words (162 effective words) took 0.0s, 110145 effective words/s\n",
      "I1208 09:54:59.474637 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.475131 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.475486 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.475836 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 566 raw words (159 effective words) took 0.0s, 108833 effective words/s\n",
      "I1208 09:54:59.477490 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.477992 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.478348 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.478678 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 566 raw words (154 effective words) took 0.0s, 104917 effective words/s\n",
      "I1208 09:54:59.480340 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.480886 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.481264 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.481602 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 566 raw words (154 effective words) took 0.0s, 100229 effective words/s\n",
      "I1208 09:54:59.481986 140399148152640 base_any2vec.py:1366] training on a 2830 raw words (789 effective words) took 0.0s, 56395 effective words/s\n",
      "W1208 09:54:59.482329 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 20/44: Accuracy 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:59.634703 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:59.635573 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:59.635962 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:59.636441 140399148152640 word2vec.py:1405] collected 222 word types from a corpus of 630 raw words and 44 sentences\n",
      "I1208 09:54:59.636806 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:59.637336 140399148152640 word2vec.py:1480] effective_min_count=2 retains 92 unique words (41% of original 222, drops 130)\n",
      "I1208 09:54:59.637708 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 500 word corpus (79% of original 630, drops 130)\n",
      "I1208 09:54:59.638493 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 222 items\n",
      "I1208 09:54:59.638872 140399148152640 word2vec.py:1550] sample=0.001 downsamples 92 most-common words\n",
      "I1208 09:54:59.639224 140399148152640 word2vec.py:1551] downsampling leaves estimated 182 word corpus (36.6% of prior 500)\n",
      "I1208 09:54:59.639745 140399148152640 base_any2vec.py:1006] estimated required memory for 92 words and 50 dimensions: 82800 bytes\n",
      "I1208 09:54:59.640134 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:54:59.663962 140399148152640 base_any2vec.py:1192] training model with 3 workers on 92 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:54:59.666465 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.666953 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.667326 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.667662 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 630 raw words (172 effective words) took 0.0s, 114972 effective words/s\n",
      "I1208 09:54:59.669353 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.669794 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.670255 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.670606 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 630 raw words (169 effective words) took 0.0s, 109257 effective words/s\n",
      "I1208 09:54:59.672303 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.672848 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.673243 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.673588 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 630 raw words (160 effective words) took 0.0s, 101636 effective words/s\n",
      "I1208 09:54:59.675276 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.675787 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.676181 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.676530 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 630 raw words (180 effective words) took 0.0s, 117286 effective words/s\n",
      "I1208 09:54:59.678190 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:54:59.678703 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:54:59.679096 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:54:59.679437 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 630 raw words (177 effective words) took 0.0s, 116021 effective words/s\n",
      "I1208 09:54:59.679806 140399148152640 base_any2vec.py:1366] training on a 3150 raw words (858 effective words) took 0.0s, 59782 effective words/s\n",
      "W1208 09:54:59.680163 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 21/44: Accuracy 0.24\n",
      "Document Processed- 22/44: Accuracy 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:54:59.981158 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:54:59.981895 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:54:59.982294 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:54:59.982811 140399148152640 word2vec.py:1405] collected 226 word types from a corpus of 711 raw words and 124 sentences\n",
      "I1208 09:54:59.983187 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:54:59.983726 140399148152640 word2vec.py:1480] effective_min_count=2 retains 100 unique words (44% of original 226, drops 126)\n",
      "I1208 09:54:59.984075 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 585 word corpus (82% of original 711, drops 126)\n",
      "I1208 09:54:59.984878 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 226 items\n",
      "I1208 09:54:59.985247 140399148152640 word2vec.py:1550] sample=0.001 downsamples 100 most-common words\n",
      "I1208 09:54:59.985576 140399148152640 word2vec.py:1551] downsampling leaves estimated 225 word corpus (38.6% of prior 585)\n",
      "I1208 09:54:59.986103 140399148152640 base_any2vec.py:1006] estimated required memory for 100 words and 50 dimensions: 90000 bytes\n",
      "I1208 09:54:59.986450 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:00.013296 140399148152640 base_any2vec.py:1192] training model with 3 workers on 100 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:00.014935 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.015341 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.015798 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.016146 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 711 raw words (215 effective words) took 0.0s, 139022 effective words/s\n",
      "I1208 09:55:00.017960 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.018385 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.018846 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.019196 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 711 raw words (221 effective words) took 0.0s, 140386 effective words/s\n",
      "I1208 09:55:00.020952 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.021503 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.021881 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.022225 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 711 raw words (220 effective words) took 0.0s, 138300 effective words/s\n",
      "I1208 09:55:00.023993 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.024402 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.024845 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.025185 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 711 raw words (208 effective words) took 0.0s, 138927 effective words/s\n",
      "I1208 09:55:00.026895 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.027272 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.027704 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.028053 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 711 raw words (236 effective words) took 0.0s, 159732 effective words/s\n",
      "I1208 09:55:00.028399 140399148152640 base_any2vec.py:1366] training on a 3555 raw words (1100 effective words) took 0.0s, 74788 effective words/s\n",
      "W1208 09:55:00.028747 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:00.229325 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:00.230050 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:00.230427 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:00.230891 140399148152640 word2vec.py:1405] collected 181 word types from a corpus of 474 raw words and 72 sentences\n",
      "I1208 09:55:00.231266 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:00.231756 140399148152640 word2vec.py:1480] effective_min_count=2 retains 75 unique words (41% of original 181, drops 106)\n",
      "I1208 09:55:00.232103 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 368 word corpus (77% of original 474, drops 106)\n",
      "I1208 09:55:00.232788 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 181 items\n",
      "I1208 09:55:00.233154 140399148152640 word2vec.py:1550] sample=0.001 downsamples 75 most-common words\n",
      "I1208 09:55:00.233480 140399148152640 word2vec.py:1551] downsampling leaves estimated 119 word corpus (32.6% of prior 368)\n",
      "I1208 09:55:00.233999 140399148152640 base_any2vec.py:1006] estimated required memory for 75 words and 50 dimensions: 67500 bytes\n",
      "I1208 09:55:00.234328 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:00.254843 140399148152640 base_any2vec.py:1192] training model with 3 workers on 75 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:00.256378 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.256846 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.257211 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.257558 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 474 raw words (112 effective words) took 0.0s, 77282 effective words/s\n",
      "I1208 09:55:00.259233 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.259702 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.260098 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.260420 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 474 raw words (120 effective words) took 0.0s, 81966 effective words/s\n",
      "I1208 09:55:00.262083 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.262571 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.262949 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.263291 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 474 raw words (105 effective words) took 0.0s, 70811 effective words/s\n",
      "I1208 09:55:00.264923 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.265390 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.265758 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.266103 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 474 raw words (125 effective words) took 0.0s, 86808 effective words/s\n",
      "I1208 09:55:00.267716 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.268193 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.268557 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.268900 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 474 raw words (117 effective words) took 0.0s, 81390 effective words/s\n",
      "I1208 09:55:00.269256 140399148152640 base_any2vec.py:1366] training on a 2370 raw words (579 effective words) took 0.0s, 41276 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 23/44: Accuracy 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:00.269718 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:00.492730 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:00.493457 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:00.493842 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 24/44: Accuracy 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:55:00.494322 140399148152640 word2vec.py:1405] collected 180 word types from a corpus of 451 raw words and 94 sentences\n",
      "I1208 09:55:00.494742 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:00.495269 140399148152640 word2vec.py:1480] effective_min_count=2 retains 72 unique words (40% of original 180, drops 108)\n",
      "I1208 09:55:00.495607 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 343 word corpus (76% of original 451, drops 108)\n",
      "I1208 09:55:00.496292 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 180 items\n",
      "I1208 09:55:00.496637 140399148152640 word2vec.py:1550] sample=0.001 downsamples 72 most-common words\n",
      "I1208 09:55:00.496991 140399148152640 word2vec.py:1551] downsampling leaves estimated 108 word corpus (31.7% of prior 343)\n",
      "I1208 09:55:00.497461 140399148152640 base_any2vec.py:1006] estimated required memory for 72 words and 50 dimensions: 64800 bytes\n",
      "I1208 09:55:00.497807 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:00.516425 140399148152640 base_any2vec.py:1192] training model with 3 workers on 72 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:00.519056 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.519523 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.519899 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.520240 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 451 raw words (104 effective words) took 0.0s, 71126 effective words/s\n",
      "I1208 09:55:00.521961 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.522443 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.522805 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.523145 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 451 raw words (109 effective words) took 0.0s, 73648 effective words/s\n",
      "I1208 09:55:00.524847 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.525400 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.525778 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.526128 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 451 raw words (116 effective words) took 0.0s, 74614 effective words/s\n",
      "I1208 09:55:00.527792 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.528300 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.528671 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.529042 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 451 raw words (110 effective words) took 0.0s, 72094 effective words/s\n",
      "I1208 09:55:00.530692 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.531168 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.531522 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.531867 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 451 raw words (121 effective words) took 0.0s, 84275 effective words/s\n",
      "I1208 09:55:00.532222 140399148152640 base_any2vec.py:1366] training on a 2255 raw words (560 effective words) took 0.0s, 39149 effective words/s\n",
      "W1208 09:55:00.532550 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 25/44: Accuracy 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:00.796694 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:00.797465 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:00.797874 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:00.798355 140399148152640 word2vec.py:1405] collected 169 word types from a corpus of 526 raw words and 115 sentences\n",
      "I1208 09:55:00.798736 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:00.799225 140399148152640 word2vec.py:1480] effective_min_count=2 retains 72 unique words (42% of original 169, drops 97)\n",
      "I1208 09:55:00.799560 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 429 word corpus (81% of original 526, drops 97)\n",
      "I1208 09:55:00.800250 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 169 items\n",
      "I1208 09:55:00.800616 140399148152640 word2vec.py:1550] sample=0.001 downsamples 72 most-common words\n",
      "I1208 09:55:00.800952 140399148152640 word2vec.py:1551] downsampling leaves estimated 134 word corpus (31.3% of prior 429)\n",
      "I1208 09:55:00.801424 140399148152640 base_any2vec.py:1006] estimated required memory for 72 words and 50 dimensions: 64800 bytes\n",
      "I1208 09:55:00.801774 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:00.820547 140399148152640 base_any2vec.py:1192] training model with 3 workers on 72 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:00.823203 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.823664 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.824051 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.824388 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 526 raw words (136 effective words) took 0.0s, 91763 effective words/s\n",
      "I1208 09:55:00.826123 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.826604 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.826977 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.827317 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 526 raw words (138 effective words) took 0.0s, 93142 effective words/s\n",
      "I1208 09:55:00.829034 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.829545 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.829920 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.830274 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 526 raw words (134 effective words) took 0.0s, 88354 effective words/s\n",
      "I1208 09:55:00.831948 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.832438 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.832826 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.833177 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 526 raw words (138 effective words) took 0.0s, 91929 effective words/s\n",
      "I1208 09:55:00.834827 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.835292 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.835648 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.836004 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 526 raw words (144 effective words) took 0.0s, 99528 effective words/s\n",
      "I1208 09:55:00.836346 140399148152640 base_any2vec.py:1366] training on a 2630 raw words (690 effective words) took 0.0s, 48174 effective words/s\n",
      "W1208 09:55:00.836694 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:00.913732 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:00.914549 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:00.914922 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:00.915318 140399148152640 word2vec.py:1405] collected 95 word types from a corpus of 166 raw words and 19 sentences\n",
      "I1208 09:55:00.915666 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:00.916098 140399148152640 word2vec.py:1480] effective_min_count=2 retains 34 unique words (35% of original 95, drops 61)\n",
      "I1208 09:55:00.916425 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 105 word corpus (63% of original 166, drops 61)\n",
      "I1208 09:55:00.916985 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 95 items\n",
      "I1208 09:55:00.917339 140399148152640 word2vec.py:1550] sample=0.001 downsamples 34 most-common words\n",
      "I1208 09:55:00.917697 140399148152640 word2vec.py:1551] downsampling leaves estimated 22 word corpus (21.2% of prior 105)\n",
      "I1208 09:55:00.918117 140399148152640 base_any2vec.py:1006] estimated required memory for 34 words and 50 dimensions: 30600 bytes\n",
      "I1208 09:55:00.918451 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:00.927573 140399148152640 base_any2vec.py:1192] training model with 3 workers on 34 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:00.929949 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.930396 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.930762 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.931102 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 166 raw words (26 effective words) took 0.0s, 18886 effective words/s\n",
      "I1208 09:55:00.932671 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.933159 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.933516 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.933885 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 166 raw words (22 effective words) took 0.0s, 15471 effective words/s\n",
      "I1208 09:55:00.935452 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.935948 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.936319 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.936649 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 166 raw words (17 effective words) took 0.0s, 12027 effective words/s\n",
      "I1208 09:55:00.938197 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.938635 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.939004 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.939333 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 166 raw words (20 effective words) took 0.0s, 14926 effective words/s\n",
      "I1208 09:55:00.940833 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:00.941271 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:00.941617 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:00.941979 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 166 raw words (26 effective words) took 0.0s, 19317 effective words/s\n",
      "I1208 09:55:00.942322 140399148152640 base_any2vec.py:1366] training on a 830 raw words (111 effective words) took 0.0s, 7747 effective words/s\n",
      "W1208 09:55:00.942658 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 26/44: Accuracy 0.16\n",
      "Document Processed- 27/44: Accuracy 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:01.136276 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:01.137008 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:01.137397 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:01.137853 140399148152640 word2vec.py:1405] collected 175 word types from a corpus of 396 raw words and 87 sentences\n",
      "I1208 09:55:01.138213 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:01.138698 140399148152640 word2vec.py:1480] effective_min_count=2 retains 68 unique words (38% of original 175, drops 107)\n",
      "I1208 09:55:01.139079 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 289 word corpus (72% of original 396, drops 107)\n",
      "I1208 09:55:01.139752 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 175 items\n",
      "I1208 09:55:01.140130 140399148152640 word2vec.py:1550] sample=0.001 downsamples 68 most-common words\n",
      "I1208 09:55:01.140459 140399148152640 word2vec.py:1551] downsampling leaves estimated 89 word corpus (31.0% of prior 289)\n",
      "I1208 09:55:01.140932 140399148152640 base_any2vec.py:1006] estimated required memory for 68 words and 50 dimensions: 61200 bytes\n",
      "I1208 09:55:01.141295 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:01.160019 140399148152640 base_any2vec.py:1192] training model with 3 workers on 68 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:01.161537 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.162013 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.162366 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.162704 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 396 raw words (91 effective words) took 0.0s, 63739 effective words/s\n",
      "I1208 09:55:01.164380 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.164858 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.165242 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.165574 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 396 raw words (94 effective words) took 0.0s, 65181 effective words/s\n",
      "I1208 09:55:01.167222 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.167659 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.168034 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.168363 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 396 raw words (96 effective words) took 0.0s, 69032 effective words/s\n",
      "I1208 09:55:01.169995 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.170537 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.170901 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.171248 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 396 raw words (95 effective words) took 0.0s, 63045 effective words/s\n",
      "I1208 09:55:01.172875 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.173346 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.173707 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.174052 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 396 raw words (98 effective words) took 0.0s, 68205 effective words/s\n",
      "I1208 09:55:01.174392 140399148152640 base_any2vec.py:1366] training on a 1980 raw words (474 effective words) took 0.0s, 33894 effective words/s\n",
      "W1208 09:55:01.174744 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 28/44: Accuracy 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:01.461332 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:01.462080 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:01.462460 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:01.463089 140399148152640 word2vec.py:1405] collected 406 word types from a corpus of 1336 raw words and 99 sentences\n",
      "I1208 09:55:01.463475 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:01.464162 140399148152640 word2vec.py:1480] effective_min_count=2 retains 172 unique words (42% of original 406, drops 234)\n",
      "I1208 09:55:01.464513 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 1102 word corpus (82% of original 1336, drops 234)\n",
      "I1208 09:55:01.465514 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 406 items\n",
      "I1208 09:55:01.465878 140399148152640 word2vec.py:1550] sample=0.001 downsamples 101 most-common words\n",
      "I1208 09:55:01.466223 140399148152640 word2vec.py:1551] downsampling leaves estimated 546 word corpus (49.6% of prior 1102)\n",
      "I1208 09:55:01.466844 140399148152640 base_any2vec.py:1006] estimated required memory for 172 words and 50 dimensions: 154800 bytes\n",
      "I1208 09:55:01.467207 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:01.511228 140399148152640 base_any2vec.py:1192] training model with 3 workers on 172 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:01.514033 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.514422 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.515090 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.515438 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 1336 raw words (528 effective words) took 0.0s, 284392 effective words/s\n",
      "I1208 09:55:01.517158 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.517560 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.518210 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.518543 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 1336 raw words (537 effective words) took 0.0s, 293475 effective words/s\n",
      "I1208 09:55:01.520244 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.520622 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.521298 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.521633 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 1336 raw words (550 effective words) took 0.0s, 301454 effective words/s\n",
      "I1208 09:55:01.523333 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.523756 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.524383 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.524708 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 1336 raw words (533 effective words) took 0.0s, 294764 effective words/s\n",
      "I1208 09:55:01.526385 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.526803 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.527417 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.527760 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 1336 raw words (534 effective words) took 0.0s, 297191 effective words/s\n",
      "I1208 09:55:01.528117 140399148152640 base_any2vec.py:1366] training on a 6680 raw words (2682 effective words) took 0.0s, 174103 effective words/s\n",
      "W1208 09:55:01.528451 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 29/44: Accuracy 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:01.870771 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:01.871538 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:01.871950 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:01.872472 140399148152640 word2vec.py:1405] collected 270 word types from a corpus of 817 raw words and 122 sentences\n",
      "I1208 09:55:01.872824 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:01.873410 140399148152640 word2vec.py:1480] effective_min_count=2 retains 125 unique words (46% of original 270, drops 145)\n",
      "I1208 09:55:01.873755 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 672 word corpus (82% of original 817, drops 145)\n",
      "I1208 09:55:01.874668 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 270 items\n",
      "I1208 09:55:01.875043 140399148152640 word2vec.py:1550] sample=0.001 downsamples 125 most-common words\n",
      "I1208 09:55:01.875372 140399148152640 word2vec.py:1551] downsampling leaves estimated 298 word corpus (44.5% of prior 672)\n",
      "I1208 09:55:01.875922 140399148152640 base_any2vec.py:1006] estimated required memory for 125 words and 50 dimensions: 112500 bytes\n",
      "I1208 09:55:01.876280 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:01.908122 140399148152640 base_any2vec.py:1192] training model with 3 workers on 125 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:01.910163 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.910546 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.911003 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.911342 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 817 raw words (287 effective words) took 0.0s, 184866 effective words/s\n",
      "I1208 09:55:01.913139 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.913529 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.914232 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.914581 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 817 raw words (297 effective words) took 0.0s, 165365 effective words/s\n",
      "I1208 09:55:01.916226 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.916641 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.917342 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.917676 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 817 raw words (311 effective words) took 0.0s, 172664 effective words/s\n",
      "I1208 09:55:01.919314 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.919751 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.920208 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.920555 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 817 raw words (296 effective words) took 0.0s, 189170 effective words/s\n",
      "I1208 09:55:01.922355 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:01.922799 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:01.923488 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:01.923862 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 817 raw words (296 effective words) took 0.0s, 161588 effective words/s\n",
      "I1208 09:55:01.924225 140399148152640 base_any2vec.py:1366] training on a 4085 raw words (1487 effective words) took 0.0s, 94902 effective words/s\n",
      "W1208 09:55:01.924561 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 30/44: Accuracy 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:02.221791 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:02.222530 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:02.222919 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:02.223381 140399148152640 word2vec.py:1405] collected 100 word types from a corpus of 281 raw words and 130 sentences\n",
      "I1208 09:55:02.223740 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:02.224175 140399148152640 word2vec.py:1480] effective_min_count=2 retains 42 unique words (42% of original 100, drops 58)\n",
      "I1208 09:55:02.224503 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 223 word corpus (79% of original 281, drops 58)\n",
      "I1208 09:55:02.225049 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 100 items\n",
      "I1208 09:55:02.225391 140399148152640 word2vec.py:1550] sample=0.001 downsamples 42 most-common words\n",
      "I1208 09:55:02.225729 140399148152640 word2vec.py:1551] downsampling leaves estimated 50 word corpus (22.5% of prior 223)\n",
      "I1208 09:55:02.226168 140399148152640 base_any2vec.py:1006] estimated required memory for 42 words and 50 dimensions: 37800 bytes\n",
      "I1208 09:55:02.226517 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:02.237593 140399148152640 base_any2vec.py:1192] training model with 3 workers on 42 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:02.240193 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.240656 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.241032 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.241358 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 281 raw words (47 effective words) took 0.0s, 33241 effective words/s\n",
      "I1208 09:55:02.243036 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.243488 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.243849 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.244185 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 281 raw words (44 effective words) took 0.0s, 31252 effective words/s\n",
      "I1208 09:55:02.245878 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.246378 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.246752 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.247098 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 281 raw words (49 effective words) took 0.0s, 33215 effective words/s\n",
      "I1208 09:55:02.248782 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.249271 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.249620 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.249979 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 281 raw words (54 effective words) took 0.0s, 37568 effective words/s\n",
      "I1208 09:55:02.251613 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.252177 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.252533 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.252876 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 281 raw words (47 effective words) took 0.0s, 31556 effective words/s\n",
      "I1208 09:55:02.253248 140399148152640 base_any2vec.py:1366] training on a 1405 raw words (241 effective words) took 0.0s, 15823 effective words/s\n",
      "W1208 09:55:02.253599 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:02.390649 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:02.391372 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:02.391754 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:02.392176 140399148152640 word2vec.py:1405] collected 124 word types from a corpus of 296 raw words and 55 sentences\n",
      "I1208 09:55:02.392515 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:02.392965 140399148152640 word2vec.py:1480] effective_min_count=2 retains 51 unique words (41% of original 124, drops 73)\n",
      "I1208 09:55:02.393297 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 223 word corpus (75% of original 296, drops 73)\n",
      "I1208 09:55:02.393883 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 124 items\n",
      "I1208 09:55:02.394234 140399148152640 word2vec.py:1550] sample=0.001 downsamples 51 most-common words\n",
      "I1208 09:55:02.394561 140399148152640 word2vec.py:1551] downsampling leaves estimated 58 word corpus (26.2% of prior 223)\n",
      "I1208 09:55:02.395013 140399148152640 base_any2vec.py:1006] estimated required memory for 51 words and 50 dimensions: 45900 bytes\n",
      "I1208 09:55:02.395353 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:02.408708 140399148152640 base_any2vec.py:1192] training model with 3 workers on 51 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:02.411172 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.411625 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.411995 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.412331 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 296 raw words (62 effective words) took 0.0s, 44213 effective words/s\n",
      "I1208 09:55:02.413956 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.414415 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.414782 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.415130 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 296 raw words (51 effective words) took 0.0s, 36404 effective words/s\n",
      "I1208 09:55:02.416717 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.417172 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.417519 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.417857 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 296 raw words (42 effective words) took 0.0s, 30609 effective words/s\n",
      "I1208 09:55:02.419421 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.419874 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.420229 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.420557 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 296 raw words (61 effective words) took 0.0s, 44931 effective words/s\n",
      "I1208 09:55:02.422170 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.422671 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.423056 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.423398 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 296 raw words (42 effective words) took 0.0s, 28983 effective words/s\n",
      "I1208 09:55:02.423757 140399148152640 base_any2vec.py:1366] training on a 1480 raw words (258 effective words) took 0.0s, 17621 effective words/s\n",
      "W1208 09:55:02.424116 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 31/44: Accuracy 0.39\n",
      "Document Processed- 32/44: Accuracy 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:02.544425 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:02.545172 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:02.545532 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:02.546008 140399148152640 word2vec.py:1405] collected 197 word types from a corpus of 564 raw words and 38 sentences\n",
      "I1208 09:55:02.546354 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:02.546864 140399148152640 word2vec.py:1480] effective_min_count=2 retains 78 unique words (39% of original 197, drops 119)\n",
      "I1208 09:55:02.547223 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 445 word corpus (78% of original 564, drops 119)\n",
      "I1208 09:55:02.547924 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 197 items\n",
      "I1208 09:55:02.548290 140399148152640 word2vec.py:1550] sample=0.001 downsamples 78 most-common words\n",
      "I1208 09:55:02.548614 140399148152640 word2vec.py:1551] downsampling leaves estimated 148 word corpus (33.5% of prior 445)\n",
      "I1208 09:55:02.549114 140399148152640 base_any2vec.py:1006] estimated required memory for 78 words and 50 dimensions: 70200 bytes\n",
      "I1208 09:55:02.549452 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:02.569629 140399148152640 base_any2vec.py:1192] training model with 3 workers on 78 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:02.572150 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.572632 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.573004 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.573338 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 564 raw words (138 effective words) took 0.0s, 93549 effective words/s\n",
      "I1208 09:55:02.574991 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.575455 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.575821 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.576162 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 564 raw words (145 effective words) took 0.0s, 99701 effective words/s\n",
      "I1208 09:55:02.577794 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.578304 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.578660 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.579021 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 564 raw words (152 effective words) took 0.0s, 101493 effective words/s\n",
      "I1208 09:55:02.580610 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.581082 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.581434 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.581780 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 564 raw words (146 effective words) took 0.0s, 101892 effective words/s\n",
      "I1208 09:55:02.583437 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.583935 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.584300 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.584632 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 564 raw words (142 effective words) took 0.0s, 95994 effective words/s\n",
      "I1208 09:55:02.585007 140399148152640 base_any2vec.py:1366] training on a 2820 raw words (723 effective words) took 0.0s, 48405 effective words/s\n",
      "W1208 09:55:02.585340 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:02.767077 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:02.767826 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:02.768224 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:02.768700 140399148152640 word2vec.py:1405] collected 194 word types from a corpus of 512 raw words and 66 sentences\n",
      "I1208 09:55:02.769064 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:02.769552 140399148152640 word2vec.py:1480] effective_min_count=2 retains 82 unique words (42% of original 194, drops 112)\n",
      "I1208 09:55:02.769902 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 400 word corpus (78% of original 512, drops 112)\n",
      "I1208 09:55:02.770630 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 194 items\n",
      "I1208 09:55:02.771008 140399148152640 word2vec.py:1550] sample=0.001 downsamples 82 most-common words\n",
      "I1208 09:55:02.771340 140399148152640 word2vec.py:1551] downsampling leaves estimated 138 word corpus (34.7% of prior 400)\n",
      "I1208 09:55:02.771830 140399148152640 base_any2vec.py:1006] estimated required memory for 82 words and 50 dimensions: 73800 bytes\n",
      "I1208 09:55:02.772182 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:02.793395 140399148152640 base_any2vec.py:1192] training model with 3 workers on 82 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:02.796010 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.796479 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.796849 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.797193 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 512 raw words (136 effective words) took 0.0s, 92821 effective words/s\n",
      "I1208 09:55:02.798864 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.799353 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.799707 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.800048 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 512 raw words (142 effective words) took 0.0s, 96613 effective words/s\n",
      "I1208 09:55:02.801679 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.802217 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.802582 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.802926 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 512 raw words (143 effective words) took 0.0s, 94943 effective words/s\n",
      "I1208 09:55:02.804513 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.805062 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.805415 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.805751 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 512 raw words (117 effective words) took 0.0s, 78617 effective words/s\n",
      "I1208 09:55:02.807346 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:02.807877 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:02.808260 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:02.808596 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 512 raw words (129 effective words) took 0.0s, 84504 effective words/s\n",
      "I1208 09:55:02.808969 140399148152640 base_any2vec.py:1366] training on a 2560 raw words (667 effective words) took 0.0s, 47802 effective words/s\n",
      "W1208 09:55:02.809304 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 33/44: Accuracy 0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:02.986951 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:02.987700 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:02.988092 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:02.988533 140399148152640 word2vec.py:1405] collected 185 word types from a corpus of 404 raw words and 69 sentences\n",
      "I1208 09:55:02.988888 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:02.989389 140399148152640 word2vec.py:1480] effective_min_count=2 retains 69 unique words (37% of original 185, drops 116)\n",
      "I1208 09:55:02.989721 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 288 word corpus (71% of original 404, drops 116)\n",
      "I1208 09:55:02.990392 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 185 items\n",
      "I1208 09:55:02.990746 140399148152640 word2vec.py:1550] sample=0.001 downsamples 69 most-common words\n",
      "I1208 09:55:02.991092 140399148152640 word2vec.py:1551] downsampling leaves estimated 89 word corpus (31.2% of prior 288)\n",
      "I1208 09:55:02.991551 140399148152640 base_any2vec.py:1006] estimated required memory for 69 words and 50 dimensions: 62100 bytes\n",
      "I1208 09:55:02.991912 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:03.009974 140399148152640 base_any2vec.py:1192] training model with 3 workers on 69 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:03.012572 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.013038 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.013395 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.013746 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 404 raw words (90 effective words) took 0.0s, 63260 effective words/s\n",
      "I1208 09:55:03.015400 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.015861 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.016232 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.016562 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 404 raw words (96 effective words) took 0.0s, 67614 effective words/s\n",
      "I1208 09:55:03.018155 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.018596 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.018969 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.019298 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 404 raw words (78 effective words) took 0.0s, 56368 effective words/s\n",
      "I1208 09:55:03.020853 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.021319 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.021672 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.022027 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 404 raw words (93 effective words) took 0.0s, 65972 effective words/s\n",
      "I1208 09:55:03.023627 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.024091 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.024440 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.024778 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 404 raw words (97 effective words) took 0.0s, 69530 effective words/s\n",
      "I1208 09:55:03.025130 140399148152640 base_any2vec.py:1366] training on a 2020 raw words (454 effective words) took 0.0s, 30995 effective words/s\n",
      "W1208 09:55:03.025461 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 34/44: Accuracy 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:03.169615 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:03.170376 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:03.170768 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:03.171252 140399148152640 word2vec.py:1405] collected 239 word types from a corpus of 645 raw words and 44 sentences\n",
      "I1208 09:55:03.171607 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:03.172169 140399148152640 word2vec.py:1480] effective_min_count=2 retains 95 unique words (39% of original 239, drops 144)\n",
      "I1208 09:55:03.172509 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 501 word corpus (77% of original 645, drops 144)\n",
      "I1208 09:55:03.173305 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 239 items\n",
      "I1208 09:55:03.173655 140399148152640 word2vec.py:1550] sample=0.001 downsamples 95 most-common words\n",
      "I1208 09:55:03.174014 140399148152640 word2vec.py:1551] downsampling leaves estimated 190 word corpus (38.0% of prior 501)\n",
      "I1208 09:55:03.174510 140399148152640 base_any2vec.py:1006] estimated required memory for 95 words and 50 dimensions: 85500 bytes\n",
      "I1208 09:55:03.174880 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:03.200572 140399148152640 base_any2vec.py:1192] training model with 3 workers on 95 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:03.202118 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.202497 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.202952 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.203291 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 645 raw words (181 effective words) took 0.0s, 122764 effective words/s\n",
      "I1208 09:55:03.204953 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.205409 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.205776 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.206120 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 645 raw words (194 effective words) took 0.0s, 132043 effective words/s\n",
      "I1208 09:55:03.207747 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.208101 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.208529 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.208876 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 645 raw words (195 effective words) took 0.0s, 136351 effective words/s\n",
      "I1208 09:55:03.210540 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.210938 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.211376 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.211718 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 645 raw words (181 effective words) took 0.0s, 123691 effective words/s\n",
      "I1208 09:55:03.213315 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.213730 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.214167 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.214516 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 645 raw words (188 effective words) took 0.0s, 127517 effective words/s\n",
      "I1208 09:55:03.214855 140399148152640 base_any2vec.py:1366] training on a 3225 raw words (939 effective words) took 0.0s, 67670 effective words/s\n",
      "W1208 09:55:03.215209 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 35/44: Accuracy 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:03.393057 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:03.393767 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:03.394153 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:03.394634 140399148152640 word2vec.py:1405] collected 225 word types from a corpus of 662 raw words and 58 sentences\n",
      "I1208 09:55:03.394996 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:03.395530 140399148152640 word2vec.py:1480] effective_min_count=2 retains 104 unique words (46% of original 225, drops 121)\n",
      "I1208 09:55:03.395877 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 541 word corpus (81% of original 662, drops 121)\n",
      "I1208 09:55:03.396722 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 225 items\n",
      "I1208 09:55:03.397089 140399148152640 word2vec.py:1550] sample=0.001 downsamples 104 most-common words\n",
      "I1208 09:55:03.397418 140399148152640 word2vec.py:1551] downsampling leaves estimated 215 word corpus (39.9% of prior 541)\n",
      "I1208 09:55:03.397955 140399148152640 base_any2vec.py:1006] estimated required memory for 104 words and 50 dimensions: 93600 bytes\n",
      "I1208 09:55:03.398303 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:03.426238 140399148152640 base_any2vec.py:1192] training model with 3 workers on 104 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:03.427854 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.428238 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.428704 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.429065 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 662 raw words (215 effective words) took 0.0s, 139833 effective words/s\n",
      "I1208 09:55:03.430811 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.431249 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.431675 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.432039 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 662 raw words (214 effective words) took 0.0s, 139375 effective words/s\n",
      "I1208 09:55:03.433752 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.434197 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.434623 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.434961 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 662 raw words (222 effective words) took 0.0s, 144382 effective words/s\n",
      "I1208 09:55:03.436671 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.437104 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.437529 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.437881 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 662 raw words (219 effective words) took 0.0s, 144796 effective words/s\n",
      "I1208 09:55:03.439565 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.440052 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.440421 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.440757 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 662 raw words (210 effective words) took 0.0s, 140255 effective words/s\n",
      "I1208 09:55:03.441114 140399148152640 base_any2vec.py:1366] training on a 3310 raw words (1080 effective words) took 0.0s, 74630 effective words/s\n",
      "W1208 09:55:03.441444 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 36/44: Accuracy 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:03.613608 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:03.614343 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:03.614730 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:03.615229 140399148152640 word2vec.py:1405] collected 244 word types from a corpus of 686 raw words and 50 sentences\n",
      "I1208 09:55:03.615571 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:03.616127 140399148152640 word2vec.py:1480] effective_min_count=2 retains 102 unique words (41% of original 244, drops 142)\n",
      "I1208 09:55:03.616463 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 544 word corpus (79% of original 686, drops 142)\n",
      "I1208 09:55:03.617366 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 244 items\n",
      "I1208 09:55:03.617761 140399148152640 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1208 09:55:03.618107 140399148152640 word2vec.py:1551] downsampling leaves estimated 215 word corpus (39.7% of prior 544)\n",
      "I1208 09:55:03.618616 140399148152640 base_any2vec.py:1006] estimated required memory for 102 words and 50 dimensions: 91800 bytes\n",
      "I1208 09:55:03.618969 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:03.644590 140399148152640 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:03.647429 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.647875 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.648353 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.648741 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 686 raw words (213 effective words) took 0.0s, 130961 effective words/s\n",
      "I1208 09:55:03.650593 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.651058 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.651514 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.651865 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 686 raw words (230 effective words) took 0.0s, 144234 effective words/s\n",
      "I1208 09:55:03.653731 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.654175 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.654644 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.655648 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 686 raw words (209 effective words) took 0.0s, 93845 effective words/s\n",
      "I1208 09:55:03.657147 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.657603 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.658070 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.658412 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 686 raw words (217 effective words) took 0.0s, 136059 effective words/s\n",
      "I1208 09:55:03.660088 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.660499 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.660938 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.661280 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 686 raw words (207 effective words) took 0.0s, 138018 effective words/s\n",
      "I1208 09:55:03.661623 140399148152640 base_any2vec.py:1366] training on a 3430 raw words (1076 effective words) took 0.0s, 65572 effective words/s\n",
      "W1208 09:55:03.661987 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 37/44: Accuracy 0.31\n",
      "Document Processed- 38/44: Accuracy 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:03.907159 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:03.907876 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:03.908278 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:03.908783 140399148152640 word2vec.py:1405] collected 214 word types from a corpus of 631 raw words and 94 sentences\n",
      "I1208 09:55:03.909149 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:03.909665 140399148152640 word2vec.py:1480] effective_min_count=2 retains 94 unique words (43% of original 214, drops 120)\n",
      "I1208 09:55:03.910029 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 511 word corpus (80% of original 631, drops 120)\n",
      "I1208 09:55:03.910807 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 214 items\n",
      "I1208 09:55:03.911179 140399148152640 word2vec.py:1550] sample=0.001 downsamples 94 most-common words\n",
      "I1208 09:55:03.911509 140399148152640 word2vec.py:1551] downsampling leaves estimated 190 word corpus (37.2% of prior 511)\n",
      "I1208 09:55:03.912026 140399148152640 base_any2vec.py:1006] estimated required memory for 94 words and 50 dimensions: 84600 bytes\n",
      "I1208 09:55:03.912374 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:03.936692 140399148152640 base_any2vec.py:1192] training model with 3 workers on 94 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:03.939355 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.939819 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.940188 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.940520 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 631 raw words (165 effective words) took 0.0s, 112696 effective words/s\n",
      "I1208 09:55:03.942261 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.942659 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.943108 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.943441 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 631 raw words (198 effective words) took 0.0s, 133513 effective words/s\n",
      "I1208 09:55:03.945170 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.945670 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.946050 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.946383 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 631 raw words (191 effective words) took 0.0s, 124710 effective words/s\n",
      "I1208 09:55:03.948108 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.948661 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.949048 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.949387 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 631 raw words (194 effective words) took 0.0s, 123672 effective words/s\n",
      "I1208 09:55:03.951094 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:03.951624 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:03.952005 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:03.952346 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 631 raw words (183 effective words) took 0.0s, 118134 effective words/s\n",
      "I1208 09:55:03.952700 140399148152640 base_any2vec.py:1366] training on a 3155 raw words (931 effective words) took 0.0s, 64042 effective words/s\n",
      "W1208 09:55:03.953052 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:04.179638 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:04.180395 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:04.180783 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:04.181271 140399148152640 word2vec.py:1405] collected 225 word types from a corpus of 570 raw words and 87 sentences\n",
      "I1208 09:55:04.181616 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:04.182152 140399148152640 word2vec.py:1480] effective_min_count=2 retains 99 unique words (44% of original 225, drops 126)\n",
      "I1208 09:55:04.182491 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 444 word corpus (77% of original 570, drops 126)\n",
      "I1208 09:55:04.183291 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 225 items\n",
      "I1208 09:55:04.183651 140399148152640 word2vec.py:1550] sample=0.001 downsamples 99 most-common words\n",
      "I1208 09:55:04.184003 140399148152640 word2vec.py:1551] downsampling leaves estimated 171 word corpus (38.7% of prior 444)\n",
      "I1208 09:55:04.184510 140399148152640 base_any2vec.py:1006] estimated required memory for 99 words and 50 dimensions: 89100 bytes\n",
      "I1208 09:55:04.184861 140399148152640 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 39/44: Accuracy 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:55:04.210570 140399148152640 base_any2vec.py:1192] training model with 3 workers on 99 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:04.213151 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.213629 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.214010 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.214365 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 570 raw words (171 effective words) took 0.0s, 112563 effective words/s\n",
      "I1208 09:55:04.216089 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.216498 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.216932 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.217272 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 570 raw words (190 effective words) took 0.0s, 127634 effective words/s\n",
      "I1208 09:55:04.218911 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.219381 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.219746 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.220082 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 570 raw words (165 effective words) took 0.0s, 113355 effective words/s\n",
      "I1208 09:55:04.221733 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.222237 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.222591 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.222960 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 570 raw words (163 effective words) took 0.0s, 107775 effective words/s\n",
      "I1208 09:55:04.224615 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.225092 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.225453 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.225798 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 570 raw words (168 effective words) took 0.0s, 114836 effective words/s\n",
      "I1208 09:55:04.226155 140399148152640 base_any2vec.py:1366] training on a 2850 raw words (857 effective words) took 0.0s, 56596 effective words/s\n",
      "W1208 09:55:04.226484 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:04.373770 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:04.374479 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:04.374872 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:04.375314 140399148152640 word2vec.py:1405] collected 169 word types from a corpus of 361 raw words and 52 sentences\n",
      "I1208 09:55:04.375663 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:04.376128 140399148152640 word2vec.py:1480] effective_min_count=2 retains 56 unique words (33% of original 169, drops 113)\n",
      "I1208 09:55:04.376461 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 248 word corpus (68% of original 361, drops 113)\n",
      "I1208 09:55:04.377124 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 169 items\n",
      "I1208 09:55:04.377482 140399148152640 word2vec.py:1550] sample=0.001 downsamples 56 most-common words\n",
      "I1208 09:55:04.377841 140399148152640 word2vec.py:1551] downsampling leaves estimated 68 word corpus (27.6% of prior 248)\n",
      "I1208 09:55:04.378291 140399148152640 base_any2vec.py:1006] estimated required memory for 56 words and 50 dimensions: 50400 bytes\n",
      "I1208 09:55:04.378627 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:04.393470 140399148152640 base_any2vec.py:1192] training model with 3 workers on 56 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:04.396015 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.396473 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.396838 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.397176 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 361 raw words (76 effective words) took 0.0s, 53703 effective words/s\n",
      "I1208 09:55:04.398855 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.399384 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.399759 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.400097 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 361 raw words (72 effective words) took 0.0s, 47970 effective words/s\n",
      "I1208 09:55:04.401698 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.402164 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.402520 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.402872 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 361 raw words (60 effective words) took 0.0s, 41893 effective words/s\n",
      "I1208 09:55:04.404506 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.405018 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.405369 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.405718 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 361 raw words (65 effective words) took 0.0s, 44485 effective words/s\n",
      "I1208 09:55:04.407331 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.407764 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.408113 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.408438 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 361 raw words (59 effective words) took 0.0s, 44184 effective words/s\n",
      "I1208 09:55:04.408788 140399148152640 base_any2vec.py:1366] training on a 1805 raw words (332 effective words) took 0.0s, 22376 effective words/s\n",
      "W1208 09:55:04.409128 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 40/44: Accuracy 0.18\n",
      "Document Processed- 41/44: Accuracy 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 09:55:04.558401 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:04.559124 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:04.559499 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:04.559968 140399148152640 word2vec.py:1405] collected 177 word types from a corpus of 455 raw words and 56 sentences\n",
      "I1208 09:55:04.560339 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:04.560822 140399148152640 word2vec.py:1480] effective_min_count=2 retains 70 unique words (39% of original 177, drops 107)\n",
      "I1208 09:55:04.561169 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 348 word corpus (76% of original 455, drops 107)\n",
      "I1208 09:55:04.561902 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 177 items\n",
      "I1208 09:55:04.562297 140399148152640 word2vec.py:1550] sample=0.001 downsamples 70 most-common words\n",
      "I1208 09:55:04.562625 140399148152640 word2vec.py:1551] downsampling leaves estimated 108 word corpus (31.3% of prior 348)\n",
      "I1208 09:55:04.563109 140399148152640 base_any2vec.py:1006] estimated required memory for 70 words and 50 dimensions: 63000 bytes\n",
      "I1208 09:55:04.563445 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:04.581647 140399148152640 base_any2vec.py:1192] training model with 3 workers on 70 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:04.584175 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.584648 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.585026 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.585364 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 455 raw words (107 effective words) took 0.0s, 73315 effective words/s\n",
      "I1208 09:55:04.587033 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.587508 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.587876 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.588218 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 455 raw words (104 effective words) took 0.0s, 71084 effective words/s\n",
      "I1208 09:55:04.589810 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.590256 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.590601 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.590946 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 455 raw words (104 effective words) took 0.0s, 74535 effective words/s\n",
      "I1208 09:55:04.592517 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.592987 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.593347 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.593700 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 455 raw words (106 effective words) took 0.0s, 74491 effective words/s\n",
      "I1208 09:55:04.595341 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.595865 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.596247 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.596591 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 455 raw words (122 effective words) took 0.0s, 80495 effective words/s\n",
      "I1208 09:55:04.596982 140399148152640 base_any2vec.py:1366] training on a 2275 raw words (543 effective words) took 0.0s, 39509 effective words/s\n",
      "W1208 09:55:04.597337 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:04.787060 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:04.787809 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:04.788213 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:04.788698 140399148152640 word2vec.py:1405] collected 201 word types from a corpus of 584 raw words and 70 sentences\n",
      "I1208 09:55:04.789071 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:04.789576 140399148152640 word2vec.py:1480] effective_min_count=2 retains 89 unique words (44% of original 201, drops 112)\n",
      "I1208 09:55:04.789921 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 472 word corpus (80% of original 584, drops 112)\n",
      "I1208 09:55:04.790673 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 201 items\n",
      "I1208 09:55:04.791051 140399148152640 word2vec.py:1550] sample=0.001 downsamples 89 most-common words\n",
      "I1208 09:55:04.791398 140399148152640 word2vec.py:1551] downsampling leaves estimated 170 word corpus (36.2% of prior 472)\n",
      "I1208 09:55:04.791892 140399148152640 base_any2vec.py:1006] estimated required memory for 89 words and 50 dimensions: 80100 bytes\n",
      "I1208 09:55:04.792244 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:04.815274 140399148152640 base_any2vec.py:1192] training model with 3 workers on 89 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:04.817912 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.818387 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.818761 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.819125 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 584 raw words (157 effective words) took 0.0s, 104123 effective words/s\n",
      "I1208 09:55:04.820835 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 42/44: Accuracy 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:55:04.821373 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.821829 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.822172 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 584 raw words (185 effective words) took 0.0s, 112193 effective words/s\n",
      "I1208 09:55:04.823879 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.824405 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.824788 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.825137 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 584 raw words (159 effective words) took 0.0s, 102176 effective words/s\n",
      "I1208 09:55:04.826809 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.827331 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.827710 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.828064 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 584 raw words (175 effective words) took 0.0s, 113528 effective words/s\n",
      "I1208 09:55:04.829735 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:04.830156 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:04.830584 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:04.830949 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 584 raw words (187 effective words) took 0.0s, 124746 effective words/s\n",
      "I1208 09:55:04.831304 140399148152640 base_any2vec.py:1366] training on a 2920 raw words (863 effective words) took 0.0s, 55442 effective words/s\n",
      "W1208 09:55:04.831633 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "W1208 09:55:05.023289 140399148152640 base_any2vec.py:720] consider setting layer size to a multiple of 4 for greater performance\n",
      "I1208 09:55:05.024029 140399148152640 word2vec.py:1399] collecting all words and their counts\n",
      "I1208 09:55:05.024423 140399148152640 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1208 09:55:05.024875 140399148152640 word2vec.py:1405] collected 142 word types from a corpus of 421 raw words and 75 sentences\n",
      "I1208 09:55:05.025246 140399148152640 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1208 09:55:05.025719 140399148152640 word2vec.py:1480] effective_min_count=2 retains 64 unique words (45% of original 142, drops 78)\n",
      "I1208 09:55:05.026078 140399148152640 word2vec.py:1486] effective_min_count=2 leaves 343 word corpus (81% of original 421, drops 78)\n",
      "I1208 09:55:05.026769 140399148152640 word2vec.py:1547] deleting the raw counts dictionary of 142 items\n",
      "I1208 09:55:05.027153 140399148152640 word2vec.py:1550] sample=0.001 downsamples 64 most-common words\n",
      "I1208 09:55:05.027491 140399148152640 word2vec.py:1551] downsampling leaves estimated 100 word corpus (29.3% of prior 343)\n",
      "I1208 09:55:05.027978 140399148152640 base_any2vec.py:1006] estimated required memory for 64 words and 50 dimensions: 57600 bytes\n",
      "I1208 09:55:05.028329 140399148152640 word2vec.py:1699] resetting layer weights\n",
      "I1208 09:55:05.044986 140399148152640 base_any2vec.py:1192] training model with 3 workers on 64 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "I1208 09:55:05.047546 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:05.048028 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:05.048386 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:05.048731 140399148152640 base_any2vec.py:1330] EPOCH - 1 : training on 421 raw words (104 effective words) took 0.0s, 72011 effective words/s\n",
      "I1208 09:55:05.050405 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:05.050869 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:05.051238 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:05.051567 140399148152640 base_any2vec.py:1330] EPOCH - 2 : training on 421 raw words (103 effective words) took 0.0s, 71934 effective words/s\n",
      "I1208 09:55:05.053194 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:05.053617 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:05.053979 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:05.054307 140399148152640 base_any2vec.py:1330] EPOCH - 3 : training on 421 raw words (96 effective words) took 0.0s, 70322 effective words/s\n",
      "I1208 09:55:05.056419 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:05.056896 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:05.057255 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:05.057588 140399148152640 base_any2vec.py:1330] EPOCH - 4 : training on 421 raw words (94 effective words) took 0.0s, 49002 effective words/s\n",
      "I1208 09:55:05.059231 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1208 09:55:05.059762 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1208 09:55:05.060142 140399148152640 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1208 09:55:05.060482 140399148152640 base_any2vec.py:1330] EPOCH - 5 : training on 421 raw words (96 effective words) took 0.0s, 63817 effective words/s\n",
      "I1208 09:55:05.060846 140399148152640 base_any2vec.py:1366] training on a 2105 raw words (493 effective words) took 0.0s, 31985 effective words/s\n",
      "W1208 09:55:05.061200 140399148152640 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 43/44: Accuracy 0.18\n",
      "Document Processed- 44/44: Accuracy 0.67\n",
      "MSE:  1.157876795416127\n"
     ]
    }
   ],
   "source": [
    "WEM_MSE = 0\n",
    "WEM_Accuracy = []\n",
    "Y_gold_wm = [] \n",
    "Y_pred_wm = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "\n",
    "            all_words = [normalize_text(sent) for sent in corpus]\n",
    "            pos_tagged = []\n",
    "            for i in range(len(all_words)):\n",
    "                all_words[i] = [w for w in word_tokenize(str(all_words[i]))]\n",
    "                pos_tagged.append(pos_tag(all_words[i]))\n",
    "\n",
    "            word2vec,vocabulary = build_vocabulary(all_words)\n",
    "            WIMP = []\n",
    "            for sentence in all_words:\n",
    "                POS_IMP = build_pos_imp(vocabulary, sentence, pos_tagged)\n",
    "\n",
    "                BOW, WV, POSA = build_word_matrix(vocabulary, sentence, POS_IMP, word2vec)\n",
    "                A = BOW * WV * POSA\n",
    "                WIMP_TEMP=[]\n",
    "                for i in range(0,len(sentence)):\n",
    "                    WIMP_TEMP.append(np.mean(A[i]))\n",
    "                    WIMP.append(WIMP_TEMP)\n",
    "\n",
    "            WIMP = NormalizeData(WIMP)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(WIMP[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    Y_gold_wm += make_scalar(scores[i], 0)\n",
    "                    Y_pred_wm += make_scalar(WIMP[i], 0)\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        if compare(float(WIMP[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "                            WEM_MSE = WEM_MSE + pow((float(WIMP[i][j])-float(scores[i][j])),2)\n",
    "\n",
    "            accuracy = correct/total\n",
    "            WEM_Accuracy.append(accuracy)\n",
    "            print(\"Document Processed- \"+str(len(WEM_Accuracy))+\"/44: Accuracy %.2f\"%accuracy)\n",
    "\n",
    "print(\"MSE: \", str(WEM_MSE))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_gold_wm, density=True, bins=10)  # density=False would make counts\n",
    "plt.hist(Y_pred_wm, density=True, bins=10)\n",
    "\n",
    "plt.ylabel('Data')\n",
    "plt.xlabel('Weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.45      0.57       328\n",
      "           2       0.33      0.06      0.10       375\n",
      "           3       0.15      0.24      0.19       184\n",
      "           4       0.12      0.50      0.19       111\n",
      "           5       0.09      0.08      0.09       106\n",
      "           6       0.13      0.05      0.07        41\n",
      "\n",
      "    accuracy                           0.25      1145\n",
      "   macro avg       0.26      0.23      0.20      1145\n",
      "weighted avg       0.38      0.25      0.25      1145\n",
      "\n",
      "Accuracy: 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[149,  13,  51,  90,  24,   1],\n",
       "       [ 33,  21, 114, 163,  35,   9],\n",
       "       [ 12,  13,  45,  97,  17,   0],\n",
       "       [  0,  10,  32,  56,  12,   1],\n",
       "       [  2,   6,  33,  54,   9,   2],\n",
       "       [  0,   0,  21,  15,   3,   2]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(Y_gold_wm,Y_pred_wm))\n",
    "print(\"Accuracy: %.2f\"%accuracy_score(Y_gold_wm, Y_pred_wm))\n",
    "cm = make_confusion_matrices(Y_gold_wm, Y_pred_wm, 1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Context-aware Term Weight Determination Using BERT\n",
    "\n",
    "Prior work has shown that these embeddings can characterize a token’s syntactic features (e.g., word dependencies) and semantic features (e.g., named entity labeling), which can help estimate a term’s importance. Here, the contextualized token embeddings have been fed into a linear layer. It maps a token’s embedding into a real-number weight.\n",
    "\n",
    "http://www.cs.cmu.edu/~zhuyund/Zhuyun_Dai_Dissertation.pdf\n",
    "\n",
    "https://www.cs.cmu.edu/~callan/Papers/TheWebConf20-Zhuyun-Dai.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1208 09:55:27.854806 140399148152640 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/aa7510/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I1208 09:55:27.855630 140399148152640 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1208 09:55:27.953411 140399148152640 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/aa7510/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading pre-trained BERT Base\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,)\n",
    "\n",
    "#Use evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_Embedding(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     print(\"Length of tokenized text: \" + str(len(tokenized_text)))\n",
    "\n",
    "    # Map tokenized text to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # `output_hidden_states = True` so the 3rd element has the hidden states for each layer. \n",
    "        # More information at: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    # `token_embeddings` is a [32 x 12 x 768] tensor, now for each token of 32:\n",
    "\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "        # Sum the vectors from the last four layers to represent the token.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "    BERT_Word_Embedding = np.zeros(shape=(len(token_vecs_sum),len(token_vecs_sum[0])))\n",
    "    for i in range(0, len(token_vecs_sum)):\n",
    "        BERT_Word_Embedding[i] = token_vecs_sum[i].numpy()\n",
    "#     print(BERT_Word_Embedding.shape)\n",
    "    return BERT_Word_Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Processed- 1/44: Accuracy 0.42\n",
      "Document Processed- 2/44: Accuracy 0.53\n",
      "Document Processed- 3/44: Accuracy 0.44\n",
      "Document Processed- 4/44: Accuracy 0.33\n",
      "Document Processed- 5/44: Accuracy 0.43\n",
      "Document Processed- 6/44: Accuracy 0.32\n",
      "Document Processed- 7/44: Accuracy 0.49\n",
      "Document Processed- 8/44: Accuracy 0.41\n",
      "Document Processed- 9/44: Accuracy 0.34\n",
      "Document Processed- 10/44: Accuracy 0.38\n",
      "Document Processed- 11/44: Accuracy 0.31\n",
      "Document Processed- 12/44: Accuracy 0.43\n",
      "Document Processed- 13/44: Accuracy 0.34\n",
      "Document Processed- 14/44: Accuracy 0.51\n",
      "Document Processed- 15/44: Accuracy 0.58\n",
      "Document Processed- 16/44: Accuracy 0.40\n",
      "Document Processed- 17/44: Accuracy 0.43\n",
      "Document Processed- 18/44: Accuracy 0.47\n",
      "Document Processed- 19/44: Accuracy 0.37\n",
      "Document Processed- 20/44: Accuracy 0.38\n",
      "Document Processed- 21/44: Accuracy 0.41\n",
      "Document Processed- 22/44: Accuracy 0.41\n",
      "Document Processed- 23/44: Accuracy 0.37\n",
      "Document Processed- 24/44: Accuracy 0.50\n",
      "Document Processed- 25/44: Accuracy 0.49\n",
      "Document Processed- 26/44: Accuracy 0.47\n",
      "Document Processed- 27/44: Accuracy 0.34\n",
      "Document Processed- 28/44: Accuracy 0.45\n",
      "Document Processed- 29/44: Accuracy 0.39\n",
      "Document Processed- 30/44: Accuracy 0.49\n",
      "Document Processed- 31/44: Accuracy 0.46\n",
      "Document Processed- 32/44: Accuracy 0.47\n",
      "Document Processed- 33/44: Accuracy 0.35\n",
      "Document Processed- 34/44: Accuracy 0.40\n",
      "Document Processed- 35/44: Accuracy 0.41\n",
      "Document Processed- 36/44: Accuracy 0.38\n",
      "Document Processed- 37/44: Accuracy 0.40\n",
      "Document Processed- 38/44: Accuracy 0.33\n",
      "Document Processed- 39/44: Accuracy 0.38\n",
      "Document Processed- 40/44: Accuracy 0.40\n",
      "Document Processed- 41/44: Accuracy 0.35\n",
      "Document Processed- 42/44: Accuracy 0.38\n",
      "Document Processed- 43/44: Accuracy 0.47\n",
      "Document Processed- 44/44: Accuracy 0.49\n"
     ]
    }
   ],
   "source": [
    "BERT_Accuracy = []\n",
    "Y_gold_bert = []\n",
    "Y_pred_bert = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "\n",
    "            all_words = [str(normalize_text(sent)) for sent in corpus]\n",
    "            \n",
    "    \n",
    "            contextual_wimp_data=[]\n",
    "            for text in all_words:\n",
    "                sentence_mat = BERT_Embedding(text)\n",
    "                WIMP_temp = []\n",
    "                \n",
    "                for word_emb in sentence_mat[1:-1]:\n",
    "                    \n",
    "                    word_emb = np.reshape(word_emb,(word_emb.size,1))\n",
    "\n",
    "                    W = np.ones(shape=(len(word_emb),1))\n",
    "\n",
    "                    X = np.dot(W.T,word_emb)+0.05\n",
    "                    WIMP_temp.append(X[0][0])\n",
    "                    \n",
    "                    \n",
    "                contextual_wimp_data.append(WIMP_temp)\n",
    "\n",
    "            contextual_wimp_data = NormalizeData(contextual_wimp_data)\n",
    "\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(contextual_wimp_data[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    Y_gold_bert += make_scalar(scores[i], 0)\n",
    "                    Y_pred_bert += make_scalar(contextual_wimp_data[i], 0)\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        if compare(float(contextual_wimp_data[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "\n",
    "            accuracy = correct/total\n",
    "            BERT_Accuracy.append(accuracy)\n",
    "            print(\"Document Processed- \"+str(len(BERT_Accuracy))+\"/44: Accuracy %.2f\"%accuracy)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_gold_bert, density=True, bins=10)  # density=False would make counts\n",
    "plt.hist(Y_pred_bert, density=True, bins=10)\n",
    "\n",
    "plt.ylabel('Data')\n",
    "plt.xlabel('Weights');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.45      0.53      3262\n",
      "           2       0.36      0.81      0.50      3748\n",
      "           3       0.14      0.05      0.07      1974\n",
      "           4       0.00      0.00      0.00      1059\n",
      "           5       0.00      0.00      0.00       897\n",
      "           6       0.00      0.00      0.00       340\n",
      "\n",
      "    accuracy                           0.41     11280\n",
      "   macro avg       0.19      0.22      0.18     11280\n",
      "weighted avg       0.34      0.41      0.33     11280\n",
      "\n",
      "Accuracy: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1459, 1676,  123,    4,    0,    0],\n",
       "       [ 369, 3046,  332,    1,    0,    0],\n",
       "       [ 222, 1656,   96,    0,    0,    0],\n",
       "       [  76,  946,   37,    0,    0,    0],\n",
       "       [  66,  795,   36,    0,    0,    0],\n",
       "       [  13,  288,   39,    0,    0,    0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(Y_gold_bert,Y_pred_bert))\n",
    "print(\"Accuracy: %.2f\"%accuracy_score(Y_gold_bert, Y_pred_bert))\n",
    "cm = make_confusion_matrices(Y_gold_bert, Y_pred_bert, 1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train BERT embeddings the Word Importance and Predict with Selected Classifier"
   ]
  },
  {
   "attachments": {
    "Screen%20Shot%202021-12-08%20at%209.56.43%20AM.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOEAAABvCAYAAAAaCafSAAABRmlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAw8ABhIoMxonJxQWOAQE+QCUMMBoVfLvGwAiiL+uCzNpUsDJbMTnS1H3avAu7tX5XY6pHAVwpqcXJQPoPEKclFxSVMDAwpgDZyuUlBSB2B5AtUgR0FJA9B8ROh7A3gNhJEPYRsJqQIGcg+waQLZCckQg0g/EFkK2ThCSejsSG2gsCPC6uPj4KroaWFm4BBJxLOihJrSgB0c75BZVFmekZJQqOwFBKVfDMS9bTUTAyMDJkYACFOUT15yBwWDKK7UOI5S9hYLD4xsDAPBEhljSFgWF7GwODxC2EmMo8Bgb+FgaGbYcKEosS4Q5g/MZSnGZsBGHz2DMwsN79//+zBgMD+0QGhr8T////vfj//7+LgebfZmA4UAkA+ItgyBS66DMAAABWZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAOShgAHAAAAEgAAAESgAgAEAAAAAQAAAOGgAwAEAAAAAQAAAG8AAAAAQVNDSUkAAABTY3JlZW5zaG90rh38bgAAAdZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IlhNUCBDb3JlIDUuNC4wIj4KICAgPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4KICAgICAgPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+MjI1PC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6VXNlckNvbW1lbnQ+U2NyZWVuc2hvdDwvZXhpZjpVc2VyQ29tbWVudD4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjExMTwvZXhpZjpQaXhlbFlEaW1lbnNpb24+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgr6qJfyAABAAElEQVR4Ae1dB2CURfb/ZbPpIaRCCC2h94BUFRQpIiKcyNnr2U7Eciq2O8/u2Rs2sB169gaCiBSlI0V67yX0QALpff/vN19m82XZTXY3Gwj+98Hmm2/Km/LNm3kz896bgDtfWWFrlhiBB69ug3+8uRpnd4pDn06xeP2rbbhiYFOUltrw/dx9eOCatpi/Oh3LN2Zg3H3d8Pz/NsNiCfCn87eLv7/UgB4gEPjEE08+2bV1NBrFhSIqIghtmtVDg5gQxEeHoGVSJOLqhyApIQwpSeGoL+HJjSLQrGE4IsKsaNU40p/O3y7+/uIFPTSIDcW6nVlo17weAj6fucd2zeBmJEg/+FvA3wKnqAUOZRTg9hf+wJSX+sKyamvmKcrWn01ttUBOXiEOHDmB41n5PsuioLBYcGYhPTMHNpvNZ3jrEqLi0jIcOpqNw/Kj+1RCbL1gvP6PrirLAGngP2cLn8oWPU15TZm7AZ9NWoItOw4jOCgARSU2JDWsj78O7Y6bLu2FwECLxyVbvn4vPvh6EZat2Q1rINSeQGRECIYP7Iq/X3ku6tcL9RhnXUuwe18m3vtyPn79fTNsZQbxsa0GntMed1zdD82TYmq9yMdzivDhT7sx9qo2CHjhs822h69tW+uZ+jPwXQuUyqj9+Js/49dFa9EkLB0JYfkItNhQJsNpRkEo9ufFo3lyU7z39NWoFx7idsafTF6OcRNno3HYUSRG5CE4sExmQSCrKBgH86KBsHh89PwNp6STul1oDyMuWrUb9z37NeKCMtAoPAfhQSUKQ26xFYfy6uFYcQzGPX4V+qQ29xCzZ9HJjo5+eQUmPX8uLBxB/XBmtcCnU1Zg3uLVSI1JU8RCAiTIZjXiwwrQOXYfDu7diWfenu52xVZs3Ie3PpmNLpK2WVSOIkAmDhCc9UOK0Db6CEKL9uPOJ748iT3dtGkT3nvvPXte//73v1FQUGB/d8fxyiuv4IknnkBxcbE70b2KQ3b93qe/QsuIA2hZ/7idAIksQoixZf1MpEjY3U99hRPZnpXf0wIlysYMCZBguWdUa0/T/+niFxYWoqio6IyoF2fBD76Yh+TIw2qHOj4hAaGhoYiLj7eXn8TYMiodsxdtwsH0LLt/VY43/iszYOhRxERY4AwnibF5vRPIzDyOOct2VkK1d+9eTJo0SRHe1q1bMXv2bLBNv/vuOzzzzDM4fvw4vvrqK/znP/9BRkYGvv76a7zwwgtYvny5wpOTk4MjR47gySefhNVqxZtvvqnCiWvWrFkqDp+bN2/GSy+9hF9++QWrVq1S7o0bN6oB4OWXXz5pcKhUSHn54NvFiA3JQ5wMVI2SkuSIzYLERo2EbRe+uxwSJCw6KBcf/7BUe9XK8+iJQox5daXCbXno3bV47bXXcMstt+Cvf/2rev788881znjLli2VcIwePRo33ngjrr76apXHzp2VP2SlyG6+PP744/aY99xzj6xfStX7Z599Bsf8GfD0008jK+vkTskO4lhnZ+ntmZ1GR0ZWHgqKihEts1PHzp1xy+23KwK8cOhQBAUFqY7F4pGVrB9Whi27090q7bZd6YgPzz8JZ/369VGvXj2Fg4QYFZiNDdsOnoTzsssuw//+9z98+eWXuPTSS1U7s12joqLw/vvvo3fv3oiJicFPP/2E33//HQ899BCmTJmi8ERGRuKss87CXXfdpQisVatWaNOmDfbs2YMlS5aoOHw2adIEPXr0UOlIkN27d8ehQ4ewfft2cCCYP3/+SeUye6zdnIYoaxZiY2MxUvp6+w4dcIX0x+CQyix7/aBsrN28x5y0VtzBQcaa3dK7Yyzuv/9+fPTRR8J6BKjnxRdfLCNepqrUiRMnVAFYSf44OnEvhw25YsUKpKWlqXA2Cn8E+pEoyKZoILvCRu7UqZPKo0WLFuAISDx8Esx50M1Rk3kQ/vjjD1Um9VL+h6MhR1yWkR9/7ty5KoQjcEpKCspk0c3Rlh+KcNVVV6kO5az8nAmXLV2G/Px8lc+9996L1atXK/Zox/YdauRVSE7zn9CQIJSIAEVpWQBWSdscOngI5/brhyWLFimC5MhO4FquqCQAocFBbpU4JDgQRaWBlXAukPY8Szr99X/7mx1HWUAQwkKt9nft4HdduXIlgoOD1WBA/+TkZDXocgDmzNesWTM1UMbFxanBgnEJ/P6XXHIJ+vfvD85sNlnc6gGV34WsLfvjW2+9pWZ9zpaEpk2bqmdnGYwee+wxnH322erd1Z+w0BCUlAWqfjVj+nSkduuGfdJXe/Xpg/MHDLAPNsWlFoQLd1GbEBcVjJfGdFFZWHq2d74TxEYjwY0aNUpF/Ne//oWHH35YNRIbdfHixfibfJxly5YptuC///0vnn/+eYwfPx7btm1TjaYJyFllOCNdfvnligj5AdjY5jzYqPfddx8+/vhjnHvuuZgzZ469LBrf+eefr1gfsip33323GmX5wfhx+bvhhhvUQHLddddhzZo1ih0i0TqWn/heffVV/L7kd1x77bWKNeIAwNGX9Xz8iccxc+ZMxVbpvE/Xkxst3To2wf7cSFWEAwf2q4Fil3AWa6WOerM7oyAEgcGh6N6piVtFHT6oKw7lG31B42QHJd7t0g8IhdI5j+aHYdj5HSrh5IBKgrjyyitVn+Cs16BBA0UUEydOxIEDB9Q33LFjB1JTU5WbCPhdCSQ4sqAcLP/xj39gx84datBl2JAhQ/DOO+9g4MCBaoZlPyCxdZBZLF5Y8AFCPBw4P/30U/VkGldwxcXdcaQoTg1QB6VMXbp2xa/yXdu2bSs7wYGIkRmSm1uHi+NwxcU9XKHxif/hzEKMenSxwmV9+7sdePXu1JMQszOy4x09elSFkW/mSBQdHa3YCxIpZz42BtlZEgQbhewGf/ww7PyuYMaMGarDED9HQs6a5jwmT56Md999F9nZ2WrB/uCDD6pBgbMeWSQCBwiuETiDP/fcc4rdnT1rtvpgjMeZkqMvWSK9tmA6zrLm8nMg4QBDlmqRzCj8KCz/HXfcofJkGfnhWfe6AA/deiFueugALuibikaJifhC2G+rsKKtW7dWM8WmXUexLbsRnrz3IgQFundMcdvl5+DHWavRtFNHO86wsDDccPPN2COEWFBqxcbjSbh8WHc0Soiq1AzMl9C8ubGjmChlIugBnG62qSOQgAj8nhyANZAL4QTAgZkzbN++fXVQpXjak4OqOzCwT2t82DQR+dYYXDOiC8a//bbqX+wrkcJyx8Y3wPTVxWiVnITzeqS4g9LrOGHCeQzubXAt1r8OMKZ0MzYSxQMPPIAJEyZg4cKF9iBuABBChIcmT082oH379qqTP/roo4oYNIuh2Ql7YgcH1y/8CHfeeSf++c9/gusCgs6Dbi6czescEilZTA1kOTnKMi3XG/z4Ez+ZqEZOpuPagrgJ4eHhuFk6FMGx/CRC5kXQT11+rk2++eYbVU7mr0dvFfk0/enYOhGfvPw3fPDNIkx/byFCisMQEliK58Z9ixOl0XKk0BjPPzgSF55zcsd3VWSe//3w7u1457MFeOitRQgvi0S4tQh33PcCskujkJ7XBLfLGdroq4zZyxUeX/mz3X0NHKw/efF6vPzRr3j9m03ITs9GpDUUb7w/GXlS3yP54TivVye8OPZS1Zd9nb8ZX2hIIAZ2T1Be1uISYzODbywkgYTEKf7FF1/EunXrlJ/5D3e7yNeTQNnRr7nmGtx0002KCDhqcfOFhEzCJIvqDIbKRsKHH36Iffv2qdGIblegy6Wf5nicsTQLNmzYMLUb17BhQxWFaxKyN0xHdlWDY/m1P586DxL49ddfj8GDB6t6ck3SqHy9ZY5/utwdWjXESw9fivnLd2D6vPU4dlw6VHgYbjy7PYb0bSNrGmO95Un5EmIi8cRdF2HEoM4yK67FvkMZCLIG4pLUlhgxsJPIE4d7gs5pXHIVXIqQq+AGyamGEFkjPzb6IuxMy8D3M1dj665DYK8/p2USLhvcBSlNTk2ZMrOL8IhsilJsDfePWy192AAZ/bVTBAnKbEKINu2nn4zQr18/W15enm3evHk2YR1UmpKSEltubu5J6e0e5Q4zHnrJB1F50W0Oc+Y2+zE+geXkT4NjHJZT2BoVrMMcy2/GoeMwga6PsMSVyqbz8j89bwFZ59uEs7DJUkF9F7Yt+w6/E5/8sd8R+N0Y/meE3PwS2/TfD6qqBX7zyWtPUnuCoGcB7eYulPbTT4ZxFONu57Fjx/DII48oFlKzjgwnML7exTJ8jL9mPPQha6j99JP+ztxmP8Yh0M/sb3YznGwp2UiCDnNWfh2mn4zPtARyBmZ/5en/41UL8CiIa3QuY3hAz91rHldxA45LCm6QkcPi8oHcVYKcgzLunw2KRcRwS1qO0qIAtSj84G+BU9UCIk2juB/OeHJuq7L94osvbHIua/v2229tchSlZj852LeHn6qyncp8Dh7Ltw1/cIHK0kotCr8q06kbZ3ft2oUykXr5/wrcmeZONmc3HgPxDJZHE/TnxhpnR260DR8+XB1LMbwuQXhEuE/2BvxaFKfxq5KtjaofjZDQmm9ynMZq+CRrmQYUm6+fREo3ge1k9leep/lP+uEDSqCAZ5U1BbMWhfXFz7fAr0VR0yZ1P31MbDxeeP8XOZMydnDdT+mPebpbYPFvU7D+98k+KUZBURkWrUlXqkxWvxaFT9r0tCApKizBxu17sV7kHPPyimQDyYq2stXeuX0LUAfQG6CA+PZdB7Bm024RB8xV56bJTRuga8cWiIs1ZEi9wVuX0tRGu3laP7MWhdWvReFp89WN+Nt3HcTXk+ciMCgCAcHRCAgUKaKCEhxauhOz5q7ERQN7olc3zw68jx7LwqffzEZBsU103GJgCRLJGGEP09cdxKKl69G1S2tcMrAHLG5K4dSNlqpcitpot8o5uPdGLYonPtyAdx44C0qLwr1k/lh1pQV27D6ErybNQXD9FmjRrhv+c98gtEhpjJDIBITFtkJEQkfMnLsKi5dXCNBXV/Ys0c4Y/+k0FAfG4pxz+uHpey5ULHNIRBzCYlJQLzEV67fsx7c/GfKO1eGri+Hmdhs1YgDuvbEvQiNjatRuNalnJS2KmiDypz21LcDNimmzlyO4XnPUqx+HC3uJqN5PO0UEypDXZGmswWFCjK0xZ+Fq0FaMOzBrwRpYQ2IR26Apzmofi69/3YsLe1fgtASK9kRsW2wV9vfg4Qx3UNapOOZ269i2mVgiCMDqLZk4t4shOsbCetNu3lbSLS0Kd5FTa0HvaLmbxlk8kZhw5l3n/FhXCpWfLsjLLxINlRMIlhmqT+cEnNc1XoQiLKAJPTNYgyNgsYZg3wFDAN8c5sy9bUcagiIbomeHePTtHKeitGxceQ1osQQiKCwOW3fsd4aiTvuZ261rm2hlKzVG1Imi61VeO3vabt5W2qxFYaEWBYGmBSjgTFlLTzoZtRE2bNjgUmHWXeVYLVzNstSGkvFTTz1l11GjagxVmwjr169Xmt7qxfSHCqe//vqrycdwHj58+CRJfsq/Ulb2VMO8FYew80AehvdNwlezdqNv1wR0bOmlpgcFKOV0wIzz+zl7cN3QFFx7UYqpasaxgsnjjHN+O3svwkOs6CRt9euyg6r8ciJySsGsRWHRWhRUpqToUB9RcKR6EAWWqaGgTRDwQJVukbNUhaUQtPldK8wycO3atUoVhTjMyrFMyzRayZZxSQwHDxoNwXeCMyVjx7Tm8tFNwWAqCFP9hWowPBQ3A9OLrKvyonkF/ghUAE4SUwcEs2Iy24Ha3gS2DXULzUrKVPilSQbaRKFpBZp30OWgOlRt2UoJDwtGfFwMinKMGW7XgVxQBGr/kTyESsdKig9TZS4uFFOFpUVo1riC3VIBLv60a90MxbmHVajGuVtwT1mwH62aGhouZaUlKM7PQLvWTV1gqbveju1WUFSKZRuP4Z4r2+LRGzvivqsN0ThP283bGpu1KCxmLQoipFwf5UA5E1BqgRrt7FQkJsr9UZ6P+ldUyGWnpjIlgfZDqDDLGZXmDKgLSILTyrFkWx2VbJ999llle4RqUUxbFTimNZeP7hEjRiiZw169eqn8qcVvnoWpK0g9RxJKx44d1QzI/DgIUPODA89/TYrJ00XzWtePdSGhjRs3ThXxxx9/VPqJVGJl/TgTckbloEU9TLpZ99oAHmIPG9RTiHAvzk+tJxbRw/H+5G3cxMTeg7nqkJsdqSBzGwadf5Y6tnCnHIP6paKsKBP9OocgObEC53UXJWP64oMoLSlCfsYWdGiXggbxhj6nO3irivPBa4/g8IE9+HjcY5j69Xhs3WBYUagqjbdh5nb76wUJSDuSj/krD2PPoTz8suSgDJqlKC3yvN28LY/WomB664/zD+C81ASlNU47MxRaZofkyE7iYEej0iSFa0ko1JanIZ8rrrgCt91220msKzXRzQq0nF2pHOtMyZa4mA+B+bgCZ2kp2KvL98knn2Ds2LFKKXfp0qVK9InmLkhgWpmUGt3U+KeiMvUYp06dqmZLDjr8QCQss2IyBxkCZ0cORiQ2znwEvlPJWLQs1KxOwr+g/wVKwZnh1ImsztQC43kLyc0a4LrLB2Hnjl147u2VonBrrP+SYsIRWHIUJSe24ZLBvdC1Uwu3s4iMCMPomy6R2X47nhn3PYpsEUrRNfP4CTSPL8Ksw2vRu3t7DO7fzW2c1UW85d7/qOOOm8Y8raLW9tEH2+2GKwZj+7Yd+Oy7A7BZo7BlZzDWbzuMpKhc5B3dgkuG9PGo3aqro6vwqPAg3HlZKxVsvU+MjxKoWUA7M2aghgOB2hA00tSuXTvQuBJnCc5sBOodmkEr9Wo/UQ1STmdKtnPnzlVh1W3uOEtLo0K6fESilXG15oOjAjDjUB+QxocmTpyIYjGWRMIddZlhvoPEaFZMJhETSGgkUIaz7tSFM+el61cirBqJmdwCLRDQypjZEJVC5sM/PEBvkhiHho32Y8PWvVLOfGzcsgttWzTGvX8fJXZgDA0QT7KMiY6U5Yho6yc1ksP6XTh+Igcz5q0SHbsGuPf2kagf5VtRO010+ulJWb2N26xJghhIjkV8g4aq3Xbs2ImYsCBk5ZbigTGXe9Vu3paloNhY2ll40xKBHcgVcCajzRf+uBFDux9kAcnycWYxw0UXXaRYVVpu4yySUq4cy93P5GRDyZa2Y8iy0aAUFYBvvfVWe8c249JlolqLY1pzPLNbp9FPc9iI4SOUoDDNKQy7ZJjaeLlk+CUqilZMppmL77//3p6MhE42k+vYN954w+6vHcynS5cu4MYP15c06UFNfT0D63i18bQGBYLb7VcM74sBPRsh7ff30TpZzvW8IEBdPovUJ6V5Q7G43QO5W35A+8Qi9O3T0ecEqPM7npGOlx66Dnt2bNRetf40t9vIC7uodouLLKxRu3la6Ky8Ynw8dadKFjD27dW2l8fIekA2LvQIzxB97GDuzJwFOCNo4AzGmY/pzOnpzxlJ6/GRAElIBM6cnFn1jMW4ZN/M6TV+Rz9zWnP5zG6dxuyn8fHJmUuXy+zWYWS5WVadntr65BBYXnID06ZNs5dV58W0LBvrwY0hgnmWVh7lf2LjEnwuO8q11OuP3ojYHLlTIbEJHh03CTFiL8VbKCrIx+sPX4/0DSuRFWjFDWNfQN/Bl3mLzmW6/Xt24OUHrkSCEGJaWCTGPD0BnbtX2JNxmdBHAZ7mr2VHHSceb4pTJLNgWnqeuvnMQgIkmAmQ7yQ+MwHSz0yAfGen0+n0U/vrjs53TYB003iQJkC+EwfBnF55OPEzpzWXz+zWeMx+Gh+f5nKZ3TpMl1Wnp3kO/rhO5tEJQeehn/Rj2ZiGxOeKABnP17D2jwV49cHr8EbWCWyWgfTSIwfx1OhhOHJwr1dZ5eXm4Pl7LkP81nXYUlKMaYX5+OK1RzFz0kSv8LlKtHntMjx7919wz7EjWCXl/ig3C+88dgvmz/jWVRKf+p/u/PNE7neS7McQ/HdR+PTTVo+MhNpv8KViYcw4EK8+hesYh4XQli+apSLMl7/95DdefqOVD9D/wlGIjHL/3LCoqACzf/pSzfT3SjnfkC3XQ4IrVdxHxN0htRdatO5cjt37B3dEly+erRD8IH9Hym+9/PqJtZfjcljZIbWn5NNFhdfGH2/z37R2KZomxSs9x5qWy3wXhV+Loqat6WF6Hstws6emQHY53JKNpnL0Mkd2qw/LUoGwRX40PUjr1JGREWjc2H22lGetI0eOVBatl9OQs7Dm3H4jAfKIhwNISnJMJU6GeXoCZOHDArJUuReLxey9kifhuPwKg6y4THaemU/zZvVrhaOoSf6dWw9XlsFVgWv4x6xFgZKSCiNJ8mH9cAa2wAXdu9uEiRNSge1++cnRTY1qITvDtsHR0QqfiDzYkmJiaoTPVeKrRoywyVaXymeBPLukpLiKWiv+pzP/9OMFNl5VT/BrUdRwRKsLyYtldrlLNo7Ol7Pck/dvvSvhLJGIIr7hYmois5ZE8rhB/4Rs6jGf62VNnV9S4l1hvUx1uvPXWhQBIi1vu8KJAWAv6+VPdgpagCKD3CQj20ZBBorV0cS8NqDM89zsrGz06t3L7dJQkogCGRRK4I71ggUyNwlw95ubZzR6zKMa4vYWiJfsIDe/mM/u3buV3HFEhOhESl14dMQLX2qaT1XlY760d8rdbIoe8uiJcr9sO+Y/QAQ5eCeJJ21XVX6uwmQClHsxbIaF9J0H/px2HdU8/yf9I1cE2ES43CbihDYRI1S1dGRBacmMtjyl07vVCiK4oOKJpI9KJ0LuNjFFaE9Lt4j12W2x2gM8cIiMsE0MSiv8zIcgJjMrldEX+VRVJDnPtcnAZZO7L9RPBgYbLcCZgW1X2/ZOzdbW7FoUrijW71/3WoACEZRrpUwv7/9IT0+HtjquS8vNH0ruUNDCHeDZLcXyeLcI7wnhbERBCzP88MMPSiiBooneAK2pcSb67bffVD4U3OcxEWdBM9Q0HzMuRzfPfXnWS00hasRQU2bQoEGVolEBgBbhzfdjVIrggxenWhQ+wOtHcYpagOeQYrdTCQZwF5TXFfBOQDOQxaNsLTu+jPLmIKdu3qREU4S8u4Pyt/r81hyZUlCUmWW4t0C2jxopzIeGfju073ASKl/kcxLScg+2F7V8SPg8r6bmjVhkrxSd92BQpc98Dlwpgg9eqtSi8AF+P4pT0AKNGzdWkkcU/ZstooNbfpwMMZ5rv76N6y9qtHD96DjTOCseZ1fOrLzbg7Ngt9SumPzM03jTJKqnb9vt37+/MxRu+TEfzjTMh5d6UmyQal/UxpkuM/dOuVzWF/lUVRi2CdXUWA/epblG6n1fp46Y8frrqiwsH7WCnA1EVeH1JMysRVHpLgozX+x31/0WEHbKNvnWW2ybw6y2QitstyY2tC2aNMl+P4e+g8PdmnD9KGJ7tp9fedk2L7GB7ScLbN9EhdumPnC/WkeJmJ/9Xg93cTqLp9ep+jl+3Djbm5062vKlDmvqR9gm//OfNVp7OsvT7KfzPZCWZpt01ZW27UEBttekrsz/py6dbEtmzFDtYE7ja7f5LgrRq8rzNX4/vlPQAh+9/bbtJSG6g9JxbEEVv2nSof41ZIhNlJo9LoXoQNoe69PbtlgIz4xzY3iQ7dn27W1TZUPIl8BNkcmPPGzbEBlWKT8OKFOFGNYsXuzL7CrhEmsQtldHXmr7uXXLSnn/EVPf9sWggbYvJk6sFN/XLyTCSfP3K7QBvIvCbwbfE0bCdVxukFBxWFrWdSQfhchorjBFPvowrjqRYcf6yOXXoNUFFygW1B021J7Q5Djy+Wd4ZMEcu8+EVu0QNPZB+7uvHNwY2SHHK3mi5/nFyop15ufhEZg3fCQC5eywp1g4qE1IE22gJ37+0Z7FRBGAL3vyKfu7Ow4eeTiuyatLR7G121/4Q12N5r+LorrW8iCcZ2sf/PdrDBp+tQepvI0aqBK2MWjRjiQj34K9mZ7rEtoRiKOo0II80ziSJWfo2ceM/Mzxauy2JiGpSxIO5VkAExHKPd9I7GLsWO4+VuNcyhHYYONd2A6Qnh+AVSbvPWUBKEmvvFtrTyLeotZA7Qa7l4y4ePY/D3lMhP67KCqa0Kcubq1/Oek3jHn4JTveEtl02LBmsXSAysrP9gguHOwXYWGxaN2hu5rV0nZtRcbR3ZW+P5NumjMN1vw8O5b6nXugQdMU+7vZQZxR9RshpZUIYUs/ysk6ju2bl8suoKkXSpz0Q/tw/OgRe9LwelFo3NzQArd7mhwBASEidH0OgoINJXBTkFvOldO+QvsJz9njro+KRc/P5tnfa+Igx5Apddm+ZSZi44JY7UrA71MogusaaFEuLCxcv1Z6spWOZ5ageYv+aNCwMdVplPDBnVf1weaNFEF3H/x3UbjfVjWKyQ+8YNbHGDqyJ8LFnKCnkHHkGBb89pWkbYBmyQXoOrSVIkgzngFDx5hfq3Xv25UmGgzT0KZDH2xc/TWGjRrgxKJ2crV4zBEKZSf2p+8+Rr9Bt8AaFGwOcst91rCrAP7Koad2+OBJIly1dCouF0O/3rLn5mJwqfHd/6bjvAv/hiAhQm/BfBeFxX8XhbfNWH26vNxs1IsWbYdI5yNrdRhiG4i6k02s3qWtQ9tOrX3SiZqkNEVedhrWr1yEnud2ckKA1ZXq5PAQEWuLTbAi60TmyYGn2YcW4srKinzSdqwKCZn4aPiqJmDWorD476KoSVNWnVbZnwmozOpVneLkULkMXD64bwWby8p40F/g08NoLpM489c5UM1f9TcoEUsCq5auVr/MzOzqq8CNN4e1ePWJKsfgXRRjXl2pPP1aFJXb5pS+/fbTTPw67Vf8OsMQlvY28zQxVjThlQn4beZCiKSot2hOSrdr0zaMF7wLflvK/Yc/LViswagXUorNOw8jIty7da03jaO1KCy9O8Z6k96fpsYtYMNuuYJs4LCB6HtezVZBTVu2QD25Qbbf4L6goSZfQUr71ogQU4h9B/QWNsxXWE/Gs3ndcmRlHBYTlDtlnbrs5Ai17GOxBoq1wShE1ItEWUmp3KRcKjvEvuU+HKvg07soHJH7391tgQA0SozFrKmzMHvGQncTuYwXlxCndv62iq3VzBOe7cS6RCoBcQnxKnj1khXYtTOtqqheh7Xr3BNRsQ1FYLwFOnR1X/3K6wydJIxKSELvnu0xf+avmD11uqg57XASy3de5rsorLyL4tW7DWNPvsvCj8mdFhh6+Qh3orkVZ8hfL1bxglCK47mFiKkf5la66iJdrPDacDzjGCLEUtyfFYJF4L2h/Fp0aI/AUrEGHx5Tq1U1a1FY9V0UtZrj/2fkNV1LcTHmwYKsabuOsFmqPqwnOrKXnkj2tGzXRmYr10sX4qpFjtX7HiSFKpJ7J1T53OCpW7VtWU1eNsEnrKr3pxMKv1mLwup4F0U1JfAHe9ACQdYgHEw7gRL5aFa5ytpTyMnKwbGjBQi01sPu7buR3Cq5WhRWUXOqCtIPpSNfJFTadGyNpSLhM/yKIW5s3wegaQvXeZfKOmrf7gw0b2mYr6wq/1MdFih2U5u37InPJvyIZi0SarQjzDPH/XuOIbFxTzkPrbqdq6un1qKY8lLfirsoqkvkD/e8BUIjIkWS5GJ89PYvonbEPW0RnTKj0VOS2U/cekYpKwuWs7yRSoJj/uzpspO6VgIdcOi0LnDpYAOnCF1ZInD2ecNFciYOmRlH8MEbU0Sx1ojlWDaeiVXy08jKnxpnSYlYIe8+WExE1HeIcfpfA0VZuV2nHkgQCZcTmcfUpou3pbIGWZDaMxZxDZKUGpm3eJjO6V0UNUHoT+u8Baitnty6I5KathA9tcqHu9yBy8k9IbZiXLB4ARax7xIkP5ldhBguHHE9imijRc4NHYGs1onMo6gfE1/lrGYRnEEhwSJeZsxYPc4ehC7dzgXv0TBDkZg6LCkWS+SRUWZvl+4gkZIJDg2rZFjZZeTTEBAkdnISGzdHg0ZNq829SGzPWOS7WUXh1xkoLrQGkjJmnPouCivvovBrUZibxrdumm8IkxnRcZuErE2oHCuEhka4lWFwSCj4cwbEFSJEwHBPtMEZ11lno5JtSWEBwiLrOcvujPVzp204yAXYSqUda8ZuVtdI+i6KS/slwbJqa90TNaquAmdyOKVKOKOVUGhYZrCigjw5kyqw3yLsbt1IeMrERWGegUsYR+KkHGeJmK/3FIivuKhQlcdWWqzE2VhOlq2srNRTdGdUfNad9eS3CAw0dlwK1XcpVALatVEZsxaFVd9FURsZ+XFWtIDq5HKvQ6Bs1qjr40wsDTs52b/S0kAJq34EtuOSTYfgIJkdK+EqkwNn0QwQzYogN2dGEnOp5M/NhkBhgc1bf7psCHCvbBU1PjNcXCbYhB1n3S2BXOXqbU9px1IZmNQ3C3bKMdSkhvouirFyNaHlxc9pON0PtdkCxkhrEIVVTL2biYb5Un0mOCRcbcgUl9/qVFV5imXGCxFitZJgTQRo4BIWUwiJ6yDGqw5YNhJgSIis6WQtVNEJjZQWS4DBBsusXSdlQ6urYBXhHHxkxJL1rLDxandKEyATWRQ3wHahsLaSA64Cl6dBWotC5eTXovC0+TyPz9nEarEqYnPs5GZsPNIoFVawKiDR2Lg5U842OY/LDkS7F7K3KfGrArKu3EByJOaKNOyYQthStpIaag5U4KwbLnIfQSI36vqbSN3VujnY50L0Z4wWBTsR7UN6A7wTsc6AaHS7o8smm5cIcKL97VgPWymJy9HX4Z3EKuxUdRAgmZaq3dGq49pks0JplVeH8AwKDxAWu0TdJF113TkwuvP9PKm6Sy0KdvqXX35Z3TZ79913q6uh3UG8RczUuQuexKUNEhpg5a3ANNLqCdx8882eRK/VuAHC6vDyUM5iVUFJsWyAOLCXjvG5wxcoRwIFsuZzjk/ykJmX4YxXHT7u3ooRNWFd5QjFafmoSiVrTNmk8UZh17H8rt5ZF/PPVTxf+nNnWB3HuFIVkzKVyGZVqXwXx7ssfVEOp1oUJECyJp999hn+/ve/Kxv9bJgVK1aoewOYMe8soBVl2uunPX++33vvvcquP7e2GV+sdikb/yRqMWuuykuCoo1Hc9w9e/aoMM5aNJJE4P0Aa0QI2Qx9xNgP7UTSHibDabyVYM5Lx2daWnauSxAo53yBsvAvzMt1cVgsH1s2CLhDp9Z51RSenUcRouArkEs9uZNXLGnz5abe3Owc+W7Zso9inHWxjZwTl5EJiZrHG1zz5Au+fFFEVjuDQnR5gidP7rQolO+s1qzVDBDVFLvaYOpN6p8qd7UpvIgg7UHc/HF2C5I1X0FuHnLEWFZudpaxW81dZhnE2B48Qw0RcxfGmtGL/FwkMWtRWHu2rxBUnTt3rjIRznSdOnVSya+99lp07dpVdf5hFw9DsYyK48ePx1VXXYUXXnxB3WJLoqRV5ZYtW2L06NHo1q0bpk+friws09LyuDfHYcxdY5SFZ3Nc3g9PQ68rV65U97zTBPu6devU/Xi8gOTCCy9UZSAuXtYRFxeHVatWYcL4CdiwcQNuuOGGSnlNnTpV3f23b98+uVpPhHDrCkjn5YYMNzn4ceXbi+SKGLqUjlAoMxDlUiyyZuQGAf1K1IE8mT9G5Fm9/HGAAMHFnVZqjqtjCcETKMQZLH7c6BFEDhspFbIvVNdR60qFNkARINetRueUMzJhUSmXEywDB8vEQhjhPKogHnN5yt9ZTv6TsqqfiWDdOZ9jHItsJlUCEovZQwZ1+a/KRof6J3FYF+pRBnC8oa8MKJwA2GxGmDAEitdXLcoGtZeTa2ejjqKUXFQsE0ueTEQifCA7yxwQWRd3ym8upjtualHYra2ZtSj06KCRkGA4DdP0OTv11Vdfjb/85S/qnZdG0mJz27ZtlTVl3nnAG4JIJM2aNVO3Bs2aNQtviAXn/hf0x/vvv68urqTlZVf3IwwYMEDdD8AZjzcNaSLU5aG58j/++AP/fvzfTvPijD1p0iQV/corr9TJ6sRTdTJ2NGlPdgzKW+bmHUeIHDEYLKHosMnsw+6tRl32IILqTIb4GKVsdAcnEVpkTcMdzVDpSHlCjOrKbtlE4YEzdzJLxbgU45D4BY0prUXykTsgygmKxMw8iZsRuWtINoxlzs/PZa9XBElJEpIa77QvkzUi3QRu2gSKTl6++BPoSwLmbFwqMzyPAZSyseAmWYWIxI5VwkgoioQlX7pVIYmATloksHvJACFuDiwyejGGgkBxB8iAw7ZQ5CVPtQFWTjxSIVUnd4mIhJefK+0qZePucm2CSy0KXo21adMmtG/fXjUKPwbNqRPIeuq72HWl9FNv39K2f6tWrdQ9A0zDK7B4+QeJkkQ1dOhQNeoyjKDT8WoqgtyOg9Quqbj88svx448VtiBVoPwhO7ts2TLIJZYgC+uYF2dyAsusPqp6q1t/jDZjRwpAWGi4SNNUSKVwECQolkyIqqx8rcK6WKVTcKZTHZOzlmy6lEI2DORsi6b8SERlJWXylI1TdiJZD3K2JTA9D+JLhTBtklYaXp17WQLLRbMknARO5JyJiZvxXv/PP5RFtvqxCciUu+V7nTtYjFbdiDuv7YdO3c4RvJyFhGvq2htNk9vi1afuwlm9+6sZdd+eraIMPByx8YlYsWQOdmxZp2RgGzdrib6D/oK2Hc6SJpChgMQv/7g5xMGD7UNCUojFnwNIQECQEK0MYOKvBiESl8RTsxQrWF5POmsKXPfqdqsprqrSu9SiePaZZ3HH6DvkiuXGOHbsmGInyQLecsst6saexx9/XBGTRq5GTnlJSUnB9ddfj9fFln9ycrK6TINhXFdynUlCJPs6atSoSnGbNm2K22+/Xa03eUcBifXb775VH9HZPXi8apqDAVneF154oVJe3EjivQycrflh9QChy1rXnqqMMnto4IBE1pIEw87NXUtjfSgEIgRRIuwmNcA5g7JtaV5Qd0R2WBIDZ0XiKCrOl3dj9uBsIQkUPqZT30w6LfMjsRv5CesmfoZWvsxj/M8ZRuKPvHa0CGefh60bV+KbT97AJZffqtjgfzw2Thddxd+xaY2YAmyL0WOfV/6rVyzA5M/H4/GXP0XvvkPw+UevoGGjJhg84lpFYFISZsKiCUh9WAeVkoQtLhlYdNk4yKj6gOVVkRQO7VQYFCLBKkSr6qGIl0TMmdKoi86rLvQNsxZFwP3jVtsclXrJeupZjxVkxyerQ1ANJE9+II7cukKcmTjzERifGzycGXUcplMdQMLNcTlr8eINHa7DdDo+dTr9VJmU/zHnRS+NT6c3x61ttzO7o455slysKwmL60OCcUQgbB1nAGlXEqDqTDKjqY7D3lbeWVUCzlichWTGIj7OIHwGCiGxk/KbBMoalHiMhBqBpGP+8pNIEibvPO4gbkmoOBNxG2tKG96UmTDzWDqiY+Nx6MAedOvVH9fe9jBuHdUTHbv2YWqiwMWX3aQ2d5598AZ06dkPu7ZuEI38RIx+8EURXk9RRZ74zrNChE0x9LIb1Xu1f1gvJ5GM/sf6G3kb74xo+KmZnIUi/ypx+F4iAxLjBQprzrYisK1UW0vljXaS+FJ3BgcGBap1oRqYVDupJE7/sN29sTuaV1AKym1f1CcR1vtEbMYRzATIME2AdJsJQRMg/TUB0m2Or+OY05nj6ptvdLgO0+n0k3idgTkvhmt81aVzhqs2/fixlMSJdAiOzOzo4SLYLXewyE/YRHYS2RDg+sodUDvRwo4GyWBnrOeMtaDadBFGVa1reEThBrBsXLdxpmW5FD5JRy2KIZfegI6pvVSn/NvIHrjm1rHqqOLhZ9+vhHnbptVo0bYzHnzqPRw9chD/umuUHJPIetJbkM7PYaK2gWx4sRzak2DZZuRQhBwVS16mhBNE80SWArXRn/xaFLX9dU34VScXGUQlm6mIrKJ7SZ8XEMITQuBZYon0AGeaDSZ0Eq9QTV609ynzlj1I5j8mlncKY4uguMSrThaVZSuSslG9iR3QDBFieXvG1M+weO5POCEz4gVDRsmMF65wvvDYbfaoKS07oMc5gxEaZmiExDdohPufeAtvPvcPvDj+R+XPTQ+uU+sSqLrLcUSwHFM4EpnRFFJeIdICaR9ncWpSF7MWRcDYt1fbfC3ETZaQs6me3TwpLFlhplNCzm4k1OyrjmqM6sWV2GkdVttPV+woZ0Cyj9WaiReCKJQPHlLemZ2V1yDoPHV2ZSZAZ3EL8nOEACKdBdn9OAMGSNms5TqG9gAHB9eajEsi/LMAz0MpKmiphvvgDi+5dmd9kt/DG3a0qLgMael5aJkUCYuZALnxouGee+4x1gjiwcN7TyRduGkiV09pVOos8KabblIbPN98843d35mD54Y///yzsyCnfjfffHMlf17lLPeteyVlUwmRT1/kC6ot/eqROru0xDEV4/BMrGqQCIwjnaQqEC7MbahYf7mdpE5HZN0r+AjXRVX19qShXKOyh2gtCnpU0qLgGR9nIp738RhAb/l/9913SElJke95svQMz/QoIUPgDah0M54ZKOXyyCOP4KOPPsIVV1yhpGyOHj2qriomC7Z161axObnLnoR+JCbOcgTi01I4OpKjZAzXSJTi4V3tBC1ls3fvXpUfzxc1UB6VNyixXCxzbQPXGhRJo/iXS5A6KqvY1YzKRnqL7IBSU79yO9txE1ehhHNToZqNBbKgjMuZzhWw/Sk1w0PsPxNw/c2zTW6SuQKuGdk+hoaJq1ie+5u1KETrhVtjBpx//vmYPXu22t3klj/v2qPkCqdh/pxJz4wbNw633noreM83z/eGDBmirj4msZmBB/s85+vVq5eapSZMmICRI0eq44ZBgwap88nXXntNJXnllVdw4403gjMzD/zdkYzh0cQ555yD77//Xh2FaCkbsogcyZKTk5U0EK+Q5p3ow4YNw1tyPfO0adPE8Gu0uag+d3O9QfMPPOQOCBS9PenM3PyQPVBhc2RjRoiTYmu0BM0d5arAjqsoH6Uy4PBciwffZOFZT4pZKRZTsGtJkKrwkQhZtgIZ8HjgTnzcOZQ9Qznsl80k2ZzgebGhb1d5zVgVXk/DSOgcpDjhsC7s9I7rNE9xVhef34BseIFqRxHz4wDIjSlJWCqEybrzSEiZ7pAy+RLMWhRW810UPMd76aWXVCM899xzihBmz5qNgQMHKnlRfjBH6ZmHHnoIlE5h5yfh3XbbbU41HzjjcOeTsxxh7NixoNTN0qVLQaLjzKtlRjlrMoyCA5ytHKVwHCVjeNjPNej999+Pnj172mdm5sMP+e6776q8WU5uw7NT8QyTgwwHj1MB7FQhcvklR1bqp1FwWG3DSebciAmWtZY7W+Isq0UOrrlRwBGcNlFO5GcqoiaRh8hREnXg2MFYzwDJj52aP/ljPB0qzHx5JkkpmZzso4oAJbJ0xGKxwB2ldk2ZhGXnlr/GRT9fEQrxWKQMvoJK3JhQtuK6OeDJ+pd1ILGTk+DAxV+BtF1hgUiICSGGBofJYElRQxkUw0TXkG0n5fMlUIviiQ834J0HzoL1oXfX2o3/kuUkexkZGQlKzyQmJmLiJxPVGouNxA0XAs/m9DGGfjqGOxaYM4/5AF5/PD3yk8B1wxXkV+QTJTt07kjGaOLWLKw5f50XO09WVpbqnGPGjMEDDzxgjlbrbpaDPxId65qTfVzEpHKMjl7MQ2VjNmM5SWjq/IqEo0pWMRIrnUPhRBk/SHZIKcydm3MC0XHxssNJ3Jq9Yk8rnyE5y5CI2OnoyzzkHJE6dWRq9dFImGwKqVlJlF3DQyMVPpW9/GFadmKWRJz8K7+KctFHg/KVPCQj1YlJ6Br09+C7/uY6jE/mwwxUWbVb+RuSQvQnamZPcbhSKatBZSqSSmcc2kufkn887GCe9DMGOkOON0DOZdU3CZOycSYWa3H5eTmKANUdhaYyC2afg9aisDreRUHZTtUIkiUJ55lnnkHDhsbdelVJz5ANpVwpN3AWLlyoWEhdahLaY489poSw+/fvr73VU42q4tJPer7x5huYO2+umrVatmqJ5OQKKRxnkjEcNPgjYW3fvl2xxAq5wx/mQSGCI0eOqAGB8qwU0aOkzqkG1fmkA9EUH7uyWVqGB+jqED1QRmo5s+BhPgmGLBIHK47S7FDyp7wTlylZzdJi0UIQG6c2CSJrSaDWPIF1V51a3MShDrW5YytHBwaBVJ4luSPImaASnbGgpA/lSSIR8i1/VxtKUh/5ryIxP/5YT7LI9hmUwSQgAcaVKEIiBuvLwcMgEhKsEYn1VsOQ1Fe5RUyPRMW6M620BkIsMnORyIiUngJmQlce1f2R9GTDufur2kPctQlmLYqAnQeybSmNKraxNQGyAQlsRHOFXEnP6AJztuT60ZxGh+mnOQ+NX/vpONxgIWFpcEcyxiwto8tPvLoszGv+/PlKyJvrz6eeego9evTAiBG+MUfv6ohC18HxWShGmkLErIUGlo/lpTiZlhtlL1edQj6HmhlVJ+cMQNZQwoQg2ZupAaAIqvy7aZxMr4iFXd34pAq3am/xoJypDlB+QlgUIOD6TIWp9MRRnjFxiNOIK6y1vFJVS8l1suNKPIYZP4rOMZ3kozq1DARCaKUkXvoy/3J5UL4zpl10rjxf+ttB4qt/5VyCvAqQ2IlL6id56O+uv7k9rZsO6lUSj7I24EYafjNvjigq3Vlv1qJgnroSOn/HypglVBzjMo2WWNHpnT3N6TR+sx/TmAmQ7+Z8+a7z0eld+ZnxMu55552nVKc4o6ampvqMAJm/p8COqmU4uTljaBqUz2LlA7Hqr/Kh2TFITFrek3VRdRPRNWorBFhkFpECcAYjEZGdJf5ympHOL1oVwo7SeDA7rt7MoYUxEoVV7oknGagNI8FHYqHRJ9XP5a+QVYVbERbxEHQMpq7wUwRCohHiUIRlUIyRRP3VhCpp+L8cp8Kg0ZTH40MFk3EWBzu+wXIbOIiAm0hqrVdeBkWsijoZKuy2iKxRuZplMTgJuo02VIOcysuekX6rtadLLYpay7EOIWbn5QbOqQIeCBsd7OQcC2UzgIqzRkcVqhOWi7NKmRxnqE5EFowdR2YLzkxqnVPOZmps3Cwpo9lDeeqLPxXBlgg+oQ8SqsIvRElcxMF1I9lVpUQsG1qBqgOzc/OeBUMDhel4mSjnLLJ/Co8mfPFT7LAuBN8lPqVuzIOiKfjUOUmgzE2IlcS3c9MShOb+bpCm1E9tzJCiVTifumjiKG8rrtlJoM4gozAWnfve6CzIIz+XWhQeYfFHdqsF1n1/Jfp28X7Xb+GWaJSEtUX/Zkvdys+dSPN3tUZ06yuRdOTfiI/1vmzmvH5fl43OV0w7/UQoA4WZfLLTt+O2C7aYi1oj96dzqMhQcyI0a1FYf5x/AOelJtSoYP7ErlsgJSkcfTq7Z07eGZbtR0NwAlbBUWEBwVk8T/zWZwQr1rdr2/pokuhoG9wTTBVxjx7XO7IVfn6X6xYw30VhcaZF4TqpP8TfAv4W8FULaC0KC3Wa/HBqWuDo0Xz8OOcApi08JJsk9sVIjTNP25+DKYJ35tL0iiVOjbHKempPFqbMO4Df/jjqA2x1F8XDr67GpF/3Y8+h6o0l+6oWWouC+Px3UfiqVd3As2HTMew5VoyoCC78jV1FN5JVG2X+7weRWxqA+pFy3OBDmCEEaJPNnOjIqkXpapolTWHki+DC4cOHsHfn1pqi8zh9bp5IFkmqsFDzatJjNB4lMN9FUUmLwiMs/shetUBCTAgSokOw52AujsjMuHFXttwxXyjb6zas2piBPQdykZXrWpjaVaYN4sPAD0s7MydyPE/vFK8MFIkJoYoI06S8R08YIodO49bAMyo6HlYRGuCxFLX4TzV0bFVf7e7u2H4c//15P+avOFLrRXCpRVHrOf8/z+Csrg2Q0iAIx3OKsGbrCcxdkY4wEaD/bdlRsRlahHW7crBh8zGs2umZRvrQgc0QESh31WcXYffODCzelOWTlh41tDlsIoGTmVOM9GP5WLHphE/wOiKJqh+rjjcixNJAVHSsY3Ctv4++ujX+MiAJKYky4EQEYsfB2mdLXWpR1Hpt/59nUE9mqj6d41QrfD8jDW2aRap1SGJcCPamF6KxzDpWEfEKlLM8TyBWZtc+8iOUieRMwDHf7FQ2iA8Ff4QZCw6gRXLdu4lXFc5HfxKbRGGk/E4FuNSiOBWZ+/MwWmDUkKYnNUW7ljXvABaRcmnRyPdruCH9kk4qr9/D+xZwqUXhPUp/SlctsDMjDj/94b0e3uG8WFgiYwSHMSO5yscT/6yiGMSLoPLcTRGI3ucbRd3NR2LR1ZNCnKK4TdsPwLjljX2WW3hD353XutSi8Flp/YhUC/Qc9a7LlqAM5Oa1vyM36xg69xxoN5RkTtDd/nKT3eXKsWf7RhxO24RWnfuK0V1D88VZ3LO0Z4sJ2uXymZN1HBtXzUVMQjO0psFeFyBy8HUSEhKbgr/qYNfW9UjfvxVtUvvJ5lDtC6+YtSgq3UVRXUH94dW3QNrurZg55fNqI1KdKCbchu6prUXFqz2WLv8JR7ICxNxCFSYwXGDlaQePEdq3aICu56fKfR7LsGV1AbLzarabGREWjEYxFgzplyrHB0ewYs7XyMwVWVXZyf2zAGVeY+Rop0PrRJx1fhesXrsYG1cWIcfNtqOEqlLQ9rBBXN5F4SEef3SHFujfv7/dLo5DkP2VwtxffPEFzhZTHJ3O6oOYWGM3sE+v7pgvdm8W/PIjaI28QYMG9jRVOX755ReFo6dYP2jWvJmK2iW1I2xiL+jHhTPQunVrdOzYsSoUJ4Xt3r1b2f8ZedlIUfXqo8ITGyUqsxkLpYzUGaXlgzMdaFmhUVISenXsjyZNmqjqdOvaGXINGSbP+1ldisS7VqqDt996s7ooJ4WbtSgCFq8/Zju7o9ERTorp96jVFnhHzG70E9UqAlWU0sQolbe6jR99/DF6iv0eDRvkdiva3akJfPf9d2jTtp0dxRIxvsVrC/5s8MGHH6K3XL+nYbPcm+JoI0mH+epZKCYPd4ikU4fkKFiLxZCNH05PC9CqXbpo+ROo68c1ordAJWiNizhoNa+mwDKZcXp7a3JNy+GL9GxbpU/pBBltFJnrybaj9T4zUEWLSwhfgVmLwuldFL7KyI/HaAFegLpCrM0FUYvdBLt27VTKqNqrUaNGiBBjUIRiOS88X0yGaGXm9XL56aG9aeUqtDpFxXP/wQNiHyXP7hErbG5sTGUOhyu5aGFzu/fqqSRE2PFWi4lKi4s1Hu3xHEmvkB6hxYRmTQ2W156ROGxy70VfYcW1orU5rC64d+7ciWVyd2WMslh+colotcE8AAaLPqHj1Wh5QphRwpoOlG/iCzDfRRGQdiTP1iTBN+osvijcnw3HRjGCvFVMavSXS1e5CeAucNSeKYTXV9Zee6QThcntwy0b13yr/ZDYe90lo/q5F1yAKWJ2cljv3h6Vy1n5WdYpsga9UW5s9qSOznD52m+NWOtLlzVeTzfWdtXlfVAG081i/uJSH9x9SSKcufwwLpXzV78WRXUtX8Pw5bL4v6BzZ487J1mfoXJF+FwxJXlY1oq+IEBWJTE+HjnSmWhwuVuLFh6Xy1lzsKwRQog0sVjXYNe2bT4hQNarUUICiuXiXF+AX4vCF63oJo4wYW1qAqHCAro/f7qXk8bny1nLl7jcq8WZHcuvRXEav9+mXbuQkXkMS9ZvwInsLOQX0WpaKZauW4vDcjHr1rT9bpXuj/Vr8b5YF1+8fqM9/hLZETWf4C1cvhjH8z2bndZs3ohJc+Zgwdr1drw70tJokkUBTTPOWrwIOw4ctoefaY5Maf9Pp0zCJ9NnIiMnD0Uyg+eeSMfSzTvsVZnw7dfILSzBbDFOXRvg16Kofy28IQAAA3ZJREFUjVZ1E+earVvw/Zz5aJPUALMWzUdaZi727t2Bldt3Y+XWHVi1oaLzV4WyR6cuKJD1Se82KXj98y/wmvyWrFmNd775VghbTOGLwaZlQpTZBZ4RYZe27ZF26BD6du6Az6ZOwUdTpmLGkiX44dfZYMfJyDwqgtwtMHnOb1UVr06HxcTEoW2jeLRu2R4z5s3GtPnz8MXM2XhDLj76Y+tOVfYc2W1+77tvsWXPnlqpi1mLQuR9NXNSK3n5kTq0ALe6o8JDsFQ2bET1D6s3rcPBEzlo2iBe9AqPe2QoiWsx7upxJOddFrTEHWyxIaugUKkGdUiuXlzLoXhqjahYS1sxDh3PRXFelugRZsmZlij4imlEwrtff43BZ5/rmPSMeqfl+GCxOBcuO6a0VFdP1KgG9OyO3QcOqno0lJuGLz+vN/akZ9ZKvfxaFLXSrO4hbShHB+d3dy5oWVxSiGUb3NcsHyMX8BDGXnetMtdnGNitKMdFAy6uePHAde3QoWLSMATXXNgfBWUi1hURjjDRzigStjk4NAJ/GzEckTKQ+AqWL12CVi2SsT8jF9kZR3D22Wf7CrVLPF27GIINPVo41w65bpjRdi+N+btLHDUJ8GtR1KT1apjWFQESbZA1BOemitiUh+DLQ2RmHVvf0BtMatioUkm0HkenyHqV/Gv60rO3Ia0So+SmW9YU3RmT3q9FccZ8Kn9Ba9ICwcJ2/i6yrtUBpWYcrb47S5Ml63BfgFmL4qS7KHyRgR9HRQvMkPsPg2og7sRbl+rL/YlHZbOkOlgrh/tdxLR/dVBP8KW0aoWVsvNnrUHZzPkUiETJRcOHe7SmNac/3e4pU6Z4LbfrTdnNd1EErsgZ+OTNl7TAHS+vwPQlhzC0TyIuHrsA6WJ8KFqsgt3wzFJEi1mGDbuzcf+4VejVIRYf/7wbz03ciGuHNPenq6Zd7rr+PIwZfxCRCU3RLbU9nvjyGLp164A8a0O89XM2rhzRC3O2WvHFwiI8NmYgXpx0ApuPRuLO6/qqdGExjdC2ZVO30mUGtrSnqyq/GWtL8Po3u/H4XYNPys9ZunE/ZeGbD57AAXTAbzsTnJYzOjFFVIKCz9j+cv3Is09pv+7RLlYuTrKit9BTgKjWmI+WvCFqf5o/cQtkZmZigGyUxIjo3HYRHLjrn//EQ/Lzg+9a4P8ANck0DbbmFsEAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of more than 11,000 tokens(generated using BERT) along their corresponding importance weights have been stored in `Data.csv` file.\n",
    "![Screen%20Shot%202021-12-08%20at%209.56.43%20AM.png](attachment:Screen%20Shot%202021-12-08%20at%209.56.43%20AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = np.loadtxt(\"Data.csv\", delimiter=\",\")\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(y_data):\n",
    "    y_all =[]\n",
    "    for y in y_data:\n",
    "        if 0 <= float(y) < 0.1:\n",
    "            y_all.append(1);\n",
    "        elif 0.1 <=  float(y) < 0.3:\n",
    "            y_all.append(2);\n",
    "        elif 0.3 <= float(y) < 0.5:\n",
    "            y_all.append(3);\n",
    "        elif 0.5 <= float(y) < 0.7:\n",
    "            y_all.append(4);\n",
    "        elif 0.7 <=  float(y) < 0.9:\n",
    "            y_all.append(5);\n",
    "        else:\n",
    "            y_all.append(6);\n",
    "    y_all = np.array(y_all)\n",
    "    return y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_data()\n",
    "X_data = X[:,:-1]\n",
    "y_data = X[:,-1]\n",
    "y_data = scale_data(y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ecad1542fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mentries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 242\u001b[0;31m     scores = parallel(\n\u001b[0m\u001b[1;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "print(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Predict with Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.49556737588652483\n",
      "Mean Squared Error: 0.8643617021276596\n",
      "Root Mean Squared Error: 0.9297105474972626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.72      0.71       464\n",
      "           2       0.64      0.68      0.66       453\n",
      "           3       0.48      0.38      0.42       145\n",
      "           4       0.48      0.33      0.39        39\n",
      "           5       0.40      0.29      0.33        21\n",
      "           6       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.64      1128\n",
      "   macro avg       0.54      0.54      0.53      1128\n",
      "weighted avg       0.63      0.64      0.63      1128\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[336, 100,  21,   4,   3,   0],\n",
       "       [105, 307,  29,   7,   2,   3],\n",
       "       [ 28,  56,  55,   3,   3,   0],\n",
       "       [ 10,  11,   3,  13,   1,   1],\n",
       "       [  5,   5,   5,   0,   6,   0],\n",
       "       [  0,   0,   1,   0,   0,   5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(classification_report(y_test,y_pred))\n",
    "cm = make_confusion_matrices(y_test, y_pred, 1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test, density=True, bins=10)  # density=False would make counts\n",
    "plt.hist(y_pred, density=True, bins=10)  # density=False would make counts\n",
    "\n",
    "plt.ylabel('Wights')\n",
    "plt.xlabel('Data');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
