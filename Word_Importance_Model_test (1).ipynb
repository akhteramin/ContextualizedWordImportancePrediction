{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "Word-Importance paper(A Corpus for Modeling Word Importance in Spoken Dialogue Transcripts): https://arxiv.org/pdf/1801.09746.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "[sudo] password for aa7510: \n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0392d69310db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load pre-trained model's tokenizer (BERT vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import heapq\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained model's tokenizer (BERT vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Definition\n",
    "folder = {\n",
    "    '20':['2005'],\n",
    "    '21':['2191'],\n",
    "    '22':['2222'],\n",
    "    '23':['2348'],\n",
    "    '24':['2450'],\n",
    "    '25':['2565'],\n",
    "    '26':['2636'],\n",
    "    '27':['2710'],\n",
    "    '28':['2886'],\n",
    "    '30':['3044','3083'],\n",
    "    '32':['3203'],\n",
    "    '33':['3301','3324'],\n",
    "    '36':['3601'],\n",
    "    '38':['3817'],\n",
    "    '40':['4010','4021'],\n",
    "    '43':['4320'],\n",
    "    '44':['4400'],\n",
    "    '45':['4531'],\n",
    "    '47':['4721']\n",
    "}\n",
    "\n",
    "file_type = ['A','B']\n",
    "size_of_bow = 100\n",
    "threshold = 0.4\n",
    "\n",
    "POS_IMPORTANCE = {'NN':3.95,'NNP':3.95,'NNS':3.95,\n",
    "                  'VB':3.82, 'VBZ':3.82,'VBP':3.82,\n",
    "                  'VBG':3.82,'VBD':3.82,'VBN':3.82,\n",
    "                  'JJ':3.80,'RB':3.43 }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model\n",
    "Measuring word importance using bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessor\n",
    " # Prcessing the text: to extract the text and corresponding sense from each line of the file\n",
    "def process_text(line):\n",
    "    splitLine = line.split(\" \")\n",
    "    return splitLine[-1].replace(\"\\n\",\"\")\n",
    "\n",
    "def process_wimp(line):\n",
    "    splitLine = line.split(\" \")\n",
    "    last_word = splitLine[-1].replace(\"\\n\",\"\")\n",
    "    splitLine[-1] =  last_word\n",
    "    return splitLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], '')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "def lemmatizing(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "def normalize_text(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = lemmatizing(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words from the switchboard word list\n",
    "def get_words_list(file_name):\n",
    "    files = open(file_name)\n",
    "    text = \"\"\n",
    "    for sentence in files.readlines():\n",
    "        sentence = sentence.split(\" \")\n",
    "        actual_sentence = \" \".join(sentence[3:])\n",
    "        text = text + actual_sentence\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(file):\n",
    "    corpus = get_words_list(file)\n",
    "    wordfreq = {}\n",
    "    tokens = nltk.word_tokenize(str(corpus))\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bag_of_words(file):\n",
    "    wordfreq = get_word_freq(file)\n",
    "    most_freq = heapq.nlargest(size_of_bow, wordfreq, key=wordfreq.get)\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(file_name):\n",
    "    files = open(file_name)\n",
    "    sentence_set=[]\n",
    "    for sentence in files.readlines():\n",
    "        sentence = sentence.split(\" \")\n",
    "        actual_sentence = \" \".join(sentence[3:])\n",
    "        sentence_set.append(actual_sentence.replace(\"\\n\",\"\"))\n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_importance(file):\n",
    "    corpus = get_sentence_list(file)\n",
    "    sentence_vectors = []\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    for sentence in corpus:\n",
    "        sentence = normalize_text(sentence)\n",
    "#         print(sentence)\n",
    "        sentence_tokens = nltk.word_tokenize(str(sentence))\n",
    "        sent_vec = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in most_freq:\n",
    "                sent_vec.append(1)\n",
    "            else:\n",
    "                sent_vec.append(0)\n",
    "\n",
    "        sentence_vectors.append(sent_vec)\n",
    "#     print(sentence_vectors)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measure\n",
    "Right now the length of this word importance and length of the kafles' word importanfeis not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wimp_scores(file_name ):\n",
    "    files = open(file_name)\n",
    "    scores = []\n",
    "    for sentence in files.readlines():\n",
    "        score_array = process_wimp(sentence) \n",
    "        scores.append(score_array[3:])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17378640776699028\n",
      "Accuracy: 0.21851851851851853\n",
      "Accuracy: 0.22713864306784662\n",
      "Accuracy: 0.18508092892329345\n",
      "Accuracy: 0.14345991561181434\n",
      "Accuracy: 0.1634182908545727\n",
      "Accuracy: 0.2249488752556237\n",
      "Accuracy: 0.251434034416826\n",
      "Accuracy: 0.23314917127071824\n",
      "Accuracy: 0.1902834008097166\n",
      "Accuracy: 0.22535211267605634\n",
      "Accuracy: 0.2568493150684932\n",
      "Accuracy: 0.18274111675126903\n",
      "Accuracy: 0.2767857142857143\n",
      "Accuracy: 0.25901639344262295\n",
      "Accuracy: 0.2837465564738292\n",
      "Accuracy: 0.21161825726141079\n",
      "Accuracy: 0.2403846153846154\n",
      "Accuracy: 0.22288261515601784\n",
      "Accuracy: 0.2135593220338983\n",
      "Accuracy: 0.22968197879858657\n",
      "Accuracy: 0.20673076923076922\n",
      "Accuracy: 0.26019690576652604\n",
      "Accuracy: 0.24472573839662448\n",
      "Accuracy: 0.2261640798226164\n",
      "Accuracy: 0.16730038022813687\n",
      "Accuracy: 0.16265060240963855\n",
      "Accuracy: 0.18686868686868688\n",
      "Accuracy: 0.2342814371257485\n",
      "Accuracy: 0.2350061199510404\n",
      "Accuracy: 0.28113879003558717\n",
      "Accuracy: 0.24662162162162163\n",
      "Accuracy: 0.2127659574468085\n",
      "Accuracy: 0.224609375\n",
      "Accuracy: 0.22911051212938005\n",
      "Accuracy: 0.21550387596899226\n",
      "Accuracy: 0.23564954682779457\n",
      "Accuracy: 0.25072886297376096\n",
      "Accuracy: 0.2266244057052298\n",
      "Accuracy: 0.24385964912280703\n",
      "Accuracy: 0.24376731301939059\n",
      "Accuracy: 0.21040189125295508\n",
      "Accuracy: 0.23885918003565063\n",
      "Accuracy: 0.16864608076009502\n",
      "1391.64\n"
     ]
    }
   ],
   "source": [
    "BOW_MSE = 0\n",
    "BOW_Accuracy = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            bow = get_word_importance(file)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(bow[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    for j in range(0,len(scores[i])):\n",
    "\n",
    "                        if (float(bow[i][j])>threshold and float(scores[i][j])>threshold) or (float(bow[i][j])<=threshold and float(scores[i][j])<=threshold):\n",
    "                            correct = correct +1\n",
    "                            BOW_MSE = BOW_MSE + pow(float(bow[i][j])-float(threshold),2)\n",
    "\n",
    "\n",
    "            accuracy = correct/total\n",
    "            BOW_Accuracy.append(accuracy)\n",
    "            print(\"Accuracy:\",accuracy)\n",
    "print(BOW_MSE)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Implementation\n",
    "The idea behind the TF-IDF approach is that the words that are more common in one sentence and less common in other sentences should be given high weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_idf(file):\n",
    "    word_idf_values = {}\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    corpus = get_sentence_list(file)\n",
    "    for token in most_freq:\n",
    "        doc_containing_word = 0\n",
    "        for document in corpus:\n",
    "            if token in nltk.word_tokenize(document):\n",
    "                doc_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "#     print(word_idf_values)\n",
    "    return word_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_tf_score(file):\n",
    "    word_tf_values = {}\n",
    "    corpus = get_sentence_list(file)\n",
    "    most_freq = make_bag_of_words(file)\n",
    "    for token in most_freq:\n",
    "        sent_tf_vector = []\n",
    "        for document in corpus:\n",
    "            doc_freq = 0\n",
    "            for word in nltk.word_tokenize(document):\n",
    "                if token == word:\n",
    "                      doc_freq += 1\n",
    "\n",
    "            word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "    return word_tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_tf_idf_score(file):\n",
    "    word_tf_values = measure_tf_score(file)\n",
    "    word_idf_values =  measure_idf(file)\n",
    "    tfidf_values = []\n",
    "    for token in word_tf_values.keys():\n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "    return tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_word_importance(file):\n",
    "    corpus = get_sentence_list(file)\n",
    "    sentence_vectors = []\n",
    "    word_tf_values = measure_tf_score(file)\n",
    "    word_idf_values = measure_idf(file)\n",
    "    count = 0 \n",
    "    for sentence in corpus:\n",
    "        sentence = normalize_text(sentence)\n",
    "        sentence_tokens = nltk.word_tokenize(str(sentence))\n",
    "        sent_vec = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in word_tf_values:\n",
    "                sent_vec.append(word_tf_values[token][count]*word_idf_values[token])\n",
    "            else:\n",
    "                sent_vec.append(0)\n",
    "        \n",
    "        sentence_vectors.append(sent_vec)\n",
    "        count=count+1\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measure\n",
    "Right now the length of this word importance and length of the kafles' word importanfeis not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(X, Y):\n",
    "    if (0 <= X <= 0.2 and 0 <= Y <= 0.2) or (0.2 < X <= 0.4 and 0.2 < Y <= 0.4) or (0.4 < X <= 0.6 and 0.4 < Y <= 0.6) or (0.6 < X <= 0.8 and 0.6 < Y <= 0.8)or (0.8 < X <= 1 and 0.8 < Y <= 1):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wimp_scores(file_name = wimp_file):\n",
    "    files = open(file_name)\n",
    "    scores = []\n",
    "    for sentence in files.readlines():\n",
    "        score_array = process_wimp(sentence) \n",
    "        scores.append(score_array[3:])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5631067961165048\n",
      "Accuracy: 0.44814814814814813\n",
      "Accuracy: 0.4026548672566372\n",
      "Accuracy: 0.41731175228712175\n",
      "Accuracy: 0.5147679324894515\n",
      "Accuracy: 0.5097451274362819\n",
      "Accuracy: 0.45194274028629855\n",
      "Accuracy: 0.5210325047801148\n",
      "Accuracy: 0.41104972375690607\n",
      "Accuracy: 0.5236167341430499\n",
      "Accuracy: 0.5736235595390525\n",
      "Accuracy: 0.4041095890410959\n",
      "Accuracy: 0.5042301184433164\n",
      "Accuracy: 0.53125\n",
      "Accuracy: 0.32131147540983607\n",
      "Accuracy: 0.3443526170798898\n",
      "Accuracy: 0.4190871369294606\n",
      "Accuracy: 0.38653846153846155\n",
      "Accuracy: 0.5230312035661219\n",
      "Accuracy: 0.5067796610169492\n",
      "Accuracy: 0.5070671378091873\n",
      "Accuracy: 0.5096153846153846\n",
      "Accuracy: 0.4092827004219409\n",
      "Accuracy: 0.44936708860759494\n",
      "Accuracy: 0.3348115299334812\n",
      "Accuracy: 0.3288973384030418\n",
      "Accuracy: 0.4457831325301205\n",
      "Accuracy: 0.29545454545454547\n",
      "Accuracy: 0.5516467065868264\n",
      "Accuracy: 0.40269277845777235\n",
      "Accuracy: 0.21352313167259787\n",
      "Accuracy: 0.3310810810810811\n",
      "Accuracy: 0.549645390070922\n",
      "Accuracy: 0.41796875\n",
      "Accuracy: 0.31805929919137466\n",
      "Accuracy: 0.5782945736434109\n",
      "Accuracy: 0.4471299093655589\n",
      "Accuracy: 0.4358600583090379\n",
      "Accuracy: 0.4041204437400951\n",
      "Accuracy: 0.3385964912280702\n",
      "Accuracy: 0.42382271468144045\n",
      "Accuracy: 0.4302600472813239\n",
      "Accuracy: 0.5347593582887701\n",
      "Accuracy: 0.41330166270783847\n",
      "SE:122.18405705849297\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_MSE = 0\n",
    "TF_IDF_Accuracy = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            sent_vector = get_tf_idf_word_importance(file)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(sent_vector[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        if compare(float(sent_vector[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "                            TF_IDF_MSE = TF_IDF_MSE + pow((float(sent_vector[i][j])-float(scores[i][j])),2)\n",
    "            accuracy = correct/total\n",
    "            TF_IDF_Accuracy.append(accuracy)\n",
    "            print(\"Accuracy: \"+ str(accuracy))\n",
    "print(\"SE:\" +str(TF_IDF_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 19:17:23.400736 139821617399616 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I1203 19:17:23.401627 139821617399616 dictionary.py:214] built Dictionary(143 unique tokens: ['actual', 'allevi', 'alzheim', 'apart', 'avel']...) from 1 documents (total 310 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 4), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 2), (7, 1), (8, 1), (9, 1), (10, 5), (11, 1), (12, 3), (13, 2), (14, 6), (15, 2), (16, 3), (17, 1), (18, 7), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 3), (25, 5), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 5), (40, 1), (41, 2), (42, 1), (43, 10), (44, 2), (45, 3), (46, 2), (47, 2), (48, 1), (49, 1), (50, 1), (51, 2), (52, 3), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 5), (67, 1), (68, 1), (69, 1), (70, 2), (71, 2), (72, 2), (73, 2), (74, 26), (75, 1), (76, 6), (77, 1), (78, 3), (79, 1), (80, 1), (81, 1), (82, 3), (83, 1), (84, 14), (85, 1), (86, 1), (87, 1), (88, 1), (89, 3), (90, 2), (91, 1), (92, 2), (93, 6), (94, 3), (95, 3), (96, 1), (97, 1), (98, 1), (99, 2), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 8), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 1), (122, 1), (123, 1), (124, 2), (125, 3), (126, 2), (127, 1), (128, 5), (129, 15), (130, 1), (131, 1), (132, 1), (133, 1), (134, 3), (135, 3), (136, 1), (137, 4), (138, 1), (139, 2), (140, 1), (141, 1), (142, 1)]]\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for key in folder.keys():\n",
    "    for value in folder[key]:\n",
    "        if count ==1:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+\"A-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+\"A-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "            processed_docs = []\n",
    "            for sent in corpus:\n",
    "                for word in preprocess(sent):\n",
    "                    if word not in ['yeah','silenc', 'laughter']:\n",
    "                        processed_docs.append(word)\n",
    "                \n",
    "            \n",
    "        count=count+1\n",
    "processed_docs = [processed_docs]\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 19:17:23.579481 139821617399616 ldamodel.py:557] using symmetric alpha at 1.0\n",
      "I1203 19:17:23.580020 139821617399616 ldamodel.py:557] using symmetric eta at 1.0\n",
      "I1203 19:17:23.580411 139821617399616 ldamodel.py:481] using serial LDA version on this node\n",
      "I1203 19:17:23.581177 139821617399616 ldamulticore.py:238] running online LDA training, 1 topics, 10 passes over the supplied corpus of 1 documents, updating every 4000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "I1203 19:17:23.582309 139821617399616 ldamulticore.py:279] training LDA model using 2 processes\n",
      "I1203 19:17:23.596432 139821617399616 ldamulticore.py:294] PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.601920 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"nois\" + 0.015*\"children\" + 0.015*\"like\" + 0.013*\"thing\"\n",
      "I1203 19:17:23.602401 139821617399616 ldamodel.py:1049] topic diff=0.452079, rho=1.000000\n",
      "I1203 19:17:23.604074 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.604583 139821617399616 ldamulticore.py:294] PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.607973 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"nois\" + 0.015*\"children\" + 0.015*\"like\" + 0.013*\"thing\"\n",
      "I1203 19:17:23.608448 139821617399616 ldamodel.py:1049] topic diff=0.000007, rho=0.707018\n",
      "I1203 19:17:23.610356 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.610828 139821617399616 ldamulticore.py:294] PROGRESS: pass 2, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.612843 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"nois\" + 0.015*\"like\" + 0.015*\"children\" + 0.013*\"thing\"\n",
      "I1203 19:17:23.613255 139821617399616 ldamodel.py:1049] topic diff=0.000002, rho=0.577302\n",
      "I1203 19:17:23.614545 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.615006 139821617399616 ldamulticore.py:294] PROGRESS: pass 3, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.616959 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"nois\" + 0.015*\"like\" + 0.015*\"children\" + 0.013*\"thing\"\n",
      "I1203 19:17:23.617388 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.499969\n",
      "I1203 19:17:23.618724 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.619181 139821617399616 ldamulticore.py:294] PROGRESS: pass 4, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.621246 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"nois\" + 0.015*\"like\" + 0.015*\"children\" + 0.013*\"thing\"\n",
      "I1203 19:17:23.621668 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.447191\n",
      "I1203 19:17:23.623056 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.623512 139821617399616 ldamulticore.py:294] PROGRESS: pass 5, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.625248 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"like\" + 0.015*\"nois\" + 0.015*\"children\" + 0.013*\"home\"\n",
      "I1203 19:17:23.625642 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.408231\n",
      "I1203 19:17:23.626973 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.627422 139821617399616 ldamulticore.py:294] PROGRESS: pass 6, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.629195 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"like\" + 0.015*\"nois\" + 0.015*\"children\" + 0.013*\"home\"\n",
      "I1203 19:17:23.629586 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.377951\n",
      "I1203 19:17:23.630977 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.631423 139821617399616 ldamulticore.py:294] PROGRESS: pass 7, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.633200 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"like\" + 0.015*\"nois\" + 0.015*\"children\" + 0.013*\"extend\"\n",
      "I1203 19:17:23.633612 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.353542\n",
      "I1203 19:17:23.634987 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.635414 139821617399616 ldamulticore.py:294] PROGRESS: pass 8, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.637182 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"like\" + 0.015*\"nois\" + 0.015*\"children\" + 0.013*\"decis\"\n",
      "I1203 19:17:23.637616 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.333324\n",
      "I1203 19:17:23.638964 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n",
      "I1203 19:17:23.639405 139821617399616 ldamulticore.py:294] PROGRESS: pass 9, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
      "I1203 19:17:23.641160 139821617399616 ldamodel.py:1171] topic #0 (1.000): 0.060*\"know\" + 0.035*\"think\" + 0.033*\"mean\" + 0.024*\"famili\" + 0.020*\"probabl\" + 0.018*\"come\" + 0.015*\"like\" + 0.015*\"nois\" + 0.015*\"children\" + 0.013*\"decis\"\n",
      "I1203 19:17:23.641578 139821617399616 ldamodel.py:1049] topic diff=0.000000, rho=0.316220\n",
      "I1203 19:17:23.642895 139821617399616 ldamodel.py:822] -4.871 per-word bound, 29.3 perplexity estimate based on a held-out corpus of 1 documents with 310 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know :  0.059603263\n",
      "think :  0.035320405\n",
      "mean :  0.033112872\n",
      "famili :  0.024282744\n",
      "probabl :  0.01986768\n",
      "come :  0.017660148\n",
      "children :  0.015452617\n",
      "nois :  0.015452617\n",
      "like :  0.015452617\n",
      "decis :  0.013245086\n",
      "care :  0.013245086\n",
      "extend :  0.013245086\n",
      "home :  0.013245086\n",
      "thing :  0.013245086\n",
      "visit :  0.011037557\n",
      "actual :  0.011037556\n",
      "live :  0.008830029\n",
      "money :  0.008830029\n",
      "nurs :  0.008830029\n",
      "okay :  0.008830029\n",
      "deal :  0.008830029\n",
      "close :  0.008830029\n",
      "chang :  0.008830029\n",
      "good :  0.008830029\n",
      "state :  0.008830029\n",
      "truli :  0.008830029\n",
      "unit :  0.008830029\n",
      "feel :  0.008830029\n",
      "environ :  0.008830029\n",
      "make :  0.008830029\n",
      "final :  0.0066225035\n",
      "fact :  0.006622503\n",
      "child :  0.006622503\n",
      "involv :  0.006622503\n",
      "wasn :  0.006622503\n",
      "find :  0.006622503\n",
      "strengthen :  0.006622503\n",
      "gonna :  0.006622503\n",
      "standard :  0.006622503\n",
      "month :  0.006622503\n",
      "grandmoth :  0.006622503\n",
      "guess :  0.006622503\n",
      "problem :  0.006622503\n",
      "interest :  0.006622503\n",
      "issu :  0.006622503\n",
      "peopl :  0.006622503\n",
      "kind :  0.006622503\n",
      "need :  0.006622503\n",
      "situat :  0.006622503\n",
      "awar :  0.006622503\n",
      "0.5805759686045349\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 1, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "\n",
    "topics_words=lda_model.show_topics(num_topics=1, num_words=50,formatted=False)\n",
    "# print(topics_words[0][1])\n",
    "\n",
    "total_weights=0\n",
    "#Below Code Prints Only Words \n",
    "for words, weights in topics_words[0][1]:\n",
    "    print(words,\": \",weights)\n",
    "    total_weights = total_weights + weights\n",
    "print(total_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating POS with Word Embedding in generating Word Importance \n",
    "We have used 1st step of Neural Bag-of-word2\n",
    "\n",
    "Importance of POS tag in general setting have been adapted from here:\n",
    "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.599.3242"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Word-embedding using Word2Vec \n",
    "Predicting word importance using word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(all_words):\n",
    "    word2vec = Word2Vec(all_words, min_count=2)\n",
    "    vocabulary = word2vec.wv.vocab\n",
    "    WV=np.zeros(shape=(len(vocabulary),100))\n",
    "    count=0\n",
    "    for voc in vocabulary.keys():\n",
    "        WV[count] = word2vec.wv[voc]\n",
    "        count = count + 1\n",
    "    return (word2vec,vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_imp(vocabulary, all_words, pos_tagged):\n",
    "    POS_IMP={}\n",
    "    count=0\n",
    "    for voc in vocabulary.keys():\n",
    "        if voc in all_words[0]:\n",
    "            for sent_pos in pos_tagged:\n",
    "                for word, pos in sent_pos:\n",
    "                    if word == voc:\n",
    "                        POS_IMP[voc] = POS_IMPORTANCE.get(pos,2.5)\n",
    "                        break\n",
    "        else:\n",
    "            POS_IMP[voc] = 2.5\n",
    "\n",
    "        count=count+1\n",
    "    return POS_IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_matrix(vocabulary, all_words, POS_IMP,word2vec):\n",
    "    BOW=np.zeros(shape=(len(all_words),1))\n",
    "    WV=np.zeros(shape=(len(all_words),100))\n",
    "    POSA = np.zeros(shape=(len(all_words),1))\n",
    "    count=0\n",
    "    for word in all_words:\n",
    "        if word in  vocabulary.keys():\n",
    "            BOW[count] = [1.0]\n",
    "            WV[count] = word2vec.wv[word]\n",
    "            POSA[count] = [POS_IMP[word]]\n",
    "        else:\n",
    "            BOW[count] = [0]\n",
    "        count=count+1\n",
    "\n",
    "    return (BOW, WV, POSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "#     print(\"data::\",data)\n",
    "    min_v = 0.01\n",
    "    max_v = 0\n",
    "    for sentence_score in data:\n",
    "        for words_score in sentence_score:\n",
    "                min_v = min(min_v, words_score)\n",
    "                max_v = max(max_v, words_score)\n",
    "    data = [(x-min_v)/(max_v-min_v) if len(x)>1 else [0] for x in data]\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:01.260207 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:01.260799 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:01.261390 139821617399616 word2vec.py:1405] collected 285 word types from a corpus of 1092 raw words and 73 sentences\n",
      "I1203 18:55:01.261749 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:01.262354 139821617399616 word2vec.py:1480] effective_min_count=2 retains 135 unique words (47% of original 285, drops 150)\n",
      "I1203 18:55:01.262729 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 942 word corpus (86% of original 1092, drops 150)\n",
      "I1203 18:55:01.263596 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 285 items\n",
      "I1203 18:55:01.263971 139821617399616 word2vec.py:1550] sample=0.001 downsamples 82 most-common words\n",
      "I1203 18:55:01.264311 139821617399616 word2vec.py:1551] downsampling leaves estimated 414 word corpus (44.0% of prior 942)\n",
      "I1203 18:55:01.264907 139821617399616 base_any2vec.py:1006] estimated required memory for 135 words and 100 dimensions: 175500 bytes\n",
      "I1203 18:55:01.265270 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:01.300951 139821617399616 base_any2vec.py:1192] training model with 3 workers on 135 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:01.302749 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.303148 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.303602 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.303962 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 1092 raw words (408 effective words) took 0.0s, 251870 effective words/s\n",
      "I1203 18:55:01.305760 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.306162 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.306611 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.307015 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 1092 raw words (417 effective words) took 0.0s, 252550 effective words/s\n",
      "I1203 18:55:01.308840 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.309255 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.309718 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.310049 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 1092 raw words (417 effective words) took 0.0s, 260192 effective words/s\n",
      "I1203 18:55:01.311796 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.312187 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.312663 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.313010 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 1092 raw words (407 effective words) took 0.0s, 253342 effective words/s\n",
      "I1203 18:55:01.314780 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.315181 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.315646 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.316001 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 1092 raw words (444 effective words) took 0.0s, 276949 effective words/s\n",
      "I1203 18:55:01.316346 139821617399616 base_any2vec.py:1366] training on a 5460 raw words (2093 effective words) took 0.0s, 139722 effective words/s\n",
      "W1203 18:55:01.316687 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:01.546573 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:01.547088 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:01.547570 139821617399616 word2vec.py:1405] collected 188 word types from a corpus of 540 raw words and 75 sentences\n",
      "I1203 18:55:01.547932 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:01.548432 139821617399616 word2vec.py:1480] effective_min_count=2 retains 83 unique words (44% of original 188, drops 105)\n",
      "I1203 18:55:01.548789 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 435 word corpus (80% of original 540, drops 105)\n",
      "I1203 18:55:01.549519 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 188 items\n",
      "I1203 18:55:01.549889 139821617399616 word2vec.py:1550] sample=0.001 downsamples 83 most-common words\n",
      "I1203 18:55:01.550227 139821617399616 word2vec.py:1551] downsampling leaves estimated 150 word corpus (34.6% of prior 435)\n",
      "I1203 18:55:01.550726 139821617399616 base_any2vec.py:1006] estimated required memory for 83 words and 100 dimensions: 107900 bytes\n",
      "I1203 18:55:01.551081 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:01.572317 139821617399616 base_any2vec.py:1192] training model with 3 workers on 83 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:01.573975 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.027933976283111745\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "0.03793397628311175\n",
      "Accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:01.574586 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.574975 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.575310 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 540 raw words (149 effective words) took 0.0s, 91566 effective words/s\n",
      "I1203 18:55:01.577065 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.577568 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.577955 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.578293 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 540 raw words (154 effective words) took 0.0s, 101695 effective words/s\n",
      "I1203 18:55:01.579942 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.580409 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.580773 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.581104 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 540 raw words (150 effective words) took 0.0s, 103947 effective words/s\n",
      "I1203 18:55:01.582761 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.583263 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.583625 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.583995 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 540 raw words (142 effective words) took 0.0s, 94322 effective words/s\n",
      "I1203 18:55:01.585669 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.586175 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.586550 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.586887 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 540 raw words (147 effective words) took 0.0s, 98512 effective words/s\n",
      "I1203 18:55:01.587233 139821617399616 base_any2vec.py:1366] training on a 2700 raw words (742 effective words) took 0.0s, 51304 effective words/s\n",
      "W1203 18:55:01.587566 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:01.807262 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:01.807769 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:01.808293 139821617399616 word2vec.py:1405] collected 244 word types from a corpus of 678 raw words and 81 sentences\n",
      "I1203 18:55:01.808638 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:01.809206 139821617399616 word2vec.py:1480] effective_min_count=2 retains 102 unique words (41% of original 244, drops 142)\n",
      "I1203 18:55:01.809557 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 536 word corpus (79% of original 678, drops 142)\n",
      "I1203 18:55:01.810373 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 244 items\n",
      "I1203 18:55:01.810737 139821617399616 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1203 18:55:01.811095 139821617399616 word2vec.py:1551] downsampling leaves estimated 210 word corpus (39.2% of prior 536)\n",
      "I1203 18:55:01.811608 139821617399616 base_any2vec.py:1006] estimated required memory for 102 words and 100 dimensions: 132600 bytes\n",
      "I1203 18:55:01.811976 139821617399616 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03793397628311175\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.043101597921485435\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07409004941899687\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "0.07488043677761608\n",
      "Accuracy: 0.19047619047619047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:01.838133 139821617399616 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:01.840861 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.841333 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.841690 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.842034 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 678 raw words (207 effective words) took 0.0s, 139123 effective words/s\n",
      "I1203 18:55:01.843703 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.844156 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.844513 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.844863 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 678 raw words (215 effective words) took 0.0s, 148401 effective words/s\n",
      "I1203 18:55:01.846593 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.847102 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.847469 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.847823 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 678 raw words (202 effective words) took 0.0s, 131772 effective words/s\n",
      "I1203 18:55:01.849520 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.850052 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.850410 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.850771 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 678 raw words (193 effective words) took 0.0s, 125475 effective words/s\n",
      "I1203 18:55:01.852453 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:01.852939 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:01.853299 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:01.853630 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 678 raw words (209 effective words) took 0.0s, 143154 effective words/s\n",
      "I1203 18:55:01.853994 139821617399616 base_any2vec.py:1366] training on a 3390 raw words (1026 effective words) took 0.0s, 67595 effective words/s\n",
      "W1203 18:55:01.854329 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07488043677761608\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1142411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1242411301844174\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "Accuracy: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:02.204183 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:02.204637 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:02.205292 139821617399616 word2vec.py:1405] collected 356 word types from a corpus of 1421 raw words and 128 sentences\n",
      "I1203 18:55:02.205663 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:02.206342 139821617399616 word2vec.py:1480] effective_min_count=2 retains 169 unique words (47% of original 356, drops 187)\n",
      "I1203 18:55:02.206689 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 1234 word corpus (86% of original 1421, drops 187)\n",
      "I1203 18:55:02.207756 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 356 items\n",
      "I1203 18:55:02.208119 139821617399616 word2vec.py:1550] sample=0.001 downsamples 86 most-common words\n",
      "I1203 18:55:02.208445 139821617399616 word2vec.py:1551] downsampling leaves estimated 606 word corpus (49.1% of prior 1234)\n",
      "I1203 18:55:02.209078 139821617399616 base_any2vec.py:1006] estimated required memory for 169 words and 100 dimensions: 219700 bytes\n",
      "I1203 18:55:02.209425 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:02.252500 139821617399616 base_any2vec.py:1192] training model with 3 workers on 169 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:02.255227 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.255615 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.256100 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.256437 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 1421 raw words (593 effective words) took 0.0s, 355641 effective words/s\n",
      "I1203 18:55:02.258345 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.258763 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.259196 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.259541 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 1421 raw words (603 effective words) took 0.0s, 365697 effective words/s\n",
      "I1203 18:55:02.261451 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.261880 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.262340 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.262695 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 1421 raw words (628 effective words) took 0.0s, 369648 effective words/s\n",
      "I1203 18:55:02.264561 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.264975 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.265445 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.265773 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 1421 raw words (622 effective words) took 0.0s, 375932 effective words/s\n",
      "I1203 18:55:02.267625 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.268032 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.268486 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.268854 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 1421 raw words (614 effective words) took 0.0s, 369007 effective words/s\n",
      "I1203 18:55:02.269205 139821617399616 base_any2vec.py:1366] training on a 7105 raw words (3060 effective words) took 0.0s, 188151 effective words/s\n",
      "W1203 18:55:02.269538 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:02.408944 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:02.409394 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:02.409830 139821617399616 word2vec.py:1405] collected 104 word types from a corpus of 237 raw words and 21 sentences\n",
      "I1203 18:55:02.410182 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:02.410599 139821617399616 word2vec.py:1480] effective_min_count=2 retains 43 unique words (41% of original 104, drops 61)\n",
      "I1203 18:55:02.410956 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 176 word corpus (74% of original 237, drops 61)\n",
      "I1203 18:55:02.411502 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 104 items\n",
      "I1203 18:55:02.411864 139821617399616 word2vec.py:1550] sample=0.001 downsamples 43 most-common words\n",
      "I1203 18:55:02.412206 139821617399616 word2vec.py:1551] downsampling leaves estimated 41 word corpus (23.8% of prior 176)\n",
      "I1203 18:55:02.412629 139821617399616 base_any2vec.py:1006] estimated required memory for 43 words and 100 dimensions: 55900 bytes\n",
      "I1203 18:55:02.412985 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:02.424417 139821617399616 base_any2vec.py:1192] training model with 3 workers on 43 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:02.426738 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.427181 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.427535 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.427885 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 237 raw words (39 effective words) took 0.0s, 28466 effective words/s\n",
      "I1203 18:55:02.429409 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.429867 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.430217 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.430546 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 237 raw words (47 effective words) took 0.0s, 34740 effective words/s\n",
      "I1203 18:55:02.432055 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.432488 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.432853 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.433185 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 237 raw words (46 effective words) took 0.0s, 34609 effective words/s\n",
      "I1203 18:55:02.434660 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.435106 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.435452 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.435800 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 237 raw words (61 effective words) took 0.0s, 45749 effective words/s\n",
      "I1203 18:55:02.437276 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.437737 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.438084 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.438410 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 237 raw words (43 effective words) took 0.0s, 32403 effective words/s\n",
      "I1203 18:55:02.438767 139821617399616 base_any2vec.py:1366] training on a 1185 raw words (236 effective words) took 0.0s, 16945 effective words/s\n",
      "W1203 18:55:02.439105 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "Accuracy: 0.5\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1282906706944787\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "Accuracy: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:02.615791 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:02.616223 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:02.616735 139821617399616 word2vec.py:1405] collected 238 word types from a corpus of 706 raw words and 64 sentences\n",
      "I1203 18:55:02.617089 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:02.617615 139821617399616 word2vec.py:1480] effective_min_count=2 retains 100 unique words (42% of original 238, drops 138)\n",
      "I1203 18:55:02.617975 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 568 word corpus (80% of original 706, drops 138)\n",
      "I1203 18:55:02.618779 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 238 items\n",
      "I1203 18:55:02.619132 139821617399616 word2vec.py:1550] sample=0.001 downsamples 100 most-common words\n",
      "I1203 18:55:02.619466 139821617399616 word2vec.py:1551] downsampling leaves estimated 221 word corpus (38.9% of prior 568)\n",
      "I1203 18:55:02.619992 139821617399616 base_any2vec.py:1006] estimated required memory for 100 words and 100 dimensions: 130000 bytes\n",
      "I1203 18:55:02.620334 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:02.645941 139821617399616 base_any2vec.py:1192] training model with 3 workers on 100 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:02.648425 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.648928 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.649281 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.649613 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 706 raw words (222 effective words) took 0.0s, 149062 effective words/s\n",
      "I1203 18:55:02.651293 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.651791 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.652129 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.652458 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 706 raw words (228 effective words) took 0.0s, 155617 effective words/s\n",
      "I1203 18:55:02.654120 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.654572 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.654940 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.655268 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 706 raw words (229 effective words) took 0.0s, 157630 effective words/s\n",
      "I1203 18:55:02.656911 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.657523 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.657892 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.658227 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 706 raw words (238 effective words) took 0.0s, 145047 effective words/s\n",
      "I1203 18:55:02.659829 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.660324 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.660692 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.661036 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 706 raw words (237 effective words) took 0.0s, 155038 effective words/s\n",
      "I1203 18:55:02.661381 139821617399616 base_any2vec.py:1366] training on a 3530 raw words (1154 effective words) took 0.0s, 82814 effective words/s\n",
      "W1203 18:55:02.661740 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:02.897981 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:02.898488 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:02.898977 139821617399616 word2vec.py:1405] collected 175 word types from a corpus of 489 raw words and 94 sentences\n",
      "I1203 18:55:02.899325 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:02.899825 139821617399616 word2vec.py:1480] effective_min_count=2 retains 74 unique words (42% of original 175, drops 101)\n",
      "I1203 18:55:02.900169 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 388 word corpus (79% of original 489, drops 101)\n",
      "I1203 18:55:02.900866 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 175 items\n",
      "I1203 18:55:02.901210 139821617399616 word2vec.py:1550] sample=0.001 downsamples 74 most-common words\n",
      "I1203 18:55:02.901539 139821617399616 word2vec.py:1551] downsampling leaves estimated 123 word corpus (32.0% of prior 388)\n",
      "I1203 18:55:02.902031 139821617399616 base_any2vec.py:1006] estimated required memory for 74 words and 100 dimensions: 96200 bytes\n",
      "I1203 18:55:02.902373 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:02.921458 139821617399616 base_any2vec.py:1192] training model with 3 workers on 74 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:02.923973 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.924479 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.924863 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.925198 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 489 raw words (117 effective words) took 0.0s, 77771 effective words/s\n",
      "I1203 18:55:02.926865 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.927333 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.927695 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.928044 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 489 raw words (126 effective words) took 0.0s, 86476 effective words/s\n",
      "I1203 18:55:02.929688 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.930175 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.930555 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.930912 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 489 raw words (136 effective words) took 0.0s, 91410 effective words/s\n",
      "I1203 18:55:02.932604 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.933094 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.933460 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.933825 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 489 raw words (110 effective words) took 0.0s, 74985 effective words/s\n",
      "I1203 18:55:02.935509 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:02.936021 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:02.936367 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:02.936713 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 489 raw words (134 effective words) took 0.0s, 92343 effective words/s\n",
      "I1203 18:55:02.937069 139821617399616 base_any2vec.py:1366] training on a 2445 raw words (623 effective words) took 0.0s, 41073 effective words/s\n",
      "W1203 18:55:02.937407 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.1643555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2043555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "Accuracy: 0.7692307692307693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:03.259313 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:03.259781 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:03.260343 139821617399616 word2vec.py:1405] collected 278 word types from a corpus of 1046 raw words and 127 sentences\n",
      "I1203 18:55:03.260698 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:03.261311 139821617399616 word2vec.py:1480] effective_min_count=2 retains 141 unique words (50% of original 278, drops 137)\n",
      "I1203 18:55:03.261653 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 909 word corpus (86% of original 1046, drops 137)\n",
      "I1203 18:55:03.262578 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 278 items\n",
      "I1203 18:55:03.262952 139821617399616 word2vec.py:1550] sample=0.001 downsamples 99 most-common words\n",
      "I1203 18:55:03.263290 139821617399616 word2vec.py:1551] downsampling leaves estimated 422 word corpus (46.4% of prior 909)\n",
      "I1203 18:55:03.263880 139821617399616 base_any2vec.py:1006] estimated required memory for 141 words and 100 dimensions: 183300 bytes\n",
      "I1203 18:55:03.264229 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:03.301174 139821617399616 base_any2vec.py:1192] training model with 3 workers on 141 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:03.302895 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.303283 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.303980 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.304328 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 1046 raw words (411 effective words) took 0.0s, 224779 effective words/s\n",
      "I1203 18:55:03.305993 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.306396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.306841 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.307191 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 1046 raw words (419 effective words) took 0.0s, 264169 effective words/s\n",
      "I1203 18:55:03.308952 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.309340 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.309807 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.310138 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 1046 raw words (437 effective words) took 0.0s, 279568 effective words/s\n",
      "I1203 18:55:03.311910 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.312312 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.312750 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.313094 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 1046 raw words (418 effective words) took 0.0s, 268598 effective words/s\n",
      "I1203 18:55:03.314902 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.315335 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.315784 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.316132 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 1046 raw words (411 effective words) took 0.0s, 253618 effective words/s\n",
      "I1203 18:55:03.316481 139821617399616 base_any2vec.py:1366] training on a 5230 raw words (2096 effective words) took 0.0s, 140646 effective words/s\n",
      "W1203 18:55:03.316840 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2443555522761186\n",
      "0.2443555522761186\n",
      "0.2543555522761186\n",
      "0.2543555522761186\n",
      "0.2643555522761186\n",
      "0.2643555522761186\n",
      "0.30435555227611866\n",
      "0.30435555227611866\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3393877963350756\n",
      "0.3493877963350756\n",
      "0.3493877963350756\n",
      "0.3493877963350756\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.353464052716463\n",
      "0.393464052716463\n",
      "0.393464052716463\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "0.43346405271646304\n",
      "Accuracy: 0.5714285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:03.612735 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:03.613179 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:03.613721 139821617399616 word2vec.py:1405] collected 288 word types from a corpus of 905 raw words and 98 sentences\n",
      "I1203 18:55:03.614077 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:03.614659 139821617399616 word2vec.py:1480] effective_min_count=2 retains 140 unique words (48% of original 288, drops 148)\n",
      "I1203 18:55:03.615019 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 757 word corpus (83% of original 905, drops 148)\n",
      "I1203 18:55:03.616010 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 288 items\n",
      "I1203 18:55:03.616363 139821617399616 word2vec.py:1550] sample=0.001 downsamples 140 most-common words\n",
      "I1203 18:55:03.616702 139821617399616 word2vec.py:1551] downsampling leaves estimated 360 word corpus (47.6% of prior 757)\n",
      "I1203 18:55:03.617293 139821617399616 base_any2vec.py:1006] estimated required memory for 140 words and 100 dimensions: 182000 bytes\n",
      "I1203 18:55:03.617627 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:03.653487 139821617399616 base_any2vec.py:1192] training model with 3 workers on 140 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:03.656112 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.656510 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.656990 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.657323 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 905 raw words (339 effective words) took 0.0s, 217641 effective words/s\n",
      "I1203 18:55:03.659089 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.659491 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.659946 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.660285 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 905 raw words (350 effective words) took 0.0s, 225367 effective words/s\n",
      "I1203 18:55:03.662046 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.662559 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.662926 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.663264 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 905 raw words (353 effective words) took 0.0s, 223972 effective words/s\n",
      "I1203 18:55:03.664989 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.665388 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.665840 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.666183 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 905 raw words (354 effective words) took 0.0s, 229167 effective words/s\n",
      "I1203 18:55:03.667948 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.668438 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.668823 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.669164 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 905 raw words (348 effective words) took 0.0s, 221261 effective words/s\n",
      "I1203 18:55:03.669510 139821617399616 base_any2vec.py:1366] training on a 4525 raw words (1744 effective words) took 0.0s, 112471 effective words/s\n",
      "W1203 18:55:03.669862 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:03.887071 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:03.887576 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:03.888111 139821617399616 word2vec.py:1405] collected 229 word types from a corpus of 762 raw words and 65 sentences\n",
      "I1203 18:55:03.888461 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:03.889030 139821617399616 word2vec.py:1480] effective_min_count=2 retains 116 unique words (50% of original 229, drops 113)\n",
      "I1203 18:55:03.889380 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 649 word corpus (85% of original 762, drops 113)\n",
      "I1203 18:55:03.890281 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 229 items\n",
      "I1203 18:55:03.890635 139821617399616 word2vec.py:1550] sample=0.001 downsamples 116 most-common words\n",
      "I1203 18:55:03.890987 139821617399616 word2vec.py:1551] downsampling leaves estimated 273 word corpus (42.2% of prior 649)\n",
      "I1203 18:55:03.891520 139821617399616 base_any2vec.py:1006] estimated required memory for 116 words and 100 dimensions: 150800 bytes\n",
      "I1203 18:55:03.891884 139821617399616 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43346405271646304\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "Accuracy: 0.5555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:03.921768 139821617399616 base_any2vec.py:1192] training model with 3 workers on 116 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:03.923438 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.923932 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.924274 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.924602 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 762 raw words (261 effective words) took 0.0s, 174669 effective words/s\n",
      "I1203 18:55:03.926315 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.926729 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.927149 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.927484 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 762 raw words (270 effective words) took 0.0s, 180534 effective words/s\n",
      "I1203 18:55:03.929219 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.929739 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.930119 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.930458 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 762 raw words (267 effective words) took 0.0s, 170780 effective words/s\n",
      "I1203 18:55:03.932144 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.932599 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.932976 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.933304 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 762 raw words (271 effective words) took 0.0s, 184171 effective words/s\n",
      "I1203 18:55:03.934967 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:03.935413 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:03.935783 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:03.936121 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 762 raw words (277 effective words) took 0.0s, 186290 effective words/s\n",
      "I1203 18:55:03.936459 139821617399616 base_any2vec.py:1366] training on a 3810 raw words (1346 effective words) took 0.0s, 94693 effective words/s\n",
      "W1203 18:55:03.936812 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:04.129245 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:04.129715 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:04.130236 139821617399616 word2vec.py:1405] collected 199 word types from a corpus of 781 raw words and 58 sentences\n",
      "I1203 18:55:04.130589 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:04.131139 139821617399616 word2vec.py:1480] effective_min_count=2 retains 104 unique words (52% of original 199, drops 95)\n",
      "I1203 18:55:04.131479 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 686 word corpus (87% of original 781, drops 95)\n",
      "I1203 18:55:04.132308 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 199 items\n",
      "I1203 18:55:04.132662 139821617399616 word2vec.py:1550] sample=0.001 downsamples 104 most-common words\n",
      "I1203 18:55:04.133026 139821617399616 word2vec.py:1551] downsampling leaves estimated 267 word corpus (39.1% of prior 686)\n",
      "I1203 18:55:04.133544 139821617399616 base_any2vec.py:1006] estimated required memory for 104 words and 100 dimensions: 135200 bytes\n",
      "I1203 18:55:04.133913 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:04.160768 139821617399616 base_any2vec.py:1192] training model with 3 workers on 104 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:04.163227 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.163707 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.164076 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.164410 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 781 raw words (260 effective words) took 0.0s, 173048 effective words/s\n",
      "I1203 18:55:04.166112 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.166576 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.166950 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.167284 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 781 raw words (268 effective words) took 0.0s, 178952 effective words/s\n",
      "I1203 18:55:04.168893 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.169339 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.169705 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.170048 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 781 raw words (259 effective words) took 0.0s, 177453 effective words/s\n",
      "I1203 18:55:04.171731 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.172224 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.172576 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.172931 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 781 raw words (282 effective words) took 0.0s, 184289 effective words/s\n",
      "I1203 18:55:04.174602 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.175115 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.175475 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.175828 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 781 raw words (290 effective words) took 0.0s, 188798 effective words/s\n",
      "I1203 18:55:04.176179 139821617399616 base_any2vec.py:1366] training on a 3905 raw words (1359 effective words) took 0.0s, 97352 effective words/s\n",
      "W1203 18:55:04.176514 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44039841773123234\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "0.44279497002740165\n",
      "Accuracy: 0.13636363636363635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:04.389572 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:04.390046 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:04.390530 139821617399616 word2vec.py:1405] collected 214 word types from a corpus of 584 raw words and 77 sentences\n",
      "I1203 18:55:04.390899 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:04.391409 139821617399616 word2vec.py:1480] effective_min_count=2 retains 89 unique words (41% of original 214, drops 125)\n",
      "I1203 18:55:04.391769 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 459 word corpus (78% of original 584, drops 125)\n",
      "I1203 18:55:04.392505 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 214 items\n",
      "I1203 18:55:04.392868 139821617399616 word2vec.py:1550] sample=0.001 downsamples 89 most-common words\n",
      "I1203 18:55:04.393199 139821617399616 word2vec.py:1551] downsampling leaves estimated 166 word corpus (36.2% of prior 459)\n",
      "I1203 18:55:04.393707 139821617399616 base_any2vec.py:1006] estimated required memory for 89 words and 100 dimensions: 115700 bytes\n",
      "I1203 18:55:04.394060 139821617399616 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "Accuracy: 0.3076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:04.417567 139821617399616 base_any2vec.py:1192] training model with 3 workers on 89 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:04.419955 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.420421 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.420815 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.421133 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 584 raw words (156 effective words) took 0.0s, 106281 effective words/s\n",
      "I1203 18:55:04.422811 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.423267 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.423618 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.423976 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 584 raw words (166 effective words) took 0.0s, 113949 effective words/s\n",
      "I1203 18:55:04.425621 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.426115 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.426476 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.426841 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 584 raw words (170 effective words) took 0.0s, 114601 effective words/s\n",
      "I1203 18:55:04.428491 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.428989 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.429344 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.429687 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 584 raw words (179 effective words) took 0.0s, 121589 effective words/s\n",
      "I1203 18:55:04.431397 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.431899 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.432267 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.432595 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 584 raw words (163 effective words) took 0.0s, 110944 effective words/s\n",
      "I1203 18:55:04.432966 139821617399616 base_any2vec.py:1366] training on a 2920 raw words (834 effective words) took 0.0s, 55651 effective words/s\n",
      "W1203 18:55:04.433300 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:04.621548 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:04.622034 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:04.622528 139821617399616 word2vec.py:1405] collected 232 word types from a corpus of 688 raw words and 62 sentences\n",
      "I1203 18:55:04.622894 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:04.623437 139821617399616 word2vec.py:1480] effective_min_count=2 retains 107 unique words (46% of original 232, drops 125)\n",
      "I1203 18:55:04.623807 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 563 word corpus (81% of original 688, drops 125)\n",
      "I1203 18:55:04.624737 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 232 items\n",
      "I1203 18:55:04.625134 139821617399616 word2vec.py:1550] sample=0.001 downsamples 107 most-common words\n",
      "I1203 18:55:04.625473 139821617399616 word2vec.py:1551] downsampling leaves estimated 227 word corpus (40.5% of prior 563)\n",
      "I1203 18:55:04.626005 139821617399616 base_any2vec.py:1006] estimated required memory for 107 words and 100 dimensions: 139100 bytes\n",
      "I1203 18:55:04.626354 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:04.654978 139821617399616 base_any2vec.py:1192] training model with 3 workers on 107 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:04.656511 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.656972 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.657331 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.657658 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 688 raw words (232 effective words) took 0.0s, 160001 effective words/s\n",
      "I1203 18:55:04.659408 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.659906 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.660274 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.660617 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 688 raw words (208 effective words) took 0.0s, 137748 effective words/s\n",
      "I1203 18:55:04.662291 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.662760 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.663117 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.663462 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 688 raw words (228 effective words) took 0.0s, 155884 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "0.4796341365450697\n",
      "Accuracy: 0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:04.664916 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.665837 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.666194 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.666522 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 688 raw words (238 effective words) took 0.0s, 146322 effective words/s\n",
      "I1203 18:55:04.668181 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.668633 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.669005 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.669335 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 688 raw words (220 effective words) took 0.0s, 151982 effective words/s\n",
      "I1203 18:55:04.669672 139821617399616 base_any2vec.py:1366] training on a 3440 raw words (1126 effective words) took 0.0s, 78760 effective words/s\n",
      "W1203 18:55:04.670031 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:04.862213 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:04.862658 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:04.863151 139821617399616 word2vec.py:1405] collected 149 word types from a corpus of 448 raw words and 70 sentences\n",
      "I1203 18:55:04.863506 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:04.864022 139821617399616 word2vec.py:1480] effective_min_count=2 retains 76 unique words (51% of original 149, drops 73)\n",
      "I1203 18:55:04.864375 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 375 word corpus (83% of original 448, drops 73)\n",
      "I1203 18:55:04.865082 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 149 items\n",
      "I1203 18:55:04.865432 139821617399616 word2vec.py:1550] sample=0.001 downsamples 76 most-common words\n",
      "I1203 18:55:04.865806 139821617399616 word2vec.py:1551] downsampling leaves estimated 124 word corpus (33.3% of prior 375)\n",
      "I1203 18:55:04.866279 139821617399616 base_any2vec.py:1006] estimated required memory for 76 words and 100 dimensions: 98800 bytes\n",
      "I1203 18:55:04.866627 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:04.887195 139821617399616 base_any2vec.py:1192] training model with 3 workers on 76 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:04.888699 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.889187 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.889524 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.889877 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 448 raw words (120 effective words) took 0.0s, 82688 effective words/s\n",
      "I1203 18:55:04.891550 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.892047 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.892396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.892742 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 448 raw words (125 effective words) took 0.0s, 85895 effective words/s\n",
      "I1203 18:55:04.894396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.894913 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.895282 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.895612 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 448 raw words (116 effective words) took 0.0s, 79445 effective words/s\n",
      "I1203 18:55:04.897255 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.897758 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.898122 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.898460 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 448 raw words (116 effective words) took 0.0s, 78739 effective words/s\n",
      "I1203 18:55:04.900144 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:04.900623 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:04.900999 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:04.901341 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 448 raw words (125 effective words) took 0.0s, 85969 effective words/s\n",
      "I1203 18:55:04.901696 139821617399616 base_any2vec.py:1366] training on a 2240 raw words (602 effective words) took 0.0s, 42672 effective words/s\n",
      "W1203 18:55:04.902052 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "0.48574533007365517\n",
      "Accuracy: 0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:05.067967 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:05.068430 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:05.068882 139821617399616 word2vec.py:1405] collected 118 word types from a corpus of 305 raw words and 66 sentences\n",
      "I1203 18:55:05.069227 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:05.069675 139821617399616 word2vec.py:1480] effective_min_count=2 retains 59 unique words (50% of original 118, drops 59)\n",
      "I1203 18:55:05.070043 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 246 word corpus (80% of original 305, drops 59)\n",
      "I1203 18:55:05.070635 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 118 items\n",
      "I1203 18:55:05.070977 139821617399616 word2vec.py:1550] sample=0.001 downsamples 59 most-common words\n",
      "I1203 18:55:05.071318 139821617399616 word2vec.py:1551] downsampling leaves estimated 70 word corpus (28.5% of prior 246)\n",
      "I1203 18:55:05.071775 139821617399616 base_any2vec.py:1006] estimated required memory for 59 words and 100 dimensions: 76700 bytes\n",
      "I1203 18:55:05.072115 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:05.087424 139821617399616 base_any2vec.py:1192] training model with 3 workers on 59 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:05.089901 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.090370 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.090729 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.091073 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 305 raw words (67 effective words) took 0.0s, 46952 effective words/s\n",
      "I1203 18:55:05.092743 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.093204 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.093550 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.093903 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 305 raw words (83 effective words) took 0.0s, 58920 effective words/s\n",
      "I1203 18:55:05.095495 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.095996 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.096358 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.096710 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 305 raw words (73 effective words) took 0.0s, 50802 effective words/s\n",
      "I1203 18:55:05.098323 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.098825 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.099183 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.099509 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 305 raw words (72 effective words) took 0.0s, 51240 effective words/s\n",
      "I1203 18:55:05.101099 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.101565 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.101940 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.102275 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 305 raw words (79 effective words) took 0.0s, 56691 effective words/s\n",
      "I1203 18:55:05.102627 139821617399616 base_any2vec.py:1366] training on a 1525 raw words (374 effective words) took 0.0s, 25309 effective words/s\n",
      "W1203 18:55:05.102974 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "0.5695889133805003\n",
      "Accuracy: 0.2\n",
      "0.5695889133805003\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "0.6676972842357352\n",
      "Accuracy: 0.23333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:05.259658 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:05.260169 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:05.260611 139821617399616 word2vec.py:1405] collected 167 word types from a corpus of 377 raw words and 63 sentences\n",
      "I1203 18:55:05.260981 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:05.261445 139821617399616 word2vec.py:1480] effective_min_count=2 retains 67 unique words (40% of original 167, drops 100)\n",
      "I1203 18:55:05.261802 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 277 word corpus (73% of original 377, drops 100)\n",
      "I1203 18:55:05.262511 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 167 items\n",
      "I1203 18:55:05.262889 139821617399616 word2vec.py:1550] sample=0.001 downsamples 67 most-common words\n",
      "I1203 18:55:05.263220 139821617399616 word2vec.py:1551] downsampling leaves estimated 86 word corpus (31.1% of prior 277)\n",
      "I1203 18:55:05.263675 139821617399616 base_any2vec.py:1006] estimated required memory for 67 words and 100 dimensions: 87100 bytes\n",
      "I1203 18:55:05.264028 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:05.281432 139821617399616 base_any2vec.py:1192] training model with 3 workers on 67 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:05.283796 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.284256 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.284617 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.284964 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 377 raw words (91 effective words) took 0.0s, 64082 effective words/s\n",
      "I1203 18:55:05.286575 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.287032 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.287390 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.287734 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 377 raw words (72 effective words) took 0.0s, 51449 effective words/s\n",
      "I1203 18:55:05.289309 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.289762 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.290124 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.290454 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 377 raw words (81 effective words) took 0.0s, 58571 effective words/s\n",
      "I1203 18:55:05.292123 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.292572 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.292942 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.293295 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 377 raw words (84 effective words) took 0.0s, 59652 effective words/s\n",
      "I1203 18:55:05.294933 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.295376 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.295744 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.296090 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 377 raw words (94 effective words) took 0.0s, 67294 effective words/s\n",
      "I1203 18:55:05.296441 139821617399616 base_any2vec.py:1366] training on a 1885 raw words (422 effective words) took 0.0s, 31112 effective words/s\n",
      "W1203 18:55:05.296809 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:05.470813 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:05.471267 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:05.471740 139821617399616 word2vec.py:1405] collected 186 word types from a corpus of 482 raw words and 68 sentences\n",
      "I1203 18:55:05.472095 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:05.472582 139821617399616 word2vec.py:1480] effective_min_count=2 retains 78 unique words (41% of original 186, drops 108)\n",
      "I1203 18:55:05.472933 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 374 word corpus (77% of original 482, drops 108)\n",
      "I1203 18:55:05.473632 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 186 items\n",
      "I1203 18:55:05.474000 139821617399616 word2vec.py:1550] sample=0.001 downsamples 78 most-common words\n",
      "I1203 18:55:05.474328 139821617399616 word2vec.py:1551] downsampling leaves estimated 126 word corpus (33.8% of prior 374)\n",
      "I1203 18:55:05.474821 139821617399616 base_any2vec.py:1006] estimated required memory for 78 words and 100 dimensions: 101400 bytes\n",
      "I1203 18:55:05.475163 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:05.495301 139821617399616 base_any2vec.py:1192] training model with 3 workers on 78 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:05.497779 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.498247 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.498603 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.498962 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 482 raw words (125 effective words) took 0.0s, 85869 effective words/s\n",
      "I1203 18:55:05.500601 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.501085 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.501420 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.501767 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 482 raw words (138 effective words) took 0.0s, 96533 effective words/s\n",
      "I1203 18:55:05.503396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.503889 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.504237 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.504584 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 482 raw words (121 effective words) took 0.0s, 83260 effective words/s\n",
      "I1203 18:55:05.506179 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.506652 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.507029 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.507362 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 482 raw words (135 effective words) took 0.0s, 93295 effective words/s\n",
      "I1203 18:55:05.508965 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.509420 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.509786 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.510134 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 482 raw words (122 effective words) took 0.0s, 85924 effective words/s\n",
      "I1203 18:55:05.510469 139821617399616 base_any2vec.py:1366] training on a 2410 raw words (641 effective words) took 0.0s, 47087 effective words/s\n",
      "W1203 18:55:05.510842 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "0.7430513806359512\n",
      "Accuracy: 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:05.720204 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:05.720692 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:05.721180 139821617399616 word2vec.py:1405] collected 164 word types from a corpus of 520 raw words and 82 sentences\n",
      "I1203 18:55:05.721525 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:05.722026 139821617399616 word2vec.py:1480] effective_min_count=2 retains 75 unique words (45% of original 164, drops 89)\n",
      "I1203 18:55:05.722364 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 431 word corpus (82% of original 520, drops 89)\n",
      "I1203 18:55:05.723055 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 164 items\n",
      "I1203 18:55:05.723404 139821617399616 word2vec.py:1550] sample=0.001 downsamples 75 most-common words\n",
      "I1203 18:55:05.723748 139821617399616 word2vec.py:1551] downsampling leaves estimated 140 word corpus (32.5% of prior 431)\n",
      "I1203 18:55:05.724233 139821617399616 base_any2vec.py:1006] estimated required memory for 75 words and 100 dimensions: 97500 bytes\n",
      "I1203 18:55:05.724572 139821617399616 word2vec.py:1699] resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7462458711127139\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "Accuracy: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:05.744004 139821617399616 base_any2vec.py:1192] training model with 3 workers on 75 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:05.746626 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.747106 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.747468 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.747825 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 520 raw words (134 effective words) took 0.0s, 91278 effective words/s\n",
      "I1203 18:55:05.749501 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.749976 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.750339 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.750666 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 520 raw words (128 effective words) took 0.0s, 88878 effective words/s\n",
      "I1203 18:55:05.752341 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.752844 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.753201 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.753540 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 520 raw words (155 effective words) took 0.0s, 106258 effective words/s\n",
      "I1203 18:55:05.755191 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.755668 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.756058 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.756386 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 520 raw words (138 effective words) took 0.0s, 95215 effective words/s\n",
      "I1203 18:55:05.758470 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.758937 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.759272 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.759578 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 520 raw words (159 effective words) took 0.0s, 82981 effective words/s\n",
      "I1203 18:55:05.759928 139821617399616 base_any2vec.py:1366] training on a 2600 raw words (714 effective words) took 0.0s, 46752 effective words/s\n",
      "W1203 18:55:05.760247 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:05.934373 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:05.934834 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:05.935317 139821617399616 word2vec.py:1405] collected 237 word types from a corpus of 673 raw words and 57 sentences\n",
      "I1203 18:55:05.935642 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:05.936177 139821617399616 word2vec.py:1480] effective_min_count=2 retains 102 unique words (43% of original 237, drops 135)\n",
      "I1203 18:55:05.936497 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 538 word corpus (79% of original 673, drops 135)\n",
      "I1203 18:55:05.937376 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 237 items\n",
      "I1203 18:55:05.937739 139821617399616 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1203 18:55:05.938066 139821617399616 word2vec.py:1551] downsampling leaves estimated 211 word corpus (39.3% of prior 538)\n",
      "I1203 18:55:05.938556 139821617399616 base_any2vec.py:1006] estimated required memory for 102 words and 100 dimensions: 132600 bytes\n",
      "I1203 18:55:05.938886 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:05.964512 139821617399616 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:05.966897 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.967333 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.967672 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.968009 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 673 raw words (201 effective words) took 0.0s, 142898 effective words/s\n",
      "I1203 18:55:05.969544 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.969992 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.970326 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.970630 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 673 raw words (223 effective words) took 0.0s, 164508 effective words/s\n",
      "I1203 18:55:05.972183 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.972695 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.973053 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.973367 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 673 raw words (208 effective words) took 0.0s, 142578 effective words/s\n",
      "I1203 18:55:05.974945 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.975389 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.975744 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.976098 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 673 raw words (214 effective words) took 0.0s, 149788 effective words/s\n",
      "I1203 18:55:05.977666 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:05.978150 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:05.978476 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:05.978823 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 673 raw words (218 effective words) took 0.0s, 152000 effective words/s\n",
      "I1203 18:55:05.979146 139821617399616 base_any2vec.py:1366] training on a 3365 raw words (1064 effective words) took 0.0s, 80571 effective words/s\n",
      "W1203 18:55:05.979462 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7503420157835115\n",
      "0.7903420157835115\n",
      "0.7903420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "0.8003420157835115\n",
      "Accuracy: 0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:06.155057 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:06.155497 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:06.155981 139821617399616 word2vec.py:1405] collected 207 word types from a corpus of 616 raw words and 56 sentences\n",
      "I1203 18:55:06.156308 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:06.156828 139821617399616 word2vec.py:1480] effective_min_count=2 retains 93 unique words (44% of original 207, drops 114)\n",
      "I1203 18:55:06.157170 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 502 word corpus (81% of original 616, drops 114)\n",
      "I1203 18:55:06.158005 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 207 items\n",
      "I1203 18:55:06.158356 139821617399616 word2vec.py:1550] sample=0.001 downsamples 93 most-common words\n",
      "I1203 18:55:06.158674 139821617399616 word2vec.py:1551] downsampling leaves estimated 184 word corpus (36.8% of prior 502)\n",
      "I1203 18:55:06.159178 139821617399616 base_any2vec.py:1006] estimated required memory for 93 words and 100 dimensions: 120900 bytes\n",
      "I1203 18:55:06.159492 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:06.182883 139821617399616 base_any2vec.py:1192] training model with 3 workers on 93 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:06.185261 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.185705 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.186059 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.186379 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 616 raw words (177 effective words) took 0.0s, 128014 effective words/s\n",
      "I1203 18:55:06.187926 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.188362 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.188706 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.189030 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 616 raw words (201 effective words) took 0.0s, 145152 effective words/s\n",
      "I1203 18:55:06.190558 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.191046 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.191393 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.191732 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 616 raw words (173 effective words) took 0.0s, 120461 effective words/s\n",
      "I1203 18:55:06.193295 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.193763 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.194102 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.194421 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 616 raw words (161 effective words) took 0.0s, 115537 effective words/s\n",
      "I1203 18:55:06.195969 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.196437 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.196798 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.197122 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 616 raw words (197 effective words) took 0.0s, 138126 effective words/s\n",
      "I1203 18:55:06.197446 139821617399616 base_any2vec.py:1366] training on a 3080 raw words (909 effective words) took 0.0s, 64512 effective words/s\n",
      "W1203 18:55:06.197775 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8003420157835115\n",
      "0.8403420157835115\n",
      "0.8403420157835115\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "Accuracy: 0.35294117647058826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:06.372116 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:06.372553 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:06.373039 139821617399616 word2vec.py:1405] collected 227 word types from a corpus of 566 raw words and 59 sentences\n",
      "I1203 18:55:06.373368 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:06.373887 139821617399616 word2vec.py:1480] effective_min_count=2 retains 90 unique words (39% of original 227, drops 137)\n",
      "I1203 18:55:06.374231 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 429 word corpus (75% of original 566, drops 137)\n",
      "I1203 18:55:06.374967 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 227 items\n",
      "I1203 18:55:06.375317 139821617399616 word2vec.py:1550] sample=0.001 downsamples 90 most-common words\n",
      "I1203 18:55:06.375628 139821617399616 word2vec.py:1551] downsampling leaves estimated 157 word corpus (36.7% of prior 429)\n",
      "I1203 18:55:06.376129 139821617399616 base_any2vec.py:1006] estimated required memory for 90 words and 100 dimensions: 117000 bytes\n",
      "I1203 18:55:06.376456 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:06.400070 139821617399616 base_any2vec.py:1192] training model with 3 workers on 90 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:06.401506 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.401973 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.402313 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.402624 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 566 raw words (160 effective words) took 0.0s, 116362 effective words/s\n",
      "I1203 18:55:06.404257 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.404722 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.405076 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.405396 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 566 raw words (164 effective words) took 0.0s, 117367 effective words/s\n",
      "I1203 18:55:06.406976 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.407439 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.407795 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.408113 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 566 raw words (142 effective words) took 0.0s, 102802 effective words/s\n",
      "I1203 18:55:06.409624 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.410097 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.410439 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.410768 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 566 raw words (157 effective words) took 0.0s, 113390 effective words/s\n",
      "I1203 18:55:06.412247 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.412695 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.413039 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.413356 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 566 raw words (157 effective words) took 0.0s, 116832 effective words/s\n",
      "I1203 18:55:06.413693 139821617399616 base_any2vec.py:1366] training on a 2830 raw words (780 effective words) took 0.0s, 58973 effective words/s\n",
      "W1203 18:55:06.414035 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8811367696673732\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "Accuracy: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:06.565490 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:06.565943 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:06.566411 139821617399616 word2vec.py:1405] collected 222 word types from a corpus of 630 raw words and 44 sentences\n",
      "I1203 18:55:06.566756 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:06.567261 139821617399616 word2vec.py:1480] effective_min_count=2 retains 92 unique words (41% of original 222, drops 130)\n",
      "I1203 18:55:06.567579 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 500 word corpus (79% of original 630, drops 130)\n",
      "I1203 18:55:06.568349 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 222 items\n",
      "I1203 18:55:06.568686 139821617399616 word2vec.py:1550] sample=0.001 downsamples 92 most-common words\n",
      "I1203 18:55:06.569004 139821617399616 word2vec.py:1551] downsampling leaves estimated 182 word corpus (36.6% of prior 500)\n",
      "I1203 18:55:06.569480 139821617399616 base_any2vec.py:1006] estimated required memory for 92 words and 100 dimensions: 119600 bytes\n",
      "I1203 18:55:06.569825 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:06.593977 139821617399616 base_any2vec.py:1192] training model with 3 workers on 92 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:06.595404 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.595856 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.596215 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.596521 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 630 raw words (172 effective words) took 0.0s, 124526 effective words/s\n",
      "I1203 18:55:06.598044 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.598474 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.598817 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.599130 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 630 raw words (192 effective words) took 0.0s, 141820 effective words/s\n",
      "I1203 18:55:06.600613 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.601053 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.601384 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.601715 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 630 raw words (192 effective words) took 0.0s, 141901 effective words/s\n",
      "I1203 18:55:06.603287 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.603729 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.604069 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.604379 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 630 raw words (188 effective words) took 0.0s, 140395 effective words/s\n",
      "I1203 18:55:06.605918 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.606348 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.606701 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.607031 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 630 raw words (182 effective words) took 0.0s, 130289 effective words/s\n",
      "I1203 18:55:06.607345 139821617399616 base_any2vec.py:1366] training on a 3150 raw words (926 effective words) took 0.0s, 71268 effective words/s\n",
      "W1203 18:55:06.607665 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.8817885746803878\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "Accuracy: 0.24\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "0.918742755115106\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:06.905944 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:06.906412 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:06.906940 139821617399616 word2vec.py:1405] collected 226 word types from a corpus of 711 raw words and 124 sentences\n",
      "I1203 18:55:06.907275 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:06.907916 139821617399616 word2vec.py:1480] effective_min_count=2 retains 100 unique words (44% of original 226, drops 126)\n",
      "I1203 18:55:06.908245 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 585 word corpus (82% of original 711, drops 126)\n",
      "I1203 18:55:06.909037 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 226 items\n",
      "I1203 18:55:06.909370 139821617399616 word2vec.py:1550] sample=0.001 downsamples 100 most-common words\n",
      "I1203 18:55:06.909696 139821617399616 word2vec.py:1551] downsampling leaves estimated 225 word corpus (38.6% of prior 585)\n",
      "I1203 18:55:06.910209 139821617399616 base_any2vec.py:1006] estimated required memory for 100 words and 100 dimensions: 130000 bytes\n",
      "I1203 18:55:06.910527 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:06.935867 139821617399616 base_any2vec.py:1192] training model with 3 workers on 100 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:06.937448 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.937901 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.938237 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.938550 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 711 raw words (215 effective words) took 0.0s, 154763 effective words/s\n",
      "I1203 18:55:06.940187 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.940630 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.940983 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.941300 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 711 raw words (213 effective words) took 0.0s, 151775 effective words/s\n",
      "I1203 18:55:06.942937 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.943396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.943747 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.944077 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 711 raw words (220 effective words) took 0.0s, 151484 effective words/s\n",
      "I1203 18:55:06.945693 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.946157 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.946490 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.946824 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 711 raw words (227 effective words) took 0.0s, 160529 effective words/s\n",
      "I1203 18:55:06.948389 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:06.948841 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:06.949189 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:06.949496 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 711 raw words (216 effective words) took 0.0s, 157559 effective words/s\n",
      "I1203 18:55:06.949851 139821617399616 base_any2vec.py:1366] training on a 3555 raw words (1091 effective words) took 0.0s, 80342 effective words/s\n",
      "W1203 18:55:06.950160 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:07.155798 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:07.156226 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:07.156672 139821617399616 word2vec.py:1405] collected 181 word types from a corpus of 474 raw words and 72 sentences\n",
      "I1203 18:55:07.157009 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:07.157467 139821617399616 word2vec.py:1480] effective_min_count=2 retains 75 unique words (41% of original 181, drops 106)\n",
      "I1203 18:55:07.157805 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 368 word corpus (77% of original 474, drops 106)\n",
      "I1203 18:55:07.158476 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 181 items\n",
      "I1203 18:55:07.158814 139821617399616 word2vec.py:1550] sample=0.001 downsamples 75 most-common words\n",
      "I1203 18:55:07.159130 139821617399616 word2vec.py:1551] downsampling leaves estimated 119 word corpus (32.6% of prior 368)\n",
      "I1203 18:55:07.159583 139821617399616 base_any2vec.py:1006] estimated required memory for 75 words and 100 dimensions: 97500 bytes\n",
      "I1203 18:55:07.159920 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:07.179818 139821617399616 base_any2vec.py:1192] training model with 3 workers on 75 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:07.181225 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.181667 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.182021 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.182334 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 474 raw words (112 effective words) took 0.0s, 83091 effective words/s\n",
      "I1203 18:55:07.183969 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.184457 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.184823 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.185150 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 474 raw words (126 effective words) took 0.0s, 87718 effective words/s\n",
      "I1203 18:55:07.186743 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.187232 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.187588 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.187924 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 474 raw words (123 effective words) took 0.0s, 86108 effective words/s\n",
      "I1203 18:55:07.189478 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.189932 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.190278 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918742755115106\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "Accuracy: 0.1590909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:07.190597 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 474 raw words (117 effective words) took 0.0s, 86877 effective words/s\n",
      "I1203 18:55:07.192305 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.192711 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.193054 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.193378 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 474 raw words (122 effective words) took 0.0s, 93650 effective words/s\n",
      "I1203 18:55:07.193719 139821617399616 base_any2vec.py:1366] training on a 2370 raw words (600 effective words) took 0.0s, 44390 effective words/s\n",
      "W1203 18:55:07.194056 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:07.417494 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:07.417952 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:07.418399 139821617399616 word2vec.py:1405] collected 180 word types from a corpus of 451 raw words and 94 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0043467031864042\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "Accuracy: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:07.418877 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:07.419373 139821617399616 word2vec.py:1480] effective_min_count=2 retains 72 unique words (40% of original 180, drops 108)\n",
      "I1203 18:55:07.419718 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 343 word corpus (76% of original 451, drops 108)\n",
      "I1203 18:55:07.420418 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 180 items\n",
      "I1203 18:55:07.420787 139821617399616 word2vec.py:1550] sample=0.001 downsamples 72 most-common words\n",
      "I1203 18:55:07.421106 139821617399616 word2vec.py:1551] downsampling leaves estimated 108 word corpus (31.7% of prior 343)\n",
      "I1203 18:55:07.421559 139821617399616 base_any2vec.py:1006] estimated required memory for 72 words and 100 dimensions: 93600 bytes\n",
      "I1203 18:55:07.421903 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:07.440574 139821617399616 base_any2vec.py:1192] training model with 3 workers on 72 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:07.442031 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.442469 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.442833 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.443136 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 451 raw words (104 effective words) took 0.0s, 77459 effective words/s\n",
      "I1203 18:55:07.444762 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.445217 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.445549 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.445899 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 451 raw words (103 effective words) took 0.0s, 73647 effective words/s\n",
      "I1203 18:55:07.447436 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.447913 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.448257 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.448568 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 451 raw words (114 effective words) took 0.0s, 83726 effective words/s\n",
      "I1203 18:55:07.450088 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.450515 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.450872 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.451188 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 451 raw words (102 effective words) took 0.0s, 76764 effective words/s\n",
      "I1203 18:55:07.452668 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.453113 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.453439 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.453766 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 451 raw words (121 effective words) took 0.0s, 91345 effective words/s\n",
      "I1203 18:55:07.454102 139821617399616 base_any2vec.py:1366] training on a 2255 raw words (544 effective words) took 0.0s, 41465 effective words/s\n",
      "W1203 18:55:07.454410 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0140905610408726\n",
      "1.0140905610408726\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:07.720035 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:07.720479 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:07.720964 139821617399616 word2vec.py:1405] collected 169 word types from a corpus of 526 raw words and 115 sentences\n",
      "I1203 18:55:07.721293 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:07.721757 139821617399616 word2vec.py:1480] effective_min_count=2 retains 72 unique words (42% of original 169, drops 97)\n",
      "I1203 18:55:07.722079 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 429 word corpus (81% of original 526, drops 97)\n",
      "I1203 18:55:07.722739 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 169 items\n",
      "I1203 18:55:07.723068 139821617399616 word2vec.py:1550] sample=0.001 downsamples 72 most-common words\n",
      "I1203 18:55:07.723379 139821617399616 word2vec.py:1551] downsampling leaves estimated 134 word corpus (31.3% of prior 429)\n",
      "I1203 18:55:07.723843 139821617399616 base_any2vec.py:1006] estimated required memory for 72 words and 100 dimensions: 93600 bytes\n",
      "I1203 18:55:07.724163 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:07.742774 139821617399616 base_any2vec.py:1192] training model with 3 workers on 72 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:07.744264 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.744709 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.745058 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.745374 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 526 raw words (136 effective words) took 0.0s, 99354 effective words/s\n",
      "I1203 18:55:07.746961 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.747404 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.747770 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.748080 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 526 raw words (129 effective words) took 0.0s, 93874 effective words/s\n",
      "I1203 18:55:07.749662 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.750144 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.750484 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.750831 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 526 raw words (143 effective words) took 0.0s, 100734 effective words/s\n",
      "I1203 18:55:07.752408 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.752885 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.753232 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.753553 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 526 raw words (143 effective words) took 0.0s, 103188 effective words/s\n",
      "I1203 18:55:07.755097 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.755533 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.755901 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.756218 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 526 raw words (120 effective words) took 0.0s, 88569 effective words/s\n",
      "I1203 18:55:07.756550 139821617399616 base_any2vec.py:1366] training on a 2630 raw words (671 effective words) took 0.0s, 50129 effective words/s\n",
      "W1203 18:55:07.756895 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:07.839102 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:07.839539 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:07.839931 139821617399616 word2vec.py:1405] collected 95 word types from a corpus of 166 raw words and 19 sentences\n",
      "I1203 18:55:07.840264 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:07.840636 139821617399616 word2vec.py:1480] effective_min_count=2 retains 34 unique words (35% of original 95, drops 61)\n",
      "I1203 18:55:07.840989 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 105 word corpus (63% of original 166, drops 61)\n",
      "I1203 18:55:07.841461 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 95 items\n",
      "I1203 18:55:07.841796 139821617399616 word2vec.py:1550] sample=0.001 downsamples 34 most-common words\n",
      "I1203 18:55:07.842116 139821617399616 word2vec.py:1551] downsampling leaves estimated 22 word corpus (21.2% of prior 105)\n",
      "I1203 18:55:07.842507 139821617399616 base_any2vec.py:1006] estimated required memory for 34 words and 100 dimensions: 44200 bytes\n",
      "I1203 18:55:07.842839 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:07.851667 139821617399616 base_any2vec.py:1192] training model with 3 workers on 34 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:07.853830 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.854253 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.854594 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.854935 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 166 raw words (26 effective words) took 0.0s, 19773 effective words/s\n",
      "I1203 18:55:07.856431 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.856904 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.857249 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.857567 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 166 raw words (20 effective words) took 0.0s, 15067 effective words/s\n",
      "I1203 18:55:07.859016 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.859412 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.859745 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.860068 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 166 raw words (17 effective words) took 0.0s, 13708 effective words/s\n",
      "I1203 18:55:07.861495 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.861941 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.862279 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.862598 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 166 raw words (23 effective words) took 0.0s, 17853 effective words/s\n",
      "I1203 18:55:07.864008 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:07.864422 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:07.864778 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:07.865079 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 166 raw words (16 effective words) took 0.0s, 12835 effective words/s\n",
      "I1203 18:55:07.865400 139821617399616 base_any2vec.py:1366] training on a 830 raw words (102 effective words) took 0.0s, 7644 effective words/s\n",
      "W1203 18:55:07.865725 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0601943054088312\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "1.0835572558034945\n",
      "Accuracy: 0.17777777777777778\n",
      "1.0835572558034945\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "Accuracy: 0.29411764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:08.054316 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:08.054770 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:08.055225 139821617399616 word2vec.py:1405] collected 175 word types from a corpus of 396 raw words and 87 sentences\n",
      "I1203 18:55:08.055551 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:08.056040 139821617399616 word2vec.py:1480] effective_min_count=2 retains 68 unique words (38% of original 175, drops 107)\n",
      "I1203 18:55:08.056375 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 289 word corpus (72% of original 396, drops 107)\n",
      "I1203 18:55:08.057092 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 175 items\n",
      "I1203 18:55:08.057439 139821617399616 word2vec.py:1550] sample=0.001 downsamples 68 most-common words\n",
      "I1203 18:55:08.057793 139821617399616 word2vec.py:1551] downsampling leaves estimated 89 word corpus (31.0% of prior 289)\n",
      "I1203 18:55:08.058257 139821617399616 base_any2vec.py:1006] estimated required memory for 68 words and 100 dimensions: 88400 bytes\n",
      "I1203 18:55:08.058578 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:08.076091 139821617399616 base_any2vec.py:1192] training model with 3 workers on 68 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:08.078401 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.078858 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.079201 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.079512 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 396 raw words (91 effective words) took 0.0s, 68104 effective words/s\n",
      "I1203 18:55:08.081093 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.081518 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.081859 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.082172 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 396 raw words (78 effective words) took 0.0s, 59227 effective words/s\n",
      "I1203 18:55:08.083695 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.084149 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.084488 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.084826 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 396 raw words (86 effective words) took 0.0s, 62813 effective words/s\n",
      "I1203 18:55:08.086300 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.086745 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.087095 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.087407 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 396 raw words (82 effective words) took 0.0s, 62032 effective words/s\n",
      "I1203 18:55:08.088917 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.089404 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.089762 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.090081 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 396 raw words (89 effective words) took 0.0s, 62852 effective words/s\n",
      "I1203 18:55:08.090406 139821617399616 base_any2vec.py:1366] training on a 1980 raw words (426 effective words) took 0.0s, 30700 effective words/s\n",
      "W1203 18:55:08.090734 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "1.0905652140592093\n",
      "Accuracy: 0.5555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:08.376278 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:08.376715 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:08.377321 139821617399616 word2vec.py:1405] collected 406 word types from a corpus of 1336 raw words and 99 sentences\n",
      "I1203 18:55:08.377650 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:08.378325 139821617399616 word2vec.py:1480] effective_min_count=2 retains 172 unique words (42% of original 406, drops 234)\n",
      "I1203 18:55:08.378648 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 1102 word corpus (82% of original 1336, drops 234)\n",
      "I1203 18:55:08.379626 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 406 items\n",
      "I1203 18:55:08.379988 139821617399616 word2vec.py:1550] sample=0.001 downsamples 101 most-common words\n",
      "I1203 18:55:08.380299 139821617399616 word2vec.py:1551] downsampling leaves estimated 546 word corpus (49.6% of prior 1102)\n",
      "I1203 18:55:08.380900 139821617399616 base_any2vec.py:1006] estimated required memory for 172 words and 100 dimensions: 223600 bytes\n",
      "I1203 18:55:08.381229 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:08.424711 139821617399616 base_any2vec.py:1192] training model with 3 workers on 172 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:08.426364 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.426756 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.427279 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.427574 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 1336 raw words (528 effective words) took 0.0s, 326825 effective words/s\n",
      "I1203 18:55:08.429311 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.429711 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.430366 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.430708 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 1336 raw words (537 effective words) took 0.0s, 299637 effective words/s\n",
      "I1203 18:55:08.432237 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.432612 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.433254 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.433571 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 1336 raw words (576 effective words) took 0.0s, 334530 effective words/s\n",
      "I1203 18:55:08.435103 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.435470 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.435916 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.436244 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 1336 raw words (535 effective words) took 0.0s, 349694 effective words/s\n",
      "I1203 18:55:08.437975 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.438385 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.438838 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.439158 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 1336 raw words (540 effective words) took 0.0s, 340062 effective words/s\n",
      "I1203 18:55:08.439483 139821617399616 base_any2vec.py:1366] training on a 6680 raw words (2716 effective words) took 0.0s, 189185 effective words/s\n",
      "W1203 18:55:08.439822 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0905652140592093\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1026571736446247\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.1307710303581122\n",
      "1.231803143328384\n",
      "1.231803143328384\n",
      "1.231803143328384\n",
      "1.231803143328384\n",
      "1.231803143328384\n",
      "Accuracy: 0.2375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:08.782635 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:08.783123 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:08.783643 139821617399616 word2vec.py:1405] collected 270 word types from a corpus of 817 raw words and 122 sentences\n",
      "I1203 18:55:08.783997 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:08.784570 139821617399616 word2vec.py:1480] effective_min_count=2 retains 125 unique words (46% of original 270, drops 145)\n",
      "I1203 18:55:08.784925 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 672 word corpus (82% of original 817, drops 145)\n",
      "I1203 18:55:08.785894 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 270 items\n",
      "I1203 18:55:08.786245 139821617399616 word2vec.py:1550] sample=0.001 downsamples 125 most-common words\n",
      "I1203 18:55:08.786571 139821617399616 word2vec.py:1551] downsampling leaves estimated 298 word corpus (44.5% of prior 672)\n",
      "I1203 18:55:08.787120 139821617399616 base_any2vec.py:1006] estimated required memory for 125 words and 100 dimensions: 162500 bytes\n",
      "I1203 18:55:08.787450 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:08.818660 139821617399616 base_any2vec.py:1192] training model with 3 workers on 125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:08.820275 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.820662 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.821092 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.821403 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 817 raw words (287 effective words) took 0.0s, 199246 effective words/s\n",
      "I1203 18:55:08.823106 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.823507 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.823920 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.824237 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 817 raw words (335 effective words) took 0.0s, 230235 effective words/s\n",
      "I1203 18:55:08.825876 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.826285 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.826701 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.827030 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 817 raw words (294 effective words) took 0.0s, 199807 effective words/s\n",
      "I1203 18:55:08.828716 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.829137 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.829544 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.829892 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 817 raw words (312 effective words) took 0.0s, 208485 effective words/s\n",
      "I1203 18:55:08.831490 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:08.831882 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:08.832287 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:08.832610 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 817 raw words (298 effective words) took 0.0s, 211205 effective words/s\n",
      "I1203 18:55:08.832954 139821617399616 base_any2vec.py:1366] training on a 4085 raw words (1526 effective words) took 0.0s, 110043 effective words/s\n",
      "W1203 18:55:08.833273 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.231803143328384\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.310020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "1.320020306556979\n",
      "Accuracy: 0.20512820512820512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:09.134412 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:09.134860 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:09.135289 139821617399616 word2vec.py:1405] collected 100 word types from a corpus of 281 raw words and 130 sentences\n",
      "I1203 18:55:09.135617 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:09.136039 139821617399616 word2vec.py:1480] effective_min_count=2 retains 42 unique words (42% of original 100, drops 58)\n",
      "I1203 18:55:09.136365 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 223 word corpus (79% of original 281, drops 58)\n",
      "I1203 18:55:09.136890 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 100 items\n",
      "I1203 18:55:09.137216 139821617399616 word2vec.py:1550] sample=0.001 downsamples 42 most-common words\n",
      "I1203 18:55:09.137523 139821617399616 word2vec.py:1551] downsampling leaves estimated 50 word corpus (22.5% of prior 223)\n",
      "I1203 18:55:09.137949 139821617399616 base_any2vec.py:1006] estimated required memory for 42 words and 100 dimensions: 54600 bytes\n",
      "I1203 18:55:09.138265 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:09.149049 139821617399616 base_any2vec.py:1192] training model with 3 workers on 42 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:09.151308 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.151772 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.152109 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.152418 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 281 raw words (47 effective words) took 0.0s, 35393 effective words/s\n",
      "I1203 18:55:09.154026 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.154470 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.154831 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.155155 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 281 raw words (44 effective words) took 0.0s, 32949 effective words/s\n",
      "I1203 18:55:09.156650 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.157107 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.157434 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.157757 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 281 raw words (45 effective words) took 0.0s, 34726 effective words/s\n",
      "I1203 18:55:09.159248 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.159663 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.160025 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.160330 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 281 raw words (49 effective words) took 0.0s, 38444 effective words/s\n",
      "I1203 18:55:09.161819 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.162257 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.162577 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.162905 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 281 raw words (58 effective words) took 0.0s, 44169 effective words/s\n",
      "I1203 18:55:09.163225 139821617399616 base_any2vec.py:1366] training on a 1405 raw words (243 effective words) took 0.0s, 18424 effective words/s\n",
      "W1203 18:55:09.163533 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:09.305495 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:09.305960 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:09.306376 139821617399616 word2vec.py:1405] collected 124 word types from a corpus of 296 raw words and 55 sentences\n",
      "I1203 18:55:09.306705 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:09.307131 139821617399616 word2vec.py:1480] effective_min_count=2 retains 51 unique words (41% of original 124, drops 73)\n",
      "I1203 18:55:09.307462 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 223 word corpus (75% of original 296, drops 73)\n",
      "I1203 18:55:09.308036 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 124 items\n",
      "I1203 18:55:09.308355 139821617399616 word2vec.py:1550] sample=0.001 downsamples 51 most-common words\n",
      "I1203 18:55:09.308691 139821617399616 word2vec.py:1551] downsampling leaves estimated 58 word corpus (26.2% of prior 223)\n",
      "I1203 18:55:09.309118 139821617399616 base_any2vec.py:1006] estimated required memory for 51 words and 100 dimensions: 66300 bytes\n",
      "I1203 18:55:09.309430 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:09.322475 139821617399616 base_any2vec.py:1192] training model with 3 workers on 51 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:09.324673 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.325102 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.325433 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.325763 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 296 raw words (62 effective words) took 0.0s, 47982 effective words/s\n",
      "I1203 18:55:09.327250 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.327670 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.328018 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.328326 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 296 raw words (58 effective words) took 0.0s, 45184 effective words/s\n",
      "I1203 18:55:09.329823 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.330239 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.330562 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.330893 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 296 raw words (59 effective words) took 0.0s, 45576 effective words/s\n",
      "I1203 18:55:09.332340 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.332816 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.333165 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.333471 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 296 raw words (51 effective words) took 0.0s, 38259 effective words/s\n",
      "I1203 18:55:09.335004 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.335476 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.335855 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.336194 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 296 raw words (54 effective words) took 0.0s, 38755 effective words/s\n",
      "I1203 18:55:09.336541 139821617399616 base_any2vec.py:1366] training on a 1480 raw words (284 effective words) took 0.0s, 22188 effective words/s\n",
      "W1203 18:55:09.336903 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.320020306556979\n",
      "1.3296314517723262\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3392425969876733\n",
      "1.3488537422030205\n",
      "1.3488537422030205\n",
      "1.3488537422030205\n",
      "1.3488537422030205\n",
      "1.3488537422030205\n",
      "1.3488537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.3888537422030205\n",
      "1.4288537422030205\n",
      "1.4288537422030205\n",
      "1.4688537422030206\n",
      "1.4688537422030206\n",
      "1.5088537422030206\n",
      "1.5088537422030206\n",
      "1.5088537422030206\n",
      "1.5088537422030206\n",
      "1.5488537422030206\n",
      "1.5488537422030206\n",
      "1.5488537422030206\n",
      "1.5488537422030206\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.5888537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.6288537422030207\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "Accuracy: 0.5614035087719298\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.648076032633715\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "1.6558275361577\n",
      "Accuracy: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:09.458113 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:09.458543 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:09.459022 139821617399616 word2vec.py:1405] collected 197 word types from a corpus of 564 raw words and 38 sentences\n",
      "I1203 18:55:09.459354 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:09.459848 139821617399616 word2vec.py:1480] effective_min_count=2 retains 78 unique words (39% of original 197, drops 119)\n",
      "I1203 18:55:09.460166 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 445 word corpus (78% of original 564, drops 119)\n",
      "I1203 18:55:09.460923 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 197 items\n",
      "I1203 18:55:09.461276 139821617399616 word2vec.py:1550] sample=0.001 downsamples 78 most-common words\n",
      "I1203 18:55:09.461591 139821617399616 word2vec.py:1551] downsampling leaves estimated 148 word corpus (33.5% of prior 445)\n",
      "I1203 18:55:09.462077 139821617399616 base_any2vec.py:1006] estimated required memory for 78 words and 100 dimensions: 101400 bytes\n",
      "I1203 18:55:09.462396 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:09.482163 139821617399616 base_any2vec.py:1192] training model with 3 workers on 78 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:09.484393 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.484865 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.485210 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.485512 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 564 raw words (138 effective words) took 0.0s, 100527 effective words/s\n",
      "I1203 18:55:09.487057 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.487501 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.487868 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.488185 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 564 raw words (145 effective words) took 0.0s, 104805 effective words/s\n",
      "I1203 18:55:09.489725 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.490198 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.490532 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.490880 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 564 raw words (152 effective words) took 0.0s, 107198 effective words/s\n",
      "I1203 18:55:09.492392 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.492831 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.493190 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.493501 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 564 raw words (166 effective words) took 0.0s, 122320 effective words/s\n",
      "I1203 18:55:09.494993 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.495418 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.495764 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.496084 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 564 raw words (151 effective words) took 0.0s, 113710 effective words/s\n",
      "I1203 18:55:09.496407 139821617399616 base_any2vec.py:1366] training on a 2820 raw words (752 effective words) took 0.0s, 54307 effective words/s\n",
      "W1203 18:55:09.496729 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:09.677430 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:09.677887 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:09.678347 139821617399616 word2vec.py:1405] collected 194 word types from a corpus of 512 raw words and 66 sentences\n",
      "I1203 18:55:09.678673 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:09.679169 139821617399616 word2vec.py:1480] effective_min_count=2 retains 82 unique words (42% of original 194, drops 112)\n",
      "I1203 18:55:09.679488 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 400 word corpus (78% of original 512, drops 112)\n",
      "I1203 18:55:09.680198 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 194 items\n",
      "I1203 18:55:09.680523 139821617399616 word2vec.py:1550] sample=0.001 downsamples 82 most-common words\n",
      "I1203 18:55:09.680858 139821617399616 word2vec.py:1551] downsampling leaves estimated 138 word corpus (34.7% of prior 400)\n",
      "I1203 18:55:09.681325 139821617399616 base_any2vec.py:1006] estimated required memory for 82 words and 100 dimensions: 106600 bytes\n",
      "I1203 18:55:09.681638 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:09.703281 139821617399616 base_any2vec.py:1192] training model with 3 workers on 82 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:09.704708 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.705158 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.705494 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.705831 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 512 raw words (136 effective words) took 0.0s, 98409 effective words/s\n",
      "I1203 18:55:09.707365 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.707813 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.708154 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.708475 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 512 raw words (134 effective words) took 0.0s, 99214 effective words/s\n",
      "I1203 18:55:09.710070 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.710521 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.710892 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.711207 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 512 raw words (138 effective words) took 0.0s, 99778 effective words/s\n",
      "I1203 18:55:09.712739 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.713203 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.713540 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.713896 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 512 raw words (159 effective words) took 0.0s, 113017 effective words/s\n",
      "I1203 18:55:09.715430 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.715897 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.716242 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.716557 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 512 raw words (142 effective words) took 0.0s, 104534 effective words/s\n",
      "I1203 18:55:09.716919 139821617399616 base_any2vec.py:1366] training on a 2560 raw words (709 effective words) took 0.0s, 53590 effective words/s\n",
      "W1203 18:55:09.717226 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6558275361577\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "Accuracy: 0.2727272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:09.895987 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:09.896425 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:09.896883 139821617399616 word2vec.py:1405] collected 185 word types from a corpus of 404 raw words and 69 sentences\n",
      "I1203 18:55:09.897212 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:09.897669 139821617399616 word2vec.py:1480] effective_min_count=2 retains 69 unique words (37% of original 185, drops 116)\n",
      "I1203 18:55:09.898014 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 288 word corpus (71% of original 404, drops 116)\n",
      "I1203 18:55:09.898646 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 185 items\n",
      "I1203 18:55:09.898994 139821617399616 word2vec.py:1550] sample=0.001 downsamples 69 most-common words\n",
      "I1203 18:55:09.899309 139821617399616 word2vec.py:1551] downsampling leaves estimated 89 word corpus (31.2% of prior 288)\n",
      "I1203 18:55:09.899763 139821617399616 base_any2vec.py:1006] estimated required memory for 69 words and 100 dimensions: 89700 bytes\n",
      "I1203 18:55:09.900097 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:09.917566 139821617399616 base_any2vec.py:1192] training model with 3 workers on 69 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:09.920002 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.920433 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.920791 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.921116 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 404 raw words (90 effective words) took 0.0s, 66634 effective words/s\n",
      "I1203 18:55:09.922635 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.923082 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.923417 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.923744 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 404 raw words (86 effective words) took 0.0s, 64160 effective words/s\n",
      "I1203 18:55:09.925246 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.925721 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.926065 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.926378 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 404 raw words (81 effective words) took 0.0s, 60206 effective words/s\n",
      "I1203 18:55:09.927910 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.928364 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.928709 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.929046 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 404 raw words (85 effective words) took 0.0s, 62507 effective words/s\n",
      "I1203 18:55:09.930548 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:09.931062 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:09.931415 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:09.931761 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 404 raw words (91 effective words) took 0.0s, 63622 effective words/s\n",
      "I1203 18:55:09.932103 139821617399616 base_any2vec.py:1366] training on a 2020 raw words (433 effective words) took 0.0s, 32293 effective words/s\n",
      "W1203 18:55:09.932423 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7043966739445255\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "Accuracy: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:10.077600 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:10.078081 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:10.078565 139821617399616 word2vec.py:1405] collected 239 word types from a corpus of 645 raw words and 44 sentences\n",
      "I1203 18:55:10.078912 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:10.079414 139821617399616 word2vec.py:1480] effective_min_count=2 retains 95 unique words (39% of original 239, drops 144)\n",
      "I1203 18:55:10.079761 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 501 word corpus (77% of original 645, drops 144)\n",
      "I1203 18:55:10.080502 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 239 items\n",
      "I1203 18:55:10.080858 139821617399616 word2vec.py:1550] sample=0.001 downsamples 95 most-common words\n",
      "I1203 18:55:10.081183 139821617399616 word2vec.py:1551] downsampling leaves estimated 190 word corpus (38.0% of prior 501)\n",
      "I1203 18:55:10.081670 139821617399616 base_any2vec.py:1006] estimated required memory for 95 words and 100 dimensions: 123500 bytes\n",
      "I1203 18:55:10.082016 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:10.106857 139821617399616 base_any2vec.py:1192] training model with 3 workers on 95 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:10.108267 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.108716 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.109063 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.109365 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 645 raw words (181 effective words) took 0.0s, 132356 effective words/s\n",
      "I1203 18:55:10.110929 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.111357 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.111701 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.112034 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 645 raw words (184 effective words) took 0.0s, 131670 effective words/s\n",
      "I1203 18:55:10.113557 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.114041 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.114382 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.114717 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 645 raw words (178 effective words) took 0.0s, 124832 effective words/s\n",
      "I1203 18:55:10.116251 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.116725 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.117075 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.117403 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 645 raw words (188 effective words) took 0.0s, 132888 effective words/s\n",
      "I1203 18:55:10.118922 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.119382 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.119731 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.120053 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 645 raw words (188 effective words) took 0.0s, 134591 effective words/s\n",
      "I1203 18:55:10.120380 139821617399616 base_any2vec.py:1366] training on a 3225 raw words (919 effective words) took 0.0s, 69907 effective words/s\n",
      "W1203 18:55:10.120722 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7115346782459915\n",
      "1.7533732694564268\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "Accuracy: 0.15217391304347827\n",
      "1.767774095033661\n",
      "1.767774095033661\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "1.993237391564438\n",
      "Accuracy: 0.42857142857142855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:10.297573 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:10.298026 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:10.298499 139821617399616 word2vec.py:1405] collected 225 word types from a corpus of 662 raw words and 58 sentences\n",
      "I1203 18:55:10.298846 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:10.299367 139821617399616 word2vec.py:1480] effective_min_count=2 retains 104 unique words (46% of original 225, drops 121)\n",
      "I1203 18:55:10.299700 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 541 word corpus (81% of original 662, drops 121)\n",
      "I1203 18:55:10.300520 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 225 items\n",
      "I1203 18:55:10.300867 139821617399616 word2vec.py:1550] sample=0.001 downsamples 104 most-common words\n",
      "I1203 18:55:10.301181 139821617399616 word2vec.py:1551] downsampling leaves estimated 215 word corpus (39.9% of prior 541)\n",
      "I1203 18:55:10.301675 139821617399616 base_any2vec.py:1006] estimated required memory for 104 words and 100 dimensions: 135200 bytes\n",
      "I1203 18:55:10.302013 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:10.329090 139821617399616 base_any2vec.py:1192] training model with 3 workers on 104 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:10.330513 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.330984 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.331310 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.331629 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 662 raw words (215 effective words) took 0.0s, 154071 effective words/s\n",
      "I1203 18:55:10.333207 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.333631 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.333988 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.334294 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 662 raw words (213 effective words) took 0.0s, 156252 effective words/s\n",
      "I1203 18:55:10.335860 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.336322 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.336663 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.337007 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 662 raw words (213 effective words) took 0.0s, 149761 effective words/s\n",
      "I1203 18:55:10.338516 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.338964 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.339302 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.339612 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 662 raw words (198 effective words) took 0.0s, 146029 effective words/s\n",
      "I1203 18:55:10.341126 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.341554 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.341914 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.342231 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 662 raw words (212 effective words) took 0.0s, 155997 effective words/s\n",
      "I1203 18:55:10.342552 139821617399616 base_any2vec.py:1366] training on a 3310 raw words (1051 effective words) took 0.0s, 80263 effective words/s\n",
      "W1203 18:55:10.342894 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:10.512365 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:10.512820 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:10.513294 139821617399616 word2vec.py:1405] collected 244 word types from a corpus of 686 raw words and 50 sentences\n",
      "I1203 18:55:10.513619 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:10.514169 139821617399616 word2vec.py:1480] effective_min_count=2 retains 102 unique words (41% of original 244, drops 142)\n",
      "I1203 18:55:10.514481 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 544 word corpus (79% of original 686, drops 142)\n",
      "I1203 18:55:10.515360 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 244 items\n",
      "I1203 18:55:10.515713 139821617399616 word2vec.py:1550] sample=0.001 downsamples 102 most-common words\n",
      "I1203 18:55:10.516044 139821617399616 word2vec.py:1551] downsampling leaves estimated 215 word corpus (39.7% of prior 544)\n",
      "I1203 18:55:10.516535 139821617399616 base_any2vec.py:1006] estimated required memory for 102 words and 100 dimensions: 132600 bytes\n",
      "I1203 18:55:10.516875 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:10.542447 139821617399616 base_any2vec.py:1192] training model with 3 workers on 102 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:10.544839 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.545278 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.545616 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.545959 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 686 raw words (213 effective words) took 0.0s, 151071 effective words/s\n",
      "I1203 18:55:10.547575 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.547993 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.548409 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.548744 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 686 raw words (217 effective words) took 0.0s, 149328 effective words/s\n",
      "I1203 18:55:10.550339 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.550802 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.551153 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.551466 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 686 raw words (226 effective words) took 0.0s, 161743 effective words/s\n",
      "I1203 18:55:10.553023 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.553466 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.553842 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.554156 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 686 raw words (215 effective words) took 0.0s, 153692 effective words/s\n",
      "I1203 18:55:10.555690 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.556158 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.556500 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.556851 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 686 raw words (213 effective words) took 0.0s, 149209 effective words/s\n",
      "I1203 18:55:10.557190 139821617399616 base_any2vec.py:1366] training on a 3430 raw words (1084 effective words) took 0.0s, 76098 effective words/s\n",
      "W1203 18:55:10.557527 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.993237391564438\n",
      "1.993237391564438\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "Accuracy: 0.3125\n",
      "2.053982005153803\n",
      "2.053982005153803\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "2.096380135932547\n",
      "Accuracy: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:10.801393 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:10.801843 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:10.802336 139821617399616 word2vec.py:1405] collected 214 word types from a corpus of 631 raw words and 94 sentences\n",
      "I1203 18:55:10.802666 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:10.803195 139821617399616 word2vec.py:1480] effective_min_count=2 retains 94 unique words (43% of original 214, drops 120)\n",
      "I1203 18:55:10.803517 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 511 word corpus (80% of original 631, drops 120)\n",
      "I1203 18:55:10.804275 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 214 items\n",
      "I1203 18:55:10.804591 139821617399616 word2vec.py:1550] sample=0.001 downsamples 94 most-common words\n",
      "I1203 18:55:10.804942 139821617399616 word2vec.py:1551] downsampling leaves estimated 190 word corpus (37.2% of prior 511)\n",
      "I1203 18:55:10.805418 139821617399616 base_any2vec.py:1006] estimated required memory for 94 words and 100 dimensions: 122200 bytes\n",
      "I1203 18:55:10.805766 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:10.830197 139821617399616 base_any2vec.py:1192] training model with 3 workers on 94 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:10.831690 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.832153 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.832485 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.832821 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 631 raw words (165 effective words) took 0.0s, 116469 effective words/s\n",
      "I1203 18:55:10.834431 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.834897 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.835238 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.835558 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 631 raw words (193 effective words) took 0.0s, 136685 effective words/s\n",
      "I1203 18:55:10.837118 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.837571 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.837932 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.838246 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 631 raw words (191 effective words) took 0.0s, 138356 effective words/s\n",
      "I1203 18:55:10.839852 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.840299 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.840630 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.840981 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 631 raw words (187 effective words) took 0.0s, 131776 effective words/s\n",
      "I1203 18:55:10.842566 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:10.843046 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:10.843396 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:10.843702 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 631 raw words (177 effective words) took 0.0s, 127004 effective words/s\n",
      "I1203 18:55:10.844025 139821617399616 base_any2vec.py:1366] training on a 3155 raw words (913 effective words) took 0.0s, 67964 effective words/s\n",
      "W1203 18:55:10.844355 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:11.073614 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:11.074100 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:11.074575 139821617399616 word2vec.py:1405] collected 225 word types from a corpus of 570 raw words and 87 sentences\n",
      "I1203 18:55:11.074922 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:11.075427 139821617399616 word2vec.py:1480] effective_min_count=2 retains 99 unique words (44% of original 225, drops 126)\n",
      "I1203 18:55:11.075778 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 444 word corpus (77% of original 570, drops 126)\n",
      "I1203 18:55:11.076534 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 225 items\n",
      "I1203 18:55:11.076888 139821617399616 word2vec.py:1550] sample=0.001 downsamples 99 most-common words\n",
      "I1203 18:55:11.077198 139821617399616 word2vec.py:1551] downsampling leaves estimated 171 word corpus (38.7% of prior 444)\n",
      "I1203 18:55:11.077690 139821617399616 base_any2vec.py:1006] estimated required memory for 99 words and 100 dimensions: 128700 bytes\n",
      "I1203 18:55:11.078020 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:11.102953 139821617399616 base_any2vec.py:1192] training model with 3 workers on 99 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.096380135932547\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.096380154340724\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.1059323348350407\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.150369866322638\n",
      "Accuracy: 0.21428571428571427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:11.105521 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.105982 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.106324 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.106642 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 570 raw words (171 effective words) took 0.0s, 122515 effective words/s\n",
      "I1203 18:55:11.108215 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.108658 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.109019 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.109338 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 570 raw words (167 effective words) took 0.0s, 121113 effective words/s\n",
      "I1203 18:55:11.110890 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.111336 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.111668 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.112010 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 570 raw words (167 effective words) took 0.0s, 120381 effective words/s\n",
      "I1203 18:55:11.113603 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.114065 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.114417 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.114735 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 570 raw words (170 effective words) took 0.0s, 121817 effective words/s\n",
      "I1203 18:55:11.116341 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.116816 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.117158 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.117477 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 570 raw words (185 effective words) took 0.0s, 132034 effective words/s\n",
      "I1203 18:55:11.117825 139821617399616 base_any2vec.py:1366] training on a 2850 raw words (860 effective words) took 0.0s, 59967 effective words/s\n",
      "W1203 18:55:11.118144 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:11.268599 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:11.269073 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:11.269496 139821617399616 word2vec.py:1405] collected 169 word types from a corpus of 361 raw words and 52 sentences\n",
      "I1203 18:55:11.269845 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:11.270282 139821617399616 word2vec.py:1480] effective_min_count=2 retains 56 unique words (33% of original 169, drops 113)\n",
      "I1203 18:55:11.270600 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 248 word corpus (68% of original 361, drops 113)\n",
      "I1203 18:55:11.271197 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 169 items\n",
      "I1203 18:55:11.271517 139821617399616 word2vec.py:1550] sample=0.001 downsamples 56 most-common words\n",
      "I1203 18:55:11.271852 139821617399616 word2vec.py:1551] downsampling leaves estimated 68 word corpus (27.6% of prior 248)\n",
      "I1203 18:55:11.272276 139821617399616 base_any2vec.py:1006] estimated required memory for 56 words and 100 dimensions: 72800 bytes\n",
      "I1203 18:55:11.272589 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:11.286899 139821617399616 base_any2vec.py:1192] training model with 3 workers on 56 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:11.289115 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.289557 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.289922 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.290243 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 361 raw words (76 effective words) took 0.0s, 56255 effective words/s\n",
      "I1203 18:55:11.291743 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.292178 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.292505 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.292838 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 361 raw words (74 effective words) took 0.0s, 55750 effective words/s\n",
      "I1203 18:55:11.294287 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.294714 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.295046 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.295360 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 361 raw words (67 effective words) took 0.0s, 52347 effective words/s\n",
      "I1203 18:55:11.296792 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.297207 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.297528 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.297865 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 361 raw words (70 effective words) took 0.0s, 53740 effective words/s\n",
      "I1203 18:55:11.299330 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.299782 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.300135 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.300444 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 361 raw words (67 effective words) took 0.0s, 50490 effective words/s\n",
      "I1203 18:55:11.300792 139821617399616 base_any2vec.py:1366] training on a 1805 raw words (354 effective words) took 0.0s, 28241 effective words/s\n",
      "W1203 18:55:11.301116 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.150369866322638\n",
      "2.150369866322638\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.153071479588327\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2045578250929596\n",
      "2.2077395369380586\n",
      "2.2077395369380586\n",
      "2.2077395369380586\n",
      "Accuracy: 0.23529411764705882\n",
      "2.2077395369380586\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2154946000006435\n",
      "2.2254946000006433\n",
      "2.2254946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "2.2654946000006433\n",
      "Accuracy: 0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:11.452219 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:11.452649 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:11.453118 139821617399616 word2vec.py:1405] collected 177 word types from a corpus of 455 raw words and 56 sentences\n",
      "I1203 18:55:11.453442 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:11.453948 139821617399616 word2vec.py:1480] effective_min_count=2 retains 70 unique words (39% of original 177, drops 107)\n",
      "I1203 18:55:11.454269 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 348 word corpus (76% of original 455, drops 107)\n",
      "I1203 18:55:11.454935 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 177 items\n",
      "I1203 18:55:11.455268 139821617399616 word2vec.py:1550] sample=0.001 downsamples 70 most-common words\n",
      "I1203 18:55:11.455581 139821617399616 word2vec.py:1551] downsampling leaves estimated 108 word corpus (31.3% of prior 348)\n",
      "I1203 18:55:11.456053 139821617399616 base_any2vec.py:1006] estimated required memory for 70 words and 100 dimensions: 91000 bytes\n",
      "I1203 18:55:11.456389 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:11.475025 139821617399616 base_any2vec.py:1192] training model with 3 workers on 70 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:11.476392 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.476846 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.477191 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.477504 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 455 raw words (107 effective words) took 0.0s, 79442 effective words/s\n",
      "I1203 18:55:11.479028 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.479460 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.479801 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.480115 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 455 raw words (105 effective words) took 0.0s, 79462 effective words/s\n",
      "I1203 18:55:11.481632 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.482104 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.482448 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.482788 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 455 raw words (111 effective words) took 0.0s, 80672 effective words/s\n",
      "I1203 18:55:11.484298 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.484760 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.485098 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.485413 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 455 raw words (93 effective words) took 0.0s, 69410 effective words/s\n",
      "I1203 18:55:11.486866 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.487289 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.487622 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.487953 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 455 raw words (111 effective words) took 0.0s, 84253 effective words/s\n",
      "I1203 18:55:11.488275 139821617399616 base_any2vec.py:1366] training on a 2275 raw words (527 effective words) took 0.0s, 40933 effective words/s\n",
      "W1203 18:55:11.488586 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:11.678847 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:11.679285 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:11.679774 139821617399616 word2vec.py:1405] collected 201 word types from a corpus of 584 raw words and 70 sentences\n",
      "I1203 18:55:11.680108 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:11.680611 139821617399616 word2vec.py:1480] effective_min_count=2 retains 89 unique words (44% of original 201, drops 112)\n",
      "I1203 18:55:11.680953 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 472 word corpus (80% of original 584, drops 112)\n",
      "I1203 18:55:11.681669 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 201 items\n",
      "I1203 18:55:11.682014 139821617399616 word2vec.py:1550] sample=0.001 downsamples 89 most-common words\n",
      "I1203 18:55:11.682337 139821617399616 word2vec.py:1551] downsampling leaves estimated 170 word corpus (36.2% of prior 472)\n",
      "I1203 18:55:11.682819 139821617399616 base_any2vec.py:1006] estimated required memory for 89 words and 100 dimensions: 115700 bytes\n",
      "I1203 18:55:11.683157 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:11.705559 139821617399616 base_any2vec.py:1192] training model with 3 workers on 89 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:11.707863 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.708304 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.708645 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.708973 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 584 raw words (157 effective words) took 0.0s, 113625 effective words/s\n",
      "I1203 18:55:11.710522 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.710967 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.711297 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.711616 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 584 raw words (166 effective words) took 0.0s, 122285 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "Accuracy: 0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 18:55:11.713019 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.713876 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.714220 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.714534 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 584 raw words (163 effective words) took 0.0s, 106366 effective words/s\n",
      "I1203 18:55:11.716101 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.716570 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.716916 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.717241 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 584 raw words (178 effective words) took 0.0s, 126240 effective words/s\n",
      "I1203 18:55:11.718811 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.719255 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.719597 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.719947 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 584 raw words (170 effective words) took 0.0s, 120725 effective words/s\n",
      "I1203 18:55:11.720271 139821617399616 base_any2vec.py:1366] training on a 2920 raw words (834 effective words) took 0.0s, 58478 effective words/s\n",
      "W1203 18:55:11.720568 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "I1203 18:55:11.913389 139821617399616 word2vec.py:1399] collecting all words and their counts\n",
      "I1203 18:55:11.913845 139821617399616 word2vec.py:1382] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I1203 18:55:11.914303 139821617399616 word2vec.py:1405] collected 142 word types from a corpus of 421 raw words and 75 sentences\n",
      "I1203 18:55:11.914633 139821617399616 word2vec.py:1458] Loading a fresh vocabulary\n",
      "I1203 18:55:11.915103 139821617399616 word2vec.py:1480] effective_min_count=2 retains 64 unique words (45% of original 142, drops 78)\n",
      "I1203 18:55:11.915424 139821617399616 word2vec.py:1486] effective_min_count=2 leaves 343 word corpus (81% of original 421, drops 78)\n",
      "I1203 18:55:11.916112 139821617399616 word2vec.py:1547] deleting the raw counts dictionary of 142 items\n",
      "I1203 18:55:11.916453 139821617399616 word2vec.py:1550] sample=0.001 downsamples 64 most-common words\n",
      "I1203 18:55:11.916804 139821617399616 word2vec.py:1551] downsampling leaves estimated 100 word corpus (29.3% of prior 343)\n",
      "I1203 18:55:11.917254 139821617399616 base_any2vec.py:1006] estimated required memory for 64 words and 100 dimensions: 83200 bytes\n",
      "I1203 18:55:11.917576 139821617399616 word2vec.py:1699] resetting layer weights\n",
      "I1203 18:55:11.934875 139821617399616 base_any2vec.py:1192] training model with 3 workers on 64 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I1203 18:55:11.936277 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.936723 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.937057 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.937369 139821617399616 base_any2vec.py:1330] EPOCH - 1 : training on 421 raw words (104 effective words) took 0.0s, 77700 effective words/s\n",
      "I1203 18:55:11.939044 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.939496 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.939864 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.940192 139821617399616 base_any2vec.py:1330] EPOCH - 2 : training on 421 raw words (105 effective words) took 0.0s, 75960 effective words/s\n",
      "I1203 18:55:11.941666 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.942115 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.942446 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.942777 139821617399616 base_any2vec.py:1330] EPOCH - 3 : training on 421 raw words (91 effective words) took 0.0s, 68631 effective words/s\n",
      "I1203 18:55:11.944248 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.944700 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.945048 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.945371 139821617399616 base_any2vec.py:1330] EPOCH - 4 : training on 421 raw words (89 effective words) took 0.0s, 65938 effective words/s\n",
      "I1203 18:55:11.946841 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 2 more threads\n",
      "I1203 18:55:11.947263 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 1 more threads\n",
      "I1203 18:55:11.947595 139821617399616 base_any2vec.py:348] worker thread finished; awaiting finish of 0 more threads\n",
      "I1203 18:55:11.947932 139821617399616 base_any2vec.py:1330] EPOCH - 5 : training on 421 raw words (99 effective words) took 0.0s, 74589 effective words/s\n",
      "I1203 18:55:11.948263 139821617399616 base_any2vec.py:1366] training on a 2105 raw words (488 effective words) took 0.0s, 37488 effective words/s\n",
      "W1203 18:55:11.948574 139821617399616 base_any2vec.py:1371] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "Accuracy: 0.14285714285714285\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "2.268889443657152\n",
      "Accuracy: 0.6666666666666666\n",
      "MSE:  2.268889443657152\n"
     ]
    }
   ],
   "source": [
    "WEM_MSE = 0\n",
    "WEM_Accuracy = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "\n",
    "            all_words = [normalize_text(sent) for sent in corpus]\n",
    "            pos_tagged = []\n",
    "            for i in range(len(all_words)):\n",
    "                all_words[i] = [w for w in word_tokenize(str(all_words[i]))]\n",
    "                pos_tagged.append(pos_tag(all_words[i]))\n",
    "\n",
    "            word2vec,vocabulary = build_vocabulary(all_words)\n",
    "            WIMP = []\n",
    "            for sentence in all_words:\n",
    "                POS_IMP = build_pos_imp(vocabulary, sentence, pos_tagged)\n",
    "\n",
    "                BOW, WV, POSA = build_word_matrix(vocabulary, sentence, POS_IMP, word2vec)\n",
    "                A = BOW * WV * POSA\n",
    "                WIMP_TEMP=[]\n",
    "                for i in range(0,len(sentence)):\n",
    "                    WIMP_TEMP.append(np.mean(A[i]))\n",
    "                    WIMP.append(WIMP_TEMP)\n",
    "\n",
    "            WIMP = NormalizeData(WIMP)\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(WIMP[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        if compare(float(WIMP[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "                            WEM_MSE = WEM_MSE + pow((float(WIMP[i][j])-float(scores[i][j])),2)\n",
    "\n",
    "            accuracy = correct/total\n",
    "            WEM_Accuracy.append(accuracy)\n",
    "            print(\"Accuracy: \"+ str(accuracy))\n",
    "print(\"MSE: \", str(WEM_MSE))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-aware Term Weight Determination Using BERT\n",
    "\n",
    "http://www.cs.cmu.edu/~zhuyund/Zhuyun_Dai_Dissertation.pdf\n",
    "https://www.cs.cmu.edu/~callan/Papers/TheWebConf20-Zhuyun-Dai.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1203 23:48:16.296631 140271680702272 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/aa7510/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I1203 23:48:16.297617 140271680702272 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1203 23:48:16.390794 140271680702272 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/aa7510/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading pre-trained BERT Base\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,)\n",
    "\n",
    "#Use evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_Embedding(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Tokenize text\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     print(\"Length of tokenized text: \" + str(len(tokenized_text)))\n",
    "\n",
    "    # Map tokenized text to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # `output_hidden_states = True` so the 3rd element has the hidden states for each layer. \n",
    "        # More information at: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    # `token_embeddings` is a [32 x 12 x 768] tensor, now for each token of 32:\n",
    "\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "        # Sum the vectors from the last four layers to represent the token.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "    BERT_Word_Embedding = np.zeros(shape=(len(token_vecs_sum),len(token_vecs_sum[0])))\n",
    "    for i in range(0, len(token_vecs_sum)):\n",
    "        BERT_Word_Embedding[i] = token_vecs_sum[i].numpy()\n",
    "#     print(BERT_Word_Embedding.shape)\n",
    "    return BERT_Word_Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4602446483180428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-79754f174c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcontextual_wimp_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0msentence_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mWIMP_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-26cb6fb8cd7e>\u001b[0m in \u001b[0;36mBERT_Embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# `output_hidden_states = True` so the 3rd element has the hidden states for each layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         )\n\u001b[0;32m--> 785\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             )\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    153\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.autofs/tools/spack/var/spack/environments/engl-581-2/.spack-env/view/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \"\"\"\n\u001b[0;32m-> 1955\u001b[0;31m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0m\u001b[1;32m   1956\u001b[0m                             torch.backends.cudnn.enabled)\n\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BERT_Accuracy = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "\n",
    "            all_words = [str(normalize_text(sent)) for sent in corpus]\n",
    "            \n",
    "    \n",
    "            contextual_wimp_data=[]\n",
    "            for text in all_words:\n",
    "                sentence_mat = BERT_Embedding(text)\n",
    "                WIMP_temp = []\n",
    "                \n",
    "                for word_emb in sentence_mat[1:-1]:\n",
    "                    \n",
    "                    word_emb = np.reshape(word_emb,(word_emb.size,1))\n",
    "\n",
    "                    W = np.ones(shape=(len(word_emb),1))\n",
    "\n",
    "                    X = np.dot(W.T,word_emb)\n",
    "                    WIMP_temp.append(X[0][0])\n",
    "                    \n",
    "                    \n",
    "                contextual_wimp_data.append(WIMP_temp)\n",
    "\n",
    "            contextual_wimp_data = NormalizeData(contextual_wimp_data)\n",
    "\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            total = 0 \n",
    "            correct = 0\n",
    "\n",
    "            for i in range(0,len(scores)):\n",
    "\n",
    "                if len(contextual_wimp_data[i]) == len(scores[i]):\n",
    "                    total = total + len(scores[i])\n",
    "                    for j in range(0,len(scores[i])):\n",
    "                        if compare(float(contextual_wimp_data[i][j]), float(scores[i][j])):\n",
    "                            correct = correct +1\n",
    "\n",
    "            accuracy = correct/total\n",
    "            BERT_Accuracy.append(accuracy)\n",
    "            print(\"Accuracy: \"+ str(accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BERT on this dataset using the above paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "41 40\n",
      "36 36\n",
      "16 16\n",
      "47 47\n",
      "33 29\n",
      "17 17\n",
      "37 37\n",
      "30 30\n",
      "46 46\n",
      "29 29\n",
      "43 43\n",
      "51 51\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "25 25\n",
      "1 1\n",
      "32 31\n",
      "15 15\n",
      "16 16\n",
      "51 48\n",
      "2 2\n",
      "21 21\n",
      "24 23\n",
      "33 33\n",
      "25 22\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "20 20\n",
      "20 20\n",
      "36 34\n",
      "33 30\n",
      "1 1\n",
      "1 1\n",
      "12 12\n",
      "37 37\n",
      "46 30\n",
      "1 1\n",
      "34 32\n",
      "22 21\n",
      "27 27\n",
      "35 34\n",
      "44 43\n",
      "11 10\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "6 6\n",
      "1 1\n",
      "7 5\n",
      "1 1\n",
      "1 1\n",
      "15 15\n",
      "25 24\n",
      "18 17\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "3 1\n",
      "19 17\n",
      "1 1\n",
      "3 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "7 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "4 4\n",
      "1 1\n",
      "22 17\n",
      "6 6\n",
      "46 42\n",
      "36 35\n",
      "21 19\n",
      "30 30\n",
      "24 23\n",
      "34 33\n",
      "17 16\n",
      "12 12\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "15 15\n",
      "21 21\n",
      "8 7\n",
      "33 30\n",
      "1 1\n",
      "7 7\n",
      "19 17\n",
      "14 14\n",
      "1 1\n",
      "1 1\n",
      "25 24\n",
      "25 22\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "14 13\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 17\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "14 13\n",
      "1 1\n",
      "28 27\n",
      "10 10\n",
      "12 9\n",
      "1 1\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "24 23\n",
      "1 1\n",
      "10 9\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 5\n",
      "40 36\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "31 29\n",
      "11 11\n",
      "11 11\n",
      "17 17\n",
      "44 43\n",
      "29 26\n",
      "18 18\n",
      "24 24\n",
      "35 34\n",
      "1 1\n",
      "13 13\n",
      "1 1\n",
      "46 45\n",
      "42 41\n",
      "1 1\n",
      "21 21\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "21 20\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "23 21\n",
      "1 1\n",
      "13 13\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "15 15\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "25 25\n",
      "1 1\n",
      "2 2\n",
      "14 13\n",
      "1 1\n",
      "17 17\n",
      "24 23\n",
      "8 8\n",
      "40 37\n",
      "8 8\n",
      "19 19\n",
      "4 4\n",
      "10 9\n",
      "2 2\n",
      "8 8\n",
      "8 8\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "12 12\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "6 5\n",
      "35 34\n",
      "18 16\n",
      "8 8\n",
      "1 1\n",
      "51 51\n",
      "5 4\n",
      "1 1\n",
      "12 12\n",
      "7 6\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "31 28\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "35 31\n",
      "10 10\n",
      "5 5\n",
      "26 22\n",
      "1 1\n",
      "1 1\n",
      "15 14\n",
      "5 4\n",
      "6 6\n",
      "39 37\n",
      "1 1\n",
      "32 32\n",
      "13 12\n",
      "2 2\n",
      "1 1\n",
      "9 9\n",
      "53 50\n",
      "1 1\n",
      "34 33\n",
      "5 5\n",
      "28 25\n",
      "8 7\n",
      "5 5\n",
      "8 7\n",
      "26 25\n",
      "1 1\n",
      "21 19\n",
      "23 23\n",
      "6 5\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "36 36\n",
      "12 12\n",
      "13 13\n",
      "13 13\n",
      "26 26\n",
      "16 15\n",
      "1 1\n",
      "15 14\n",
      "22 21\n",
      "14 13\n",
      "16 16\n",
      "1 1\n",
      "34 33\n",
      "12 11\n",
      "26 22\n",
      "1 1\n",
      "4 4\n",
      "49 44\n",
      "32 32\n",
      "14 13\n",
      "26 22\n",
      "1 1\n",
      "18 18\n",
      "12 11\n",
      "1 1\n",
      "6 5\n",
      "15 15\n",
      "37 37\n",
      "1 1\n",
      "10 9\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "18 16\n",
      "30 25\n",
      "14 14\n",
      "1 1\n",
      "11 10\n",
      "1 1\n",
      "1 1\n",
      "44 41\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "42 38\n",
      "19 17\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "36 34\n",
      "1 1\n",
      "35 35\n",
      "14 14\n",
      "1 1\n",
      "38 38\n",
      "19 17\n",
      "21 21\n",
      "1 1\n",
      "4 1\n",
      "2 2\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "9 9\n",
      "14 14\n",
      "29 29\n",
      "17 16\n",
      "1 1\n",
      "22 20\n",
      "4 4\n",
      "1 1\n",
      "12 12\n",
      "17 17\n",
      "26 24\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "13 12\n",
      "2 2\n",
      "12 9\n",
      "1 1\n",
      "34 33\n",
      "1 1\n",
      "7 6\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "30 30\n",
      "1 1\n",
      "40 40\n",
      "29 28\n",
      "1 1\n",
      "12 11\n",
      "20 19\n",
      "33 31\n",
      "33 26\n",
      "8 8\n",
      "8 8\n",
      "17 14\n",
      "1 1\n",
      "1 1\n",
      "23 21\n",
      "50 47\n",
      "1 1\n",
      "9 6\n",
      "1 1\n",
      "13 12\n",
      "3 3\n",
      "25 23\n",
      "22 22\n",
      "5 5\n",
      "26 25\n",
      "16 14\n",
      "18 17\n",
      "2 2\n",
      "1 1\n",
      "26 24\n",
      "15 15\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "21 21\n",
      "25 25\n",
      "24 22\n",
      "47 43\n",
      "5 5\n",
      "1 1\n",
      "14 14\n",
      "1 1\n",
      "1 1\n",
      "14 14\n",
      "25 24\n",
      "28 26\n",
      "17 14\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "19 18\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "34 33\n",
      "28 28\n",
      "29 27\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "33 32\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "27 26\n",
      "8 8\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "20 17\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "22 22\n",
      "28 28\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "43 41\n",
      "13 11\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "10 10\n",
      "25 24\n",
      "44 44\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "23 23\n",
      "37 36\n",
      "14 12\n",
      "10 10\n",
      "14 14\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "33 33\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "36 35\n",
      "22 19\n",
      "35 33\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "15 12\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "3 2\n",
      "10 10\n",
      "7 7\n",
      "5 5\n",
      "3 3\n",
      "16 15\n",
      "1 1\n",
      "9 9\n",
      "9 6\n",
      "2 2\n",
      "39 38\n",
      "16 15\n",
      "39 36\n",
      "39 34\n",
      "8 7\n",
      "30 28\n",
      "25 21\n",
      "1 1\n",
      "1 1\n",
      "16 16\n",
      "1 1\n",
      "17 16\n",
      "1 1\n",
      "39 35\n",
      "1 1\n",
      "21 17\n",
      "37 34\n",
      "3 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "35 35\n",
      "11 9\n",
      "37 36\n",
      "13 13\n",
      "8 8\n",
      "1 1\n",
      "31 29\n",
      "1 1\n",
      "31 29\n",
      "18 18\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 2\n",
      "9 9\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "25 23\n",
      "7 7\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "20 19\n",
      "25 23\n",
      "42 40\n",
      "6 5\n",
      "13 13\n",
      "12 12\n",
      "19 19\n",
      "5 5\n",
      "2 2\n",
      "3 3\n",
      "17 17\n",
      "33 32\n",
      "1 1\n",
      "6 6\n",
      "24 23\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "10 10\n",
      "10 5\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "4 3\n",
      "9 8\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "2 2\n",
      "19 19\n",
      "15 15\n",
      "6 5\n",
      "1 1\n",
      "19 19\n",
      "8 8\n",
      "7 7\n",
      "21 21\n",
      "23 20\n",
      "1 1\n",
      "20 19\n",
      "16 16\n",
      "1 1\n",
      "13 13\n",
      "12 10\n",
      "12 12\n",
      "5 5\n",
      "14 14\n",
      "1 1\n",
      "33 31\n",
      "6 6\n",
      "18 17\n",
      "7 7\n",
      "32 30\n",
      "2 2\n",
      "8 8\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "23 23\n",
      "28 27\n",
      "7 7\n",
      "41 41\n",
      "23 23\n",
      "8 8\n",
      "7 7\n",
      "15 14\n",
      "16 16\n",
      "1 1\n",
      "21 21\n",
      "3 3\n",
      "8 5\n",
      "7 7\n",
      "21 20\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "14 10\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "16 15\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "17 17\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "2 2\n",
      "37 34\n",
      "27 27\n",
      "47 46\n",
      "36 36\n",
      "48 44\n",
      "24 19\n",
      "24 22\n",
      "52 44\n",
      "4 4\n",
      "33 33\n",
      "8 8\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "41 35\n",
      "18 16\n",
      "9 9\n",
      "41 38\n",
      "39 38\n",
      "34 34\n",
      "33 31\n",
      "16 14\n",
      "20 19\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "32 27\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "16 15\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "41 39\n",
      "16 15\n",
      "11 9\n",
      "7 5\n",
      "9 9\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "23 22\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "27 27\n",
      "36 36\n",
      "18 18\n",
      "43 41\n",
      "9 8\n",
      "11 10\n",
      "16 16\n",
      "10 10\n",
      "15 15\n",
      "29 26\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "30 28\n",
      "17 17\n",
      "34 33\n",
      "27 24\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "38 37\n",
      "31 31\n",
      "18 18\n",
      "21 21\n",
      "32 31\n",
      "1 1\n",
      "19 18\n",
      "42 39\n",
      "1 1\n",
      "1 1\n",
      "39 38\n",
      "1 1\n",
      "6 6\n",
      "26 24\n",
      "8 7\n",
      "4 4\n",
      "11 10\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "37 34\n",
      "33 31\n",
      "43 41\n",
      "26 25\n",
      "2 2\n",
      "9 9\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "12 11\n",
      "23 22\n",
      "1 1\n",
      "27 27\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "30 29\n",
      "35 34\n",
      "1 1\n",
      "26 24\n",
      "11 11\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 16\n",
      "7 7\n",
      "1 1\n",
      "18 18\n",
      "23 23\n",
      "23 22\n",
      "7 6\n",
      "15 15\n",
      "20 19\n",
      "11 11\n",
      "4 4\n",
      "1 1\n",
      "17 16\n",
      "8 6\n",
      "24 23\n",
      "23 20\n",
      "1 1\n",
      "1 1\n",
      "13 13\n",
      "39 38\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "7 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "22 21\n",
      "24 23\n",
      "1 1\n",
      "18 16\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "14 14\n",
      "8 8\n",
      "9 9\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "32 29\n",
      "26 23\n",
      "6 6\n",
      "47 38\n",
      "13 13\n",
      "16 16\n",
      "17 17\n",
      "30 28\n",
      "19 13\n",
      "16 16\n",
      "34 33\n",
      "56 54\n",
      "12 12\n",
      "27 26\n",
      "2 2\n",
      "35 34\n",
      "1 1\n",
      "13 13\n",
      "34 34\n",
      "11 11\n",
      "52 50\n",
      "8 8\n",
      "3 3\n",
      "41 41\n",
      "29 23\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "14 10\n",
      "1 1\n",
      "11 7\n",
      "1 1\n",
      "16 14\n",
      "28 22\n",
      "17 15\n",
      "1 1\n",
      "17 14\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "39 38\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "4 3\n",
      "53 51\n",
      "27 27\n",
      "12 12\n",
      "1 1\n",
      "12 9\n",
      "3 3\n",
      "1 1\n",
      "45 41\n",
      "12 12\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "7 6\n",
      "53 53\n",
      "30 29\n",
      "25 24\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "36 34\n",
      "30 29\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "13 11\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "25 25\n",
      "28 27\n",
      "1 1\n",
      "4 2\n",
      "3 3\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "6 5\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "22 21\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "1 1\n",
      "7 5\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "12 11\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "13 11\n",
      "21 21\n",
      "19 16\n",
      "22 21\n",
      "1 1\n",
      "9 9\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 15\n",
      "1 1\n",
      "10 7\n",
      "6 6\n",
      "1 1\n",
      "21 19\n",
      "38 38\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "18 14\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "3 1\n",
      "21 18\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 13\n",
      "1 1\n",
      "6 2\n",
      "1 1\n",
      "7 3\n",
      "1 1\n",
      "24 24\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "33 30\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "27 26\n",
      "3 3\n",
      "1 1\n",
      "5 5\n",
      "24 23\n",
      "29 28\n",
      "12 11\n",
      "1 1\n",
      "20 20\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "11 10\n",
      "3 3\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "24 23\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "14 12\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "11 11\n",
      "13 13\n",
      "52 52\n",
      "15 13\n",
      "33 25\n",
      "1 1\n",
      "1 1\n",
      "27 25\n",
      "5 4\n",
      "31 30\n",
      "27 27\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "17 16\n",
      "1 1\n",
      "11 9\n",
      "14 13\n",
      "1 1\n",
      "14 14\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "7 7\n",
      "27 25\n",
      "9 7\n",
      "1 1\n",
      "14 14\n",
      "1 1\n",
      "19 19\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "13 13\n",
      "12 12\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "8 8\n",
      "10 6\n",
      "26 22\n",
      "1 1\n",
      "25 22\n",
      "1 1\n",
      "5 5\n",
      "3 3\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "15 15\n",
      "1 1\n",
      "14 14\n",
      "1 1\n",
      "20 16\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "49 40\n",
      "28 25\n",
      "23 21\n",
      "32 28\n",
      "39 36\n",
      "2 2\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "9 9\n",
      "3 1\n",
      "3 1\n",
      "7 7\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "26 26\n",
      "29 29\n",
      "1 1\n",
      "4 1\n",
      "1 1\n",
      "12 11\n",
      "1 1\n",
      "12 12\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "18 18\n",
      "1 1\n",
      "46 43\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "10 10\n",
      "5 5\n",
      "10 10\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "12 12\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 17\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "8 8\n",
      "18 17\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "14 14\n",
      "11 11\n",
      "27 25\n",
      "28 27\n",
      "2 2\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "39 34\n",
      "25 25\n",
      "36 32\n",
      "35 35\n",
      "37 37\n",
      "34 32\n",
      "1 1\n",
      "28 25\n",
      "14 13\n",
      "34 31\n",
      "37 37\n",
      "44 43\n",
      "1 1\n",
      "1 1\n",
      "28 27\n",
      "28 28\n",
      "38 35\n",
      "1 1\n",
      "3 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "11 10\n",
      "45 43\n",
      "7 7\n",
      "1 1\n",
      "19 19\n",
      "22 20\n",
      "24 24\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "7 6\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "5 2\n",
      "1 1\n",
      "17 16\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "10 9\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 15\n",
      "14 14\n",
      "16 15\n",
      "54 47\n",
      "34 34\n",
      "2 2\n",
      "1 1\n",
      "25 23\n",
      "8 4\n",
      "21 21\n",
      "1 1\n",
      "18 14\n",
      "16 16\n",
      "22 22\n",
      "19 18\n",
      "28 28\n",
      "1 1\n",
      "24 21\n",
      "44 44\n",
      "36 34\n",
      "1 1\n",
      "1 1\n",
      "11 8\n",
      "1 1\n",
      "25 23\n",
      "35 34\n",
      "41 40\n",
      "5 5\n",
      "1 1\n",
      "26 25\n",
      "13 13\n",
      "1 1\n",
      "23 20\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "10 10\n",
      "24 16\n",
      "1 1\n",
      "1 1\n",
      "21 17\n",
      "1 1\n",
      "36 36\n",
      "1 1\n",
      "40 40\n",
      "14 11\n",
      "32 31\n",
      "1 1\n",
      "23 23\n",
      "17 16\n",
      "30 30\n",
      "48 46\n",
      "38 35\n",
      "14 11\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "34 27\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "9 9\n",
      "1 1\n",
      "35 35\n",
      "1 1\n",
      "4 1\n",
      "1 1\n",
      "20 17\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "18 18\n",
      "18 16\n",
      "26 24\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "9 9\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 12\n",
      "1 1\n",
      "6 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "44 42\n",
      "43 42\n",
      "41 41\n",
      "13 11\n",
      "23 23\n",
      "13 13\n",
      "1 1\n",
      "37 34\n",
      "38 33\n",
      "1 1\n",
      "31 29\n",
      "1 1\n",
      "44 43\n",
      "30 29\n",
      "27 27\n",
      "1 1\n",
      "37 34\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "12 11\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "35 35\n",
      "22 22\n",
      "38 37\n",
      "1 1\n",
      "1 1\n",
      "34 33\n",
      "12 12\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "33 31\n",
      "2 2\n",
      "31 19\n",
      "1 1\n",
      "44 43\n",
      "31 31\n",
      "7 7\n",
      "9 9\n",
      "18 18\n",
      "36 34\n",
      "33 33\n",
      "26 25\n",
      "9 9\n",
      "8 8\n",
      "26 26\n",
      "16 16\n",
      "41 41\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "9 6\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "2 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "9 9\n",
      "18 18\n",
      "29 26\n",
      "17 17\n",
      "27 27\n",
      "29 28\n",
      "42 42\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "2 1\n",
      "3 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "5 5\n",
      "1 1\n",
      "2 2\n",
      "5 3\n",
      "37 37\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "5 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "30 21\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "15 14\n",
      "4 4\n",
      "1 1\n",
      "12 10\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "14 12\n",
      "43 42\n",
      "16 16\n",
      "23 23\n",
      "24 24\n",
      "46 45\n",
      "1 1\n",
      "31 31\n",
      "10 8\n",
      "6 6\n",
      "27 25\n",
      "15 15\n",
      "10 10\n",
      "1 1\n",
      "1 1\n",
      "8 8\n",
      "8 7\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "31 30\n",
      "31 30\n",
      "26 25\n",
      "29 28\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "21 21\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "14 14\n",
      "2 1\n",
      "1 1\n",
      "7 6\n",
      "11 11\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "35 34\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "2 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "4 1\n",
      "1 1\n",
      "28 25\n",
      "18 18\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "4 4\n",
      "13 12\n",
      "5 5\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "13 13\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "6 5\n",
      "8 8\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "25 22\n",
      "17 16\n",
      "11 11\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "24 24\n",
      "1 1\n",
      "7 6\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "15 15\n",
      "3 3\n",
      "36 36\n",
      "34 34\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "2 2\n",
      "6 6\n",
      "6 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "9 8\n",
      "40 38\n",
      "13 12\n",
      "20 19\n",
      "1 1\n",
      "2 2\n",
      "34 33\n",
      "11 10\n",
      "4 3\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "25 24\n",
      "10 8\n",
      "1 1\n",
      "3 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "52 48\n",
      "16 16\n",
      "1 1\n",
      "13 13\n",
      "15 15\n",
      "39 32\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "30 29\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 17\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "35 33\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "12 10\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "22 21\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "18 15\n",
      "1 1\n",
      "30 28\n",
      "18 18\n",
      "28 28\n",
      "1 1\n",
      "27 25\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "12 12\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "10 9\n",
      "1 1\n",
      "5 3\n",
      "1 1\n",
      "3 3\n",
      "15 13\n",
      "7 7\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "3 2\n",
      "18 18\n",
      "10 10\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "6 5\n",
      "1 1\n",
      "1 1\n",
      "28 23\n",
      "32 30\n",
      "1 1\n",
      "18 17\n",
      "1 1\n",
      "12 12\n",
      "16 16\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "23 23\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "20 17\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "10 9\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "10 8\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "37 35\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "9 9\n",
      "11 11\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "10 10\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "39 35\n",
      "10 10\n",
      "18 17\n",
      "34 17\n",
      "31 27\n",
      "43 41\n",
      "24 24\n",
      "25 24\n",
      "12 12\n",
      "1 1\n",
      "7 7\n",
      "15 13\n",
      "6 6\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "6 2\n",
      "1 1\n",
      "28 28\n",
      "1 1\n",
      "26 20\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 3\n",
      "13 13\n",
      "19 19\n",
      "19 18\n",
      "1 1\n",
      "40 37\n",
      "5 5\n",
      "1 1\n",
      "41 41\n",
      "24 24\n",
      "27 25\n",
      "23 22\n",
      "46 44\n",
      "28 28\n",
      "25 24\n",
      "25 24\n",
      "6 6\n",
      "10 9\n",
      "1 1\n",
      "13 13\n",
      "4 4\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "17 17\n",
      "42 39\n",
      "20 20\n",
      "15 15\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "41 22\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "2 2\n",
      "18 13\n",
      "26 24\n",
      "9 8\n",
      "40 40\n",
      "24 24\n",
      "52 45\n",
      "10 10\n",
      "2 2\n",
      "19 19\n",
      "54 52\n",
      "5 5\n",
      "44 42\n",
      "3 2\n",
      "1 1\n",
      "7 6\n",
      "2 2\n",
      "15 14\n",
      "25 24\n",
      "28 23\n",
      "24 21\n",
      "7 5\n",
      "2 2\n",
      "1 1\n",
      "13 12\n",
      "14 13\n",
      "13 12\n",
      "4 4\n",
      "6 6\n",
      "1 1\n",
      "35 27\n",
      "15 15\n",
      "28 28\n",
      "15 14\n",
      "8 8\n",
      "1 1\n",
      "17 17\n",
      "30 30\n",
      "18 18\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 15\n",
      "5 5\n",
      "1 1\n",
      "20 18\n",
      "40 40\n",
      "31 29\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 12\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "37 34\n",
      "12 12\n",
      "45 42\n",
      "16 15\n",
      "15 14\n",
      "1 1\n",
      "15 15\n",
      "19 19\n",
      "39 39\n",
      "11 11\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "6 6\n",
      "15 14\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "1 1\n",
      "10 9\n",
      "18 18\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "4 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "5 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "12 10\n",
      "1 1\n",
      "5 3\n",
      "1 1\n",
      "1 1\n",
      "21 20\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "38 37\n",
      "9 8\n",
      "1 1\n",
      "33 28\n",
      "20 19\n",
      "1 1\n",
      "8 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "17 16\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "30 30\n",
      "39 38\n",
      "7 7\n",
      "11 11\n",
      "1 1\n",
      "18 15\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "2 2\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "15 15\n",
      "1 1\n",
      "14 14\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "7 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 2\n",
      "5 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "14 14\n",
      "1 1\n",
      "7 7\n",
      "11 11\n",
      "1 1\n",
      "5 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "7 7\n",
      "6 6\n",
      "2 2\n",
      "5 5\n",
      "12 11\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "2 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "1 1\n",
      "6 6\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "4 2\n",
      "4 4\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "16 14\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "15 14\n",
      "6 6\n",
      "9 9\n",
      "1 1\n",
      "5 5\n",
      "7 5\n",
      "1 1\n",
      "21 20\n",
      "10 9\n",
      "3 3\n",
      "22 20\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "8 8\n",
      "13 13\n",
      "7 7\n",
      "4 4\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "53 47\n",
      "1 1\n",
      "27 25\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "28 21\n",
      "8 7\n",
      "1 1\n",
      "8 6\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 5\n",
      "4 4\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "21 21\n",
      "42 40\n",
      "14 14\n",
      "1 1\n",
      "31 31\n",
      "14 13\n",
      "38 36\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "39 37\n",
      "38 37\n",
      "36 35\n",
      "29 29\n",
      "37 34\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "34 32\n",
      "25 24\n",
      "35 33\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "16 16\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "22 21\n",
      "26 24\n",
      "47 44\n",
      "25 24\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "13 13\n",
      "12 12\n",
      "15 14\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "30 27\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "22 22\n",
      "1 1\n",
      "22 20\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "8 7\n",
      "4 3\n",
      "1 1\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "6 6\n",
      "4 2\n",
      "9 7\n",
      "25 24\n",
      "1 1\n",
      "6 5\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "39 35\n",
      "1 1\n",
      "1 1\n",
      "21 20\n",
      "21 19\n",
      "48 46\n",
      "17 16\n",
      "44 44\n",
      "6 2\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "13 13\n",
      "1 1\n",
      "20 18\n",
      "31 30\n",
      "22 20\n",
      "8 5\n",
      "1 1\n",
      "5 5\n",
      "44 44\n",
      "1 1\n",
      "20 20\n",
      "37 32\n",
      "19 18\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "19 19\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "14 13\n",
      "1 1\n",
      "18 16\n",
      "18 18\n",
      "20 20\n",
      "22 20\n",
      "18 17\n",
      "4 4\n",
      "19 17\n",
      "5 5\n",
      "1 1\n",
      "6 5\n",
      "12 11\n",
      "1 1\n",
      "3 3\n",
      "21 21\n",
      "2 2\n",
      "11 11\n",
      "25 21\n",
      "1 1\n",
      "1 1\n",
      "19 19\n",
      "17 12\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "8 8\n",
      "23 12\n",
      "4 4\n",
      "1 1\n",
      "41 38\n",
      "1 1\n",
      "16 16\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "3 3\n",
      "41 41\n",
      "37 37\n",
      "30 30\n",
      "15 15\n",
      "49 45\n",
      "35 35\n",
      "27 24\n",
      "46 28\n",
      "40 39\n",
      "22 22\n",
      "35 34\n",
      "22 21\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "52 51\n",
      "32 19\n",
      "32 31\n",
      "37 36\n",
      "1 1\n",
      "1 1\n",
      "14 14\n",
      "24 24\n",
      "2 2\n",
      "10 10\n",
      "49 47\n",
      "1 1\n",
      "26 26\n",
      "22 21\n",
      "24 24\n",
      "17 17\n",
      "7 6\n",
      "17 16\n",
      "16 15\n",
      "1 1\n",
      "10 9\n",
      "1 1\n",
      "3 1\n",
      "4 3\n",
      "19 18\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "26 25\n",
      "13 13\n",
      "22 21\n",
      "9 8\n",
      "7 4\n",
      "1 1\n",
      "1 1\n",
      "45 42\n",
      "19 18\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "6 6\n",
      "32 32\n",
      "12 12\n",
      "26 22\n",
      "40 40\n",
      "30 30\n",
      "21 21\n",
      "35 32\n",
      "6 6\n",
      "9 9\n",
      "1 1\n",
      "1 1\n",
      "7 5\n",
      "32 32\n",
      "1 1\n",
      "2 1\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "17 17\n",
      "4 3\n",
      "1 1\n",
      "28 22\n",
      "46 44\n",
      "34 31\n",
      "3 3\n",
      "1 1\n",
      "39 38\n",
      "5 5\n",
      "38 38\n",
      "4 4\n",
      "1 1\n",
      "33 33\n",
      "23 21\n",
      "1 1\n",
      "10 10\n",
      "42 39\n",
      "24 24\n",
      "14 13\n",
      "1 1\n",
      "10 10\n",
      "35 30\n",
      "16 11\n",
      "1 1\n",
      "26 25\n",
      "1 1\n",
      "14 12\n",
      "1 1\n",
      "29 28\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "1 1\n",
      "36 36\n",
      "23 20\n",
      "1 1\n",
      "22 16\n",
      "2 2\n",
      "27 25\n",
      "1 1\n",
      "47 46\n",
      "19 19\n",
      "35 34\n",
      "1 1\n",
      "17 17\n",
      "15 15\n",
      "5 5\n",
      "13 10\n",
      "12 12\n",
      "27 26\n",
      "11 11\n",
      "5 4\n",
      "2 1\n",
      "57 55\n",
      "7 7\n",
      "31 28\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "31 29\n",
      "34 30\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "33 29\n",
      "3 3\n",
      "8 7\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "3 1\n",
      "3 1\n",
      "28 28\n",
      "12 12\n",
      "2 2\n",
      "34 33\n",
      "2 2\n",
      "2 2\n",
      "7 7\n",
      "4 3\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "19 18\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "8 7\n",
      "14 13\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 2\n",
      "9 6\n",
      "10 10\n",
      "7 7\n",
      "15 15\n",
      "17 16\n",
      "9 9\n",
      "12 12\n",
      "23 23\n",
      "26 26\n",
      "2 2\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "4 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "2 2\n",
      "5 5\n",
      "1 1\n",
      "1 1\n",
      "10 10\n",
      "1 1\n",
      "33 33\n",
      "1 1\n",
      "14 12\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "6 4\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 1\n",
      "15 15\n",
      "20 19\n",
      "2 2\n",
      "6 5\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "10 10\n",
      "5 5\n",
      "6 6\n",
      "3 3\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "11 10\n",
      "12 12\n",
      "1 1\n",
      "12 10\n",
      "6 6\n",
      "1 1\n",
      "24 23\n",
      "18 18\n",
      "1 1\n",
      "35 33\n",
      "1 1\n",
      "13 12\n",
      "1 1\n",
      "24 24\n",
      "28 27\n",
      "3 2\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "1 1\n",
      "16 15\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "26 25\n",
      "1 1\n",
      "19 19\n",
      "23 23\n",
      "21 20\n",
      "1 1\n",
      "5 4\n",
      "20 18\n",
      "1 1\n",
      "15 13\n",
      "4 4\n",
      "1 1\n",
      "8 8\n",
      "19 18\n",
      "10 9\n",
      "1 1\n",
      "8 5\n",
      "6 6\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "27 26\n",
      "17 16\n",
      "17 17\n",
      "19 19\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "5 4\n",
      "1 1\n",
      "18 13\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "16 15\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "30 28\n",
      "29 29\n",
      "31 29\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "12 12\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "25 25\n",
      "13 11\n",
      "1 1\n",
      "8 8\n",
      "38 37\n",
      "15 13\n",
      "14 14\n",
      "1 1\n",
      "16 16\n",
      "29 27\n",
      "37 30\n",
      "23 21\n",
      "29 25\n",
      "40 36\n",
      "19 19\n",
      "33 32\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "10 8\n",
      "1 1\n",
      "4 4\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "4 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "10 9\n",
      "20 19\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "15 15\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "19 19\n",
      "37 37\n",
      "53 48\n",
      "26 26\n",
      "1 1\n",
      "14 13\n",
      "2 2\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "12 11\n",
      "1 1\n",
      "18 18\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "11 11\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "9 8\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "33 32\n",
      "16 16\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "3 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "40 36\n",
      "15 11\n",
      "1 1\n",
      "1 1\n",
      "7 6\n",
      "36 33\n",
      "36 33\n",
      "9 8\n",
      "1 1\n",
      "3 3\n",
      "25 22\n",
      "1 1\n",
      "23 22\n",
      "24 23\n",
      "1 1\n",
      "46 42\n",
      "1 1\n",
      "32 32\n",
      "1 1\n",
      "16 11\n",
      "1 1\n",
      "4 4\n",
      "3 3\n",
      "4 3\n",
      "32 30\n",
      "23 21\n",
      "29 28\n",
      "18 18\n",
      "15 14\n",
      "26 25\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "25 23\n",
      "18 18\n",
      "4 4\n",
      "21 20\n",
      "1 1\n",
      "4 4\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "18 14\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "9 8\n",
      "2 2\n",
      "1 1\n",
      "21 19\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "28 28\n",
      "1 1\n",
      "18 16\n",
      "4 4\n",
      "1 1\n",
      "41 38\n",
      "1 1\n",
      "2 2\n",
      "16 16\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "54 54\n",
      "1 1\n",
      "3 3\n",
      "24 23\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "36 30\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "6 6\n",
      "31 30\n",
      "1 1\n",
      "2 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "2 2\n",
      "1 1\n",
      "1 1\n",
      "[[2.048173189163208, 0.3661077618598938, 2.532742500305176, -0.8462165594100952, 0.5551263689994812, -1.235276460647583, -1.583504319190979, 0.3300264775753021, -0.10033093392848969, -0.49529772996902466, 2.8560354709625244, 1.127993106842041, 0.3396507799625397, 3.9331860542297363, 0.8479900360107422, 2.3331234455108643, -0.6368998289108276, 1.3886442184448242, 1.2890006303787231, 3.6744353771209717, 3.1184005737304688, 1.6592237949371338, 0.11602438986301422, 2.233837842941284, 4.664031028747559, -4.283971786499023, -0.157917320728302, -0.6289287805557251, -2.6595776081085205, 0.041053831577301025, 1.4851539134979248, -2.2770888805389404, -1.9595820903778076, -1.830431342124939, 0.029170215129852295, -2.245759963989258, 0.4516276717185974, 1.5542404651641846, -1.203356385231018, 2.833281993865967, 3.5573434829711914, 1.7012180089950562, 0.833052396774292, -1.837195634841919, 0.2825664281845093, 0.7987513542175293, -0.9390964508056641, -5.159853458404541, 3.3178207874298096, -5.873495101928711, -2.7817025184631348, -0.5001527070999146, -4.969981670379639, -1.0034055709838867, 0.6519896388053894, 0.13121217489242554, -2.858968496322632, -4.464591979980469, -3.1432156562805176, 1.2214126586914062, -1.5675601959228516, 0.46176087856292725, 1.0667299032211304, -1.6001698970794678, 0.04735215753316879, -1.881493330001831, 3.0388948917388916, -0.7802250385284424, -1.5783164501190186, 4.546161651611328, 2.314600944519043, -1.8340139389038086, 0.3977503180503845, 2.5998663902282715, 1.7404382228851318, 0.6826582551002502, 1.8339669704437256, 1.0128612518310547, -2.6273303031921387, 0.8838911056518555, 0.7696446180343628, 2.537731647491455, -0.8664147853851318, 1.8392744064331055, 3.0798165798187256, 1.5879257917404175, -0.8986930847167969, 2.5091042518615723, -1.268114686012268, 1.051720380783081, 1.5682696104049683, -1.9086828231811523, -2.958205461502075, -3.585392951965332, 1.2548109292984009, 1.0284031629562378, 0.1324324905872345, 0.8350359797477722, 2.3673465251922607, -0.23924793303012848, 2.665839672088623, -7.8602495193481445, 0.3149070143699646, -3.977442741394043, 0.11495891213417053, 2.1019744873046875, 2.760554552078247, -1.9727503061294556, 2.655424118041992, -1.183582067489624, 0.9029048681259155, 1.165734887123108, 0.039512693881988525, -0.38189616799354553, -0.7039029002189636, 3.017096519470215, 1.3441396951675415, 0.7032384276390076, 0.953357994556427, 2.0746231079101562, -4.329214096069336, -0.1361490935087204, -0.8863527178764343, 1.247753381729126, 0.9937788248062134, 3.816326141357422, 0.5905301570892334, -1.785927176475525, -2.662972927093506, 0.5956545472145081, 0.5407944321632385, 0.45684510469436646, 0.8346709609031677, -0.6533198952674866, -4.28318452835083, -1.8790204524993896, 0.3108554482460022, -3.3407468795776367, -1.9813960790634155, -0.3911261260509491, 1.2168755531311035, -1.3492341041564941, 1.170891284942627, -0.6219577789306641, 2.1789755821228027, 0.8486196994781494, -2.359339714050293, 0.35380351543426514, 0.00398477166891098, 4.417462348937988, -2.4593167304992676, 0.5240586400032043, -1.0324525833129883, -3.6227869987487793, -2.9390039443969727, 1.2218040227890015, -5.5981035232543945, -1.5641090869903564, -1.0311018228530884, 1.9350378513336182, 4.035221099853516, 1.5182013511657715, -0.770187497138977, 1.397770643234253, 1.1873377561569214, 4.735429286956787, -5.135782718658447, 0.9275381565093994, 0.2202465832233429, 0.9152030944824219, 3.513439178466797, 0.5932562351226807, 0.416437029838562, -1.0527170896530151, 2.7139620780944824, 0.5772057771682739, 1.924742579460144, 1.0281403064727783, -1.8479335308074951, 1.045836091041565, -1.258720874786377, -0.05473452806472778, -0.9070267677307129, 1.9838707447052002, 0.4707939028739929, 0.4627220332622528, 3.956010103225708, -0.6466809511184692, 1.192919373512268, 0.504214882850647, 3.26278018951416, -1.3009297847747803, -1.3390119075775146, 0.7519096732139587, 3.333065986633301, 0.2015230357646942, -0.6114786267280579, -4.337099075317383, -1.6112127304077148, 0.06550803780555725, 0.9997211694717407, 1.459424376487732, -0.30038967728614807, 2.536776065826416, -1.4960159063339233, 0.6834297180175781, -2.260025978088379, 0.3929321765899658, 0.5377828478813171, 1.2854455709457397, -0.8061585426330566, 3.986954927444458, 0.4873068332672119, -1.1118419170379639, 2.4060909748077393, -1.036466121673584, -1.6965516805648804, -0.537761926651001, 1.5740422010421753, -0.7922128438949585, -1.0214169025421143, -5.517156600952148, 0.20943284034729004, -0.7615171670913696, 1.9406452178955078, 3.4190871715545654, 1.5294171571731567, -1.8450238704681396, 1.267638087272644, -0.8039402365684509, -1.7235690355300903, -3.1719090938568115, 1.413804531097412, 0.15402141213417053, -0.2080712467432022, -0.6307393312454224, -1.5612547397613525, -3.6747119426727295, -1.2908689975738525, 1.5406484603881836, 0.5704004168510437, 1.7206320762634277, 1.2411587238311768, 2.243978977203369, -2.2450811862945557, -2.1514501571655273, -4.45841121673584, -6.118025302886963, -3.1404616832733154, 0.5764713287353516, -3.6620097160339355, 3.0681777000427246, 0.9211878776550293, -4.538689136505127, 1.5301153659820557, 2.4622278213500977, -1.317460298538208, 1.1827139854431152, -2.498047113418579, -3.5104498863220215, -0.27883461117744446, 1.3291716575622559, -0.680802047252655, -0.5240675210952759, -1.8769207000732422, 2.780130624771118, -2.5002059936523438, -0.0363299623131752, 0.8128423690795898, 3.4044551849365234, 2.858194351196289, -0.23666870594024658, 0.5246258974075317, 2.470076322555542, -0.37385329604148865, -3.8223791122436523, 3.795917510986328, 1.2261271476745605, -0.10470667481422424, -1.2971051931381226, 0.12137618660926819, 3.8262782096862793, -0.9648874402046204, -1.857344627380371, 0.9733677506446838, -2.193922996520996, -0.27844250202178955, -1.4115923643112183, -1.8804032802581787, -3.345573663711548, -2.318282127380371, 0.9754434823989868, -2.581416368484497, 0.14515244960784912, 3.512070655822754, 1.2504981756210327, 2.3471732139587402, 1.6810652017593384, 1.062911868095398, -5.291266918182373, 1.3268027305603027, 0.6338521242141724, -2.8279759883880615, -2.1726925373077393, -0.7596974968910217, -2.559948682785034, -1.7260161638259888, -2.3234004974365234, -26.956769943237305, -3.141510486602783, -0.49128490686416626, 0.31432104110717773, 0.5888758897781372, 2.1688272953033447, 1.6927380561828613, 1.4521777629852295, -2.3468801975250244, -2.5599470138549805, 2.0045058727264404, -1.592456579208374, -2.903087615966797, -2.3995509147644043, 1.5475393533706665, -2.172856092453003, 3.080517053604126, 1.4449018239974976, -1.9833667278289795, 1.7633883953094482, -2.6871178150177, -0.4912783205509186, 2.8941144943237305, -1.7683782577514648, 0.748108446598053, 2.5388402938842773, 0.8294248580932617, 0.2416953444480896, 0.3993576169013977, 0.28561732172966003, -0.2716418504714966, -1.7260186672210693, 0.12024616450071335, 4.763667106628418, 0.5823931097984314, 1.4126067161560059, 0.538814127445221, 0.3295421600341797, 0.4553394019603729, 0.1984729915857315, 1.2233026027679443, 1.0322259664535522, -1.209289789199829, -3.7617716789245605, -1.4103145599365234, 1.197313904762268, 2.826070785522461, -1.5946698188781738, -2.2671902179718018, 1.1409320831298828, -5.167927265167236, -1.5178186893463135, 2.6193370819091797, -3.1797733306884766, 1.1650959253311157, 0.8051384687423706, 0.2422257512807846, 2.887789011001587, 2.124330997467041, -2.651093006134033, 1.2898406982421875, -1.8237887620925903, -0.1699402630329132, -3.0591320991516113, -1.3723325729370117, 1.493034839630127, 0.38170671463012695, 0.5154252052307129, -1.6658668518066406, -4.224275588989258, 0.5761809349060059, -3.6742911338806152, -0.2564866542816162, -10.326909065246582, -3.9047975540161133, -2.4888901710510254, -1.1820091009140015, -0.7000657916069031, 1.6277453899383545, -0.43440577387809753, -0.11914167553186417, 0.27755090594291687, 1.7854838371276855, 0.9010707139968872, -2.3680763244628906, -1.0081181526184082, 1.145621657371521, -3.8746497631073, 0.05282235145568848, 1.9771782159805298, -0.901292622089386, 3.714132785797119, -0.34709063172340393, 1.702358603477478, -1.756493091583252, -1.1711180210113525, 2.2770423889160156, -4.1004180908203125, 0.7163853645324707, 3.244422435760498, -3.334911584854126, 4.8441619873046875, 3.5921366214752197, -1.0589537620544434, 1.867145299911499, -1.305971384048462, 1.7311302423477173, -1.3488250970840454, -0.1803516447544098, 2.333442449569702, 3.0167675018310547, -5.189304828643799, -1.1616218090057373, 1.3541734218597412, -2.78578782081604, -4.740046977996826, -0.25750577449798584, -0.03088933229446411, 0.5250037908554077, -2.9855756759643555, -2.8575873374938965, 0.5474182963371277, -4.747402191162109, -1.520426630973816, -5.122389793395996, -0.11878974735736847, -0.4784809350967407, -0.9090084433555603, -1.5767648220062256, 4.919524192810059, 2.69270658493042, 1.3843576908111572, -1.9449996948242188, 2.6131253242492676, -1.2136591672897339, -0.8134009838104248, 2.8036458492279053, -5.938331127166748, 1.0440584421157837, -0.6204221248626709, 0.9763050079345703, 2.4802324771881104, 0.3293726146221161, 4.87764835357666, 2.597278594970703, 1.5213730335235596, -0.696615993976593, 0.42496180534362793, -0.20730918645858765, 2.6835522651672363, 0.7238690853118896, -0.580376148223877, 1.5010347366333008, -3.860511302947998, -0.5928359031677246, 1.738502025604248, 2.77396297454834, -2.8920950889587402, -2.1439225673675537, -0.804013729095459, -0.2982857823371887, -0.6762877702713013, -0.6529989838600159, 4.973699569702148, 1.5886307954788208, 4.364034652709961, -2.589022159576416, 1.5655922889709473, 1.8386139869689941, -0.7004270553588867, -3.3074042797088623, 3.8549890518188477, -3.7313148975372314, 2.1592438220977783, 2.8593907356262207, -2.512788772583008, -0.24958352744579315, -0.45624420046806335, -2.5749361515045166, -1.5324597358703613, -1.2161967754364014, -0.20970845222473145, 3.1313438415527344, -0.9698237180709839, 1.2432562112808228, -0.562969982624054, 1.4661293029785156, -1.8328996896743774, 1.2689323425292969, 1.8550024032592773, -0.7955882549285889, 4.152810096740723, 0.7562448978424072, -0.8284973502159119, 0.7842965722084045, -0.41771647334098816, 1.9536807537078857, -0.7113413214683533, 0.10217191278934479, 2.128303289413452, 0.9505492448806763, -2.517935037612915, 0.763178825378418, 3.1702795028686523, -1.198431134223938, -3.364370346069336, 0.11078545451164246, -1.3712425231933594, -1.9890903234481812, 0.1362234354019165, -2.046219825744629, 0.2977793216705322, 2.2406182289123535, -0.7948452830314636, -0.36922580003738403, -3.796900749206543, 0.22935670614242554, 0.23146511614322662, -1.1150577068328857, 0.9195851683616638, -2.3693642616271973, -0.9183236956596375, 1.3188155889511108, 3.3089308738708496, -2.477419376373291, -1.7446593046188354, -1.517979621887207, -0.16280652582645416, 0.5690041780471802, 2.556427240371704, 0.8978905081748962, -7.944443702697754, -2.164135456085205, -2.7632765769958496, -1.4522157907485962, 0.6438045501708984, 1.4098817110061646, -1.3567496538162231, 1.8648402690887451, 3.1171176433563232, 1.6575607061386108, -2.7237229347229004, 1.2763372659683228, -4.726117134094238, -2.027517795562744, -1.2309733629226685, -0.03716675937175751, -1.1580827236175537, 0.9022796154022217, 3.3223190307617188, 6.291447639465332, -0.1809244155883789, 1.151587724685669, -2.576662302017212, -1.7103161811828613, -4.029998302459717, -1.2219164371490479, 3.4183247089385986, -2.706360340118408, 2.703197956085205, -1.834723949432373, 0.2049318552017212, -1.1220834255218506, -0.9718635082244873, -1.9932438135147095, 1.258275032043457, 0.6292574405670166, 2.229818820953369, -0.31075119972229004, -0.8469712734222412, -2.0285892486572266, -0.8366318345069885, 2.5143299102783203, -2.0128471851348877, 0.5897513628005981, -0.016003131866455078, -1.9005110263824463, -1.6991138458251953, 2.882376194000244, 3.2694599628448486, -1.388692855834961, 2.0240116119384766, 0.7214314937591553, 1.3201768398284912, 4.10437536239624, -5.559826374053955, -1.0707554817199707, -1.2769229412078857, -1.4924077987670898, 0.6750385761260986, -1.3059343099594116, 4.562160491943359, -2.40512752532959, 4.54378604888916, 2.3758704662323, -1.3349788188934326, 3.924072504043579, 1.2388324737548828, 1.2438247203826904, 1.915866732597351, 0.021398574113845825, -1.5265822410583496, -4.000311851501465, 1.2514886856079102, -2.533813238143921, 3.5260400772094727, -0.7044086456298828, 3.9258806705474854, 1.7095165252685547, -1.8790696859359741, -2.4349310398101807, -1.2588789463043213, 0.961792528629303, 1.2863348722457886, 5.53193473815918, -2.210070848464966, -0.2356870472431183, -3.4809844493865967, -1.7995233535766602, 2.464651584625244, 0.15096458792686462, 1.5245728492736816, 0.3248547911643982, -0.3119756281375885, -1.3160616159439087, 0.2718614935874939, -0.9228174686431885, -2.6184916496276855, 2.015200138092041, 6.016644477844238, 2.9354588985443115, 4.454959869384766, -3.5965230464935303, 1.7536497116088867, -0.726012110710144, 0.05788670852780342, -1.3507380485534668, -4.508312225341797, 1.9621645212173462, 2.132657051086426, -1.855189561843872, 3.0560014247894287, -3.1305580139160156, 1.3237240314483643, 0.44117406010627747, -1.4682397842407227, -3.9581172466278076, 0.8135530948638916, 4.71319580078125, 2.4112281799316406, 2.844951629638672, 1.7609484195709229, 1.3043804168701172, -2.185027599334717, -3.493284225463867, 0.07160750776529312, -0.7583320140838623, -1.9044053554534912, 2.299287796020508, -1.367112159729004, 1.5654839277267456, 0.6355786919593811, 0.4654582440853119, -0.9414189457893372, 0.9277809858322144, -0.6938427686691284, -1.638296365737915, 2.431591033935547, -3.471282958984375, -0.4709424078464508, -2.4749796390533447, -0.38908737897872925, -2.010831117630005, 4.019482135772705, -2.5699822902679443, 0.5621122121810913, 0.744976282119751, -0.38248658180236816, 1.7929404973983765, 0.15350563824176788, 0.5512022972106934, -1.7876607179641724, -3.2297513484954834, -0.2393670678138733, 0.2658891975879669, -3.2187211513519287, -2.2003421783447266, 1.63241446018219, -2.0600361824035645, -0.9839064478874207, -1.199173092842102, 1.0362117290496826, 2.1092917919158936, -4.8903489112854, 3.1028313636779785, 0.24224120378494263, 2.0937488079071045, 1.3298606872558594, 3.437952756881714, 0.6545395851135254, -0.8292872309684753, 1.5078179836273193, 4.966833114624023, 0.4586646854877472, -0.461232453584671, 2.371988296508789, 0.6274609565734863, 0.41305840015411377, -0.45271530747413635, -3.1475226879119873, 1.7800790071487427, 4.9106974601745605, -1.522773265838623, 0.5761834383010864, -5.858518123626709, 2.647268295288086, 1.740234136581421, -2.463564395904541, -4.326423168182373, -0.37844541668891907, -2.332829475402832, -1.1981825828552246, -0.8585615158081055, -0.25818800926208496, 0.2483471930027008, -7.8586859703063965, -3.0836799144744873, 0.7267621755599976, -2.9315528869628906, 1.2462736368179321, -0.06021643429994583, 4.305059909820557, 1.4516664743423462, -3.446608066558838, -0.14121142029762268, -0.43753230571746826, 0.49191921949386597, 1.7034693956375122, 3.49969744682312, -0.5020135641098022, -0.85323166847229, 0.9946832060813904, -4.152976036071777, -1.0748660564422607, -0.06680218875408173, -0.10131639242172241, -1.9937529563903809, 0.8593407869338989, -0.17820538580417633, 0.8751769661903381, -1.1752638816833496, -1.3160685300827026, 0.2085852026939392, 0.5406569838523865, -0.5372307896614075, 2.599583387374878, 0.8954641819000244, -2.3387863636016846, 3.5905985832214355, -0.9866849184036255, '0.0']]\n"
     ]
    }
   ],
   "source": [
    "BERT_Accuracy = []\n",
    "Doc_Emb = []\n",
    "for typ in file_type:\n",
    "    for key in folder.keys():\n",
    "        for value in folder[key]:\n",
    "            file = \"./swb_ms98_transcriptions/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            wimp_file = \"./wimp_corpus/annotations/\"+key+\"/\"+value+\"/sw\"+value+typ+\"-ms98-a-trans.text\"\n",
    "            corpus = get_sentence_list(file)\n",
    "\n",
    "            all_words = [str(normalize_text(sent)) for sent in corpus]\n",
    "            \n",
    "    \n",
    "            contextual_wimp_data=[]\n",
    "            scores = get_wimp_scores(wimp_file)\n",
    "            counti = 0\n",
    "            Xdata = []\n",
    "            for text in all_words:\n",
    "                sentence_mat = BERT_Embedding(text)\n",
    "#                 print(len(sentence_mat[0]))\n",
    "                WIMP_temp = []\n",
    "                \n",
    "                print(len(sentence_mat[1:-1]), len(scores[counti]))\n",
    "                if len(sentence_mat[1:-1]) == len(scores[counti]):\n",
    "                    countj=0\n",
    "                    for word_emb in sentence_mat[1:-1]:\n",
    "                        temp_emb = list(word_emb)\n",
    "                        temp_emb.append(scores[counti][countj])\n",
    "                        Doc_Emb.append(temp_emb)\n",
    "                counti = counti + 1\n",
    "print(doc_emb)\n",
    "                        \n",
    "#                     print(type(doc_emb))\n",
    "                    \n",
    "#                     data = zip(doc_emb, scores[count])\n",
    "#                     Xdata.append(doc_emb)\n",
    "                    \n",
    "#                 counti = counti + 1 \n",
    "#             print(len(Xdata), len(Xdata[0]))\n",
    "#                 for word_emb in sentence_mat[1:-1]:\n",
    "                    \n",
    "#                     word_emb = np.reshape(word_emb,(word_emb.size,1))\n",
    "\n",
    "#                     W = np.ones(shape=(len(word_emb),1))\n",
    "\n",
    "#                     X = np.dot(W.T,word_emb)\n",
    "#                     WIMP_temp.append(X[0][0])\n",
    "                    \n",
    "                    \n",
    "#                 contextual_wimp_data.append(WIMP_temp)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11280 769\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2866857f2d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc_Emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc_Emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc_Emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(Doc_Emb), len(Doc_Emb[0]))\n",
    "df = pd.DataFrame.from_records(Doc_Emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.17378640776699028, 0.5631067961165048, 0.25),\n",
       " (0.21851851851851853, 0.44814814814814813, 0.19047619047619047),\n",
       " (0.22713864306784662, 0.4026548672566372, 0.22),\n",
       " (0.18508092892329345, 0.41731175228712175, 0.5),\n",
       " (0.14345991561181434, 0.5147679324894515, 0.1),\n",
       " (0.1634182908545727, 0.5097451274362819, 0.5),\n",
       " (0.2249488752556237, 0.45194274028629855, 0.7692307692307693),\n",
       " (0.251434034416826, 0.5210325047801148, 0.5714285714285714),\n",
       " (0.23314917127071824, 0.41104972375690607, 0.5555555555555556),\n",
       " (0.1902834008097166, 0.5236167341430499, 0.13636363636363635),\n",
       " (0.22535211267605634, 0.5736235595390525, 0.3076923076923077),\n",
       " (0.2568493150684932, 0.4041095890410959, 0.29411764705882354),\n",
       " (0.18274111675126903, 0.5042301184433164, 0.2222222222222222),\n",
       " (0.2767857142857143, 0.53125, 0.2),\n",
       " (0.25901639344262295, 0.32131147540983607, 0.23333333333333334),\n",
       " (0.2837465564738292, 0.3443526170798898, 0.7142857142857143),\n",
       " (0.21161825726141079, 0.4190871369294606, 0.32),\n",
       " (0.2403846153846154, 0.38653846153846155, 0.7777777777777778),\n",
       " (0.22288261515601784, 0.5230312035661219, 0.35294117647058826),\n",
       " (0.2135593220338983, 0.5067796610169492, 0.2),\n",
       " (0.22968197879858657, 0.5070671378091873, 0.24),\n",
       " (0.20673076923076922, 0.5096153846153846, 1.0),\n",
       " (0.26019690576652604, 0.4092827004219409, 0.1590909090909091),\n",
       " (0.24472573839662448, 0.44936708860759494, 0.4),\n",
       " (0.2261640798226164, 0.3348115299334812, 0.5),\n",
       " (0.16730038022813687, 0.3288973384030418, 0.17777777777777778),\n",
       " (0.16265060240963855, 0.4457831325301205, 0.29411764705882354),\n",
       " (0.18686868686868688, 0.29545454545454547, 0.5555555555555556),\n",
       " (0.2342814371257485, 0.5516467065868264, 0.2375),\n",
       " (0.2350061199510404, 0.40269277845777235, 0.20512820512820512),\n",
       " (0.28113879003558717, 0.21352313167259787, 0.5614035087719298),\n",
       " (0.24662162162162163, 0.3310810810810811, 0.125),\n",
       " (0.2127659574468085, 0.549645390070922, 0.2727272727272727),\n",
       " (0.224609375, 0.41796875, 0.375),\n",
       " (0.22911051212938005, 0.31805929919137466, 0.15217391304347827),\n",
       " (0.21550387596899226, 0.5782945736434109, 0.42857142857142855),\n",
       " (0.23564954682779457, 0.4471299093655589, 0.3125),\n",
       " (0.25072886297376096, 0.4358600583090379, 0.25),\n",
       " (0.2266244057052298, 0.4041204437400951, 0.21428571428571427),\n",
       " (0.24385964912280703, 0.3385964912280702, 0.23529411764705882),\n",
       " (0.24376731301939059, 0.42382271468144045, 0.5882352941176471),\n",
       " (0.21040189125295508, 0.4302600472813239, 0.26666666666666666),\n",
       " (0.23885918003565063, 0.5347593582887701, 0.14285714285714285),\n",
       " (0.16864608076009502, 0.41330166270783847, 0.6666666666666666)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = zip(BOW_Accuracy,TF_IDF_Accuracy,WEM_Accuracy)\n",
    "a = list(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-106-eb4a92bb3213>:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  vectors = word2vec[word2vec.wv.vocab]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#retrive the vectors from the model \n",
    "vectors = word2vec[word2vec.wv.vocab]\n",
    "#instantiate the PCA class with 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "#train the model \n",
    "result = pca.fit_transform(vectors)\n",
    "#plot a scatter plot\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "plt.scatter(x=result[:, 0], y=result[:, 1])\n",
    "#add annotations of the words to the data points\n",
    "words = list(word2vec.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, size=10, xy=(result[i, 0], result[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
